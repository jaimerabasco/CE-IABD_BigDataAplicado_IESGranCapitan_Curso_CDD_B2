{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"CE Inteligencia Artificial y Big Data","text":""},{"location":"index.html#big-data-aplicado-ce-iabd-ies-gran-capitan","title":"Big Data Aplicado - CE IABD - IES Gran Capit\u00e1n","text":"<p>Apuntes[ES] del m\u00f3dulo Big Data Aplicado del Curso de Especializaci\u00f3n de Inteligencia Artificial y Big Data impartido en el IES Gran Capit\u00e1n de C\u00f3rdoba.</p> <p>El Curso de Especializaci\u00f3n se basa en el Real Decreto 279/2021</p>"},{"location":"index.html#funciones-a-desempenar","title":"Funciones a desempe\u00f1ar","text":"<p>Este m\u00f3dulo profesional contiene la formaci\u00f3n necesaria para desempe\u00f1ar las funciones de aplicar soluciones de Big Data, gestionar y monitorizar el almacenamiento de datos e informaci\u00f3n para tomar decisiones en los negocios.</p> <p>Las funciones antes citadas incluyen aspectos como:</p> <ul> <li>Conocer distintos modelos de negocios de las empresas u organizaciones.</li> <li>Gestionar, seleccionar y transformar la informaci\u00f3n y datos de los distintos negocios.</li> <li>Implantar modelos adecuados de miner\u00eda de datos.</li> <li>Desarrollar e implementar soluciones a problemas propuestos.</li> <li>Validar los modelos para la toma de decisiones de negocio.</li> </ul>"},{"location":"index.html#objetivos-generales","title":"Objetivos generales","text":"<p>Las l\u00edneas de actuaci\u00f3n en el proceso de ense\u00f1anza aprendizaje que permiten alcanzar los objetivos generales del m\u00f3dulo est\u00e1n relacionadas con:</p> <p>Objetivos Generales</p> <ul> <li>La gesti\u00f3n de soluciones a problemas con sistemas Big Data.</li> <li>La gesti\u00f3n de sistemas de almacenamiento del ecosistema Big Data.</li> <li>La generaci\u00f3n de mecanismos de integraci\u00f3n de datos, controlando y comprobando el mantenimiento de los sistemas.</li> <li>La monitorizaci\u00f3n de un sistema que asegure la fiabilidad y estabilidad de los servicios Big Data programados.</li> <li>La validaci\u00f3n de t\u00e9cnicas Big Data para transformar la informaci\u00f3n obtenida en decisiones acertadas de negocios BI.</li> </ul>"},{"location":"index.html#temporalizacion","title":"Temporalizaci\u00f3n","text":"<p> Unidades Did\u00e1cticas Oct Nov Dic Ene Feb Mar Abr May UD1 - Gesti\u00f3n de Soluciones 8 UD2 - Procesado y Presentaci\u00f3n Datos Almacenados 12 8 UD3 - Introducci\u00f3n a la computaci\u00f3n paralela y distribuida 4 UD4 - Apache Hadoop Core 4 8 UD5 - Cluster Apache Hadoop 4 UD6 - Apache Hadoop - Hive 12 UD7 - Ingesta de Datos 4 4 UD8 - Interfaces y herramientas de monitorizaci\u00f3n 12 UD9 - Cloud Big Data 12 UD10 - Aplicaci\u00f3n de Tecnolog\u00edas Big data 20 16 <p></p>"},{"location":"creditos.html","title":"Creditos","text":"<ul> <li>Autor\u00eda: Jaime Rabasco Ronda</li> <li>Descripci\u00f3n: M\u00f3dulo Bug Data Aplicado del Curso de Especializaci\u00f3n de Inteligencia Artificial y Big Data impartido en el IES Gran Capit\u00e1n de C\u00f3rdoba </li> <li>Licencia de uso: CC BY-NC-SA 4.0</li> </ul>"},{"location":"guia_didactica.html","title":"Gu\u00eda Did\u00e1ctica","text":""},{"location":"guia_didactica.html#relacion-de-tareas-con-criterios-de-evaluacion","title":"Relaci\u00f3n de Tareas con Criterios de Evaluaci\u00f3n","text":"Tarea Criterio de Evaluaci\u00f3n Tiempo Herramientas Pr\u00e1ctica HDFS 1 RA5075.3 / CE.3a 30 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica HDFS 2 RA5075.3 / CE.3a y CE.3c 30 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica HDFS 3 RA5075.4 / CE.4e y CE.4f 30 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica HDFS 4 RA5075.4 / CE.4e y CE.4f 30 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Linea Tiempo Tarea HDFS Tarea Criterio de Evaluaci\u00f3n Tiempo Herramientas Pr\u00e1ctica MapReduce-Yarn 1 RA5075.2 / CE.2b 10 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica MapReduce-Yarn 2 RA5075.2 / CE.2b 10 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica MapReduce-Yarn 3 RA5075.4 / CE.4a 10 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica MapReduce-Yarn 4 RA5075.2 / CE.2b 15 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code Pr\u00e1ctica MapReduce-Yarn 5 A5075.2 / CE.2b y RA5075.4 / CE.4a 45 minutos VirtualBox,Apache Hadoop, Github, Visual Studio Code"},{"location":"guia_didactica.html#instrumentos-de-evaluacion","title":"Instrumentos de Evaluaci\u00f3n","text":"<p>Est\u00e1n son los 2 instrumentos de evaluaci\u00f3n que voy a utilizar</p> <ol> <li>R\u00fabricas de Evaluaci\u00f3n</li> </ol> <ul> <li> <p>Descripci\u00f3n: Una r\u00fabrica es una herramienta de evaluaci\u00f3n que identifica criterios espec\u00edficos para una tarea y proporciona niveles graduales de desempe\u00f1o para cada criterio. Permite una evaluaci\u00f3n detallada y objetiva del trabajo de los estudiantes. C\u00f3mo y cu\u00e1ndo usarla: Dada la complejidad del m\u00f3dulo, son id\u00f3neas para ciertas pr\u00e1cticas, tareas y proyectos complejos</p> </li> <li> <p>Momentos de Evaluaci\u00f3n:</p> <ul> <li>Inicio del Proyecto: Presentaci\u00f3n de la r\u00fabrica al alumnado para establecer expectativas claras.</li> <li>Mitad del Proyecto: Revisi\u00f3n formativa para ofrecer retroalimentaci\u00f3n y ajustar enfoques si es necesario.</li> <li>Final del Proyecto: Evaluaci\u00f3n sumativa usando la r\u00fabrica para calificar el desempe\u00f1o final.</li> </ul> </li> <li> <p>Obtenci\u00f3n y Tratamiento de Datos:</p> </li> <li>Recolecci\u00f3n de Datos: Durante las fases de revisi\u00f3n, se recopilar\u00e1n datos cualitativos basados en las observaciones del desempe\u00f1o de los estudiantes y su cumplimiento con los criterios de la r\u00fabrica.</li> <li> <p>Registros de Evaluaciones: Cada estudiante tendr\u00e1 un registro individual donde se anotar\u00e1n las puntuaciones y comentarios de cada criterio.</p> </li> <li> <p>An\u00e1lisis de Datos:</p> </li> <li>Evaluaci\u00f3n de Tendencias: Identificar tendencias comunes en el desempe\u00f1o de los estudiantes, como \u00e1reas de fortaleza y debilidad general.</li> <li> <p>Ajuste de Instrucci\u00f3n: Usar los datos para informar y adaptar estrategias de ense\u00f1anza, enfoc\u00e1ndose en \u00e1reas donde los estudiantes muestran m\u00e1s dificultades.</p> </li> <li> <p>Uso de Datos M\u00e1s All\u00e1 de la Calificaci\u00f3n:</p> </li> <li>Desarrollo Personal del Estudiante: Los datos ayudar\u00e1n a los estudiantes a comprender mejor sus propias habilidades y \u00e1reas de mejora.</li> <li>Planificaci\u00f3n Curricular: Los resultados pueden influir en la planificaci\u00f3n de futuras unidades o cursos, destacando qu\u00e9 aspectos del curr\u00edculo son m\u00e1s efectivos y cu\u00e1les necesitan revisi\u00f3n.</li> <li> <p>Retroalimentaci\u00f3n Constructiva: Los comentarios detallados permiten una retroalimentaci\u00f3n m\u00e1s personalizada y constructiva, fomentando un aprendizaje m\u00e1s profundo.</p> </li> <li> <p>Tarea El RA5075.5 est\u00e1 relacionado con el Business Intelligence. Cada uno de los criterios de Evaluaci\u00f3n establece un criterio de una parte de este resultado de aprendizaje. Un r\u00fabrica para esta tarea del RA5075-CE5.a ser\u00eda:</p> </li> <li> <p>Excelente: Selecciona una amplia y relevante variedad de datos estructurados y no estructurados, demostrando una comprensi\u00f3n profunda de c\u00f3mo estos refuerzan la funci\u00f3n de BI.</p> </li> <li>Bueno: Selecciona datos estructurados y no estructurados relevantes pero con alcance limitado.</li> <li>Suficiente: Selecciona datos estructurados y no estructurados, pero la selecci\u00f3n carece de relevancia o diversidad.</li> <li>Insuficiente: La selecci\u00f3n de datos es pobre o no adecuada para reforzar la funci\u00f3n de BI.  </li> </ul> <ol> <li>Pruebas escritas y pr\u00e1cticas</li> </ol> <ul> <li>Momentos de Evaluaci\u00f3n:</li> <li>Pruebas Escritas:<ul> <li>Durante el Curso: Evaluaciones peri\u00f3dicas para comprobar la comprensi\u00f3n te\u00f3rica. Por ejemplo, despu\u00e9s de cada unidad importante.</li> <li>Final del Curso: Un examen final que abarque todo el contenido del curso.</li> </ul> </li> <li> <p>Pruebas Pr\u00e1cticas:</p> <ul> <li>Evaluaciones Regulares: Tareas pr\u00e1cticas despu\u00e9s de cada tema relevante para aplicar los conceptos aprendidos.</li> <li>Proyecto Final: Un proyecto de Big Data que requiera aplicar todas las habilidades y conocimientos adquiridos durante el curso.</li> </ul> </li> <li> <p>Obtenci\u00f3n y Tratamiento de Datos:</p> </li> <li>Recolecci\u00f3n de Datos: Para las pruebas escritas, se recoger\u00e1n las calificaciones y respuestas de los estudiantes. En las pruebas pr\u00e1cticas, se evaluar\u00e1n las habilidades t\u00e9cnicas, la precisi\u00f3n y la creatividad en la resoluci\u00f3n de problemas.</li> <li> <p>Documentaci\u00f3n de Retroalimentaci\u00f3n: Registrar los comentarios y observaciones detalladas para cada estudiante, enfoc\u00e1ndose tanto en lo que han hecho bien como en las \u00e1reas que necesitan mejorar.</p> </li> <li> <p>An\u00e1lisis de Datos:</p> </li> <li>An\u00e1lisis Cuantitativo y Cualitativo: Analizar las calificaciones para identificar tendencias y \u00e1reas de dificultad comunes. Adem\u00e1s, examinar las respuestas cualitativas para entender mejor c\u00f3mo los estudiantes aplican su conocimiento en situaciones pr\u00e1cticas.</li> <li> <p>Adaptaci\u00f3n del Enfoque Pedag\u00f3gico: Utilizar estos datos para adaptar y mejorar la ense\u00f1anza, enfoc\u00e1ndose en \u00e1reas donde los estudiantes muestran m\u00e1s dificultades.</p> </li> <li> <p>Uso de Datos M\u00e1s All\u00e1 de la Calificaci\u00f3n:</p> </li> <li>Desarrollo de Habilidades: Los datos de las pruebas pr\u00e1cticas pueden usarse para identificar y fomentar las habilidades espec\u00edficas de cada estudiante, promoviendo un aprendizaje m\u00e1s personalizado.</li> <li>Retroalimentaci\u00f3n para el Aprendizaje Continuo: Proporcionar retroalimentaci\u00f3n constructiva que gu\u00ede a los estudiantes en su aprendizaje continuo, m\u00e1s all\u00e1 de la obtenci\u00f3n de una calificaci\u00f3n.</li> <li>Reflexi\u00f3n y Autoevaluaci\u00f3n: Fomentar la reflexi\u00f3n y autoevaluaci\u00f3n en los estudiantes, ayud\u00e1ndoles a reconocer sus propias fortalezas y \u00e1reas de mejora.</li> </ul> <p>En la medida de lo posible, me decantar\u00e9 por la elaboraci\u00f3n de proyectos, ya que dado el tipo de formaci\u00f3n, lo m\u00e1s importante, a mi humilde entender, es que el alumnado est\u00e9 preparado para enfrentarse al mundo laboral, y para ello debe tener la capacidad de desplegar todo su conocimiento de forma pr\u00e1ctica en la empresa lo m\u00e1s pronto posible.</p> <ul> <li>Proyecto</li> </ul> <p>Como indico anteriormente, un proyecto imprescindible ser\u00e1 el proyecto final, cuyos requisitos ser\u00e1n usar todas las habilidades y conocimientos adquiridos durante el curso en Big Data en diferentes proyectos individuales o en grupos donde el propio alumnado elegir\u00e1 la tem\u00e1tica de los datos y cuya elaboraci\u00f3n ser\u00e1 flexible adaptada al entorno real usado pero sin dejar de cumplir requisitos.</p>"},{"location":"guia_didactica.html#uso-en-el-aula","title":"Uso en el Aula","text":"<p>Para todo aquel profesor/a que quiera usarlo y adaptarlo en clase, s\u00f3lo tiene que hacer un fork del repositorio (es p\u00fablico) y subir los cambios que crea oportunos. Se desplegar\u00e1n de forma autom\u00e1tica en una p\u00e1gina web completa.</p>"},{"location":"metadatos.html","title":"Metadatos","text":""},{"location":"metadatos.html#los-metadatos-estan-basados-en-dublin-core","title":"Los metadatos est\u00e1n basados en \"Dublin Core\"","text":"<ul> <li>T\u00edtulo: Big Data Aplicado</li> <li>Creador: Jaime Rabasco Ronda</li> <li>Tema: CE Inteligencia Artificial y Big Data</li> <li>Descripci\u00f3n: M\u00f3dulo Bug Data Aplicado del Curso de Especializaci\u00f3n de Inteligencia Artificial y Big Data impartido en el IES Gran Capit\u00e1n de C\u00f3rdoba </li> <li>Editor: Jaime Rabasco Ronda</li> <li>Colaboradores:</li> <li>Fecha: 05-01-2024</li> <li>Formato: Web</li> <li>Fuente: Propia. Recursos de Internet</li> <li>Idioma: Espa\u00f1ol</li> <li>Derechos: CC BY-NC-SA 4.0</li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/index.html","title":"UD 1 - Gesti\u00f3n de Soluciones","text":""},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/index.html#resultados-de-aprendizaje","title":"Resultados de Aprendizaje","text":"<ul> <li>RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos.</li> <li>RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.</li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/index.html#contenido-y-criterios-de-evaluacion","title":"Contenido y Criterios de Evaluaci\u00f3n","text":"Contenido Criterios de Evaluaci\u00f3n Dise\u00f1o y Construcci\u00f3n de Soluciones RA5075.1a - Se ha caracterizado el proceso de dise\u00f1o y construcci\u00f3n de soluciones en sistemas de almacenamiento de datos.  RA5075.2a - Se ha determinado la importancia de los sistemas de almacenamiento para depositar y procesar grandes cantidades de cualquier tipo de datos r\u00e1pidamente. Procedimientos y Mecanismos de Ingesta de Datos RA5075.1b - Se han determinado los procedimientos y mecanismos para la ingesti\u00f3n de datos. Formato de Datos Adecuado para el Almacenamiento RA5075.1c - Se ha determinado el formato de datos adecuado para el almacenamiento."},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html","title":"UD 1 Gesti\u00f3n de Soluciones - Dise\u00f1o y Construcci\u00f3n de Soluciones","text":""},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#1-introduccion-de-los-datos-al-conocimiento","title":"1. Introducci\u00f3n de los datos al conocimiento","text":"<p>El dato es una representaci\u00f3n sint\u00e1ctica, generalmente num\u00e9rica, que puede manejar un dispositivo electr\u00f3nico - normalmente un ordenador - sin significado por s\u00ed solo. Sin embargo, el dato es a su vez el ingrediente fundamental y el elemento de entrada necesario en cualquier sistema y/o proceso que pretenda extraer informaci\u00f3n o conocimiento sobre un dominio determinado. En este sentido, 7 es un dato, como tambi\u00e9n lo es \u03c0 o como son los t\u00e9rminos aprobado o suspenso.</p> <p>Por su parte, la informaci\u00f3n es el dato interpretado, es decir, el dato con significado. Para obtener informaci\u00f3n, ha sido necesario un proceso en el que, a partir de un dato como elemento de entrada, se realice una interpretaci\u00f3n de ese dato que permita obtener su significado, es decir, informaci\u00f3n a partir de \u00e9l. La informaci\u00f3n es tambi\u00e9n el elemento de entrada y de salida en cualquier proceso de toma de decisiones. Partiendo de los datos del ejemplo anterior, informaci\u00f3n obtenida a partir de los mismos puede ser: El 7 es un n\u00famero primo, \u03c0 es una constante cuyo valor es 3, 141592653..., Mar\u00eda ha aprobado el examen de conducir, Pablo est\u00e1 suspenso en matem\u00e1ticas.</p> <p>A partir de informaci\u00f3n, es posible construir conocimiento. El conocimiento es informaci\u00f3n aprendida, que se traduce a su vez en reglas, asociaciones, algoritmos, etc. que permiten resolver el proceso de toma de decisiones. As\u00ed pues, la informaci\u00f3n obtenida a partir de los datos permite generar conocimiento, es decir, aprender. El conocimiento no es est\u00e1tico, como tampoco lo es siempre el aprendizaje. Aprender, construir conocimiento, implica necesariamente contrastar y validar el conocimiento construido con nueva informaci\u00f3n que permita, a su vez, guiar el aprendizaje y construir conocimiento nuevo. Siguiendo con los ejemplos anteriores, el conocimiento que permite obtener que el 7 es un n\u00famero primo puede ser el algoritmo de Erat\u00f3stenes. Por otra parte, el conocimiento que permite obtener el valor del n\u00famero \u03c0 puede extraerse de los resultados de los trabajos de Jones, Euler o Arqu\u00edmedes, mientras que el aprobado de Mar\u00eda en el examen de conducir y el suspenso de Pablo en matem\u00e1ticas, se pueden obtener de la regla que en una escala de diez asigna el aprobado a notas mayores o iguales que 5 y el suspenso a notas menores.</p> Figura 1.1: Relaci\u00f3n entre datos, informaci\u00f3n y conocimiento en el proceso de toma de decisiones. (Fuente: UCLM) <p>Por tanto, datos, informaci\u00f3n y conocimiento est\u00e1n estrechamente relacionados entre s\u00ed y dirigen cualquier proceso de toma de decisiones  La figura1.1 muestra la relaci\u00f3n entre datos, informaci\u00f3n y conocimiento, en un proceso gen\u00e9rico de toma de decisiones. M\u00e1s concretamente, en el ejemplo del suspenso de Pablo en matem\u00e1ticas, el proceso de toma de decisi\u00f3n acerca de la calificaci\u00f3n de Pablo se estructurar\u00eda de la siguiente forma:</p> <ol> <li> <p>El profesor corrige el examen de Pablo, que ha sacado un 3. Esta calificaci\u00f3n, por s\u00ed sola, es simplemente un dato.</p> </li> <li> <p>A continuaci\u00f3n, el profesor calcula la calificaci\u00f3n final de Pablo, en base a la nota del examen, sus trabajos y pr\u00e1cticas de laboratorio. La nota final de Pablo es un 4. Esto \u00faltimo es informaci\u00f3n.</p> </li> <li> <p>\u00bfHa aprobado Pablo? La informaci\u00f3n de entrada al proceso de decisi\u00f3n es su calificaci\u00f3n final de 4 puntos, obtenida en el paso anterior. El conocimiento del profesor sobre el sistema de calificaci\u00f3n le indica que una nota menor a 5 puntos se corresponde con un suspenso y, en caso contrario, con un aprobado.</p> </li> <li> <p>La informaci\u00f3n de salida tras este proceso de decisi\u00f3n es que Pablo est\u00e1 suspenso en matem\u00e1ticas.</p> </li> </ol> <p>Question</p> <p>Siguiendo el ejemplo anterior \u00bfC\u00f3mo se produce el proceso de toma de decisiones para determinar si un n\u00famero es primo?</p> <p>Aunque se trate de un ejemplo trivial, la importancia del proceso de toma de decisiones no lo es. En marketing, por ejemplo, se analizan bases de datos de clientes para identificar distintos grupos e intentar predecir el comportamiento de estos. En el mundo de las finanzas, las inversiones realizadas por grandes empresas responden a un proceso complejo de toma de decisiones donde los datos son el eje fundamental de este proceso. En medicina, existe una gran cantidad de sistemas de ayuda a la decisi\u00f3n que permiten a los doctores contrastar y validar sus diagn\u00f3sticos de forma precoz. En definitiva, no hay \u00e1rea de conocimiento ni \u00e1mbito de aplicaci\u00f3n que escape al proceso de toma de decisiones.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#2-la-carrera-entre-los-datos-y-la-tecnologia","title":"2. La carrera entre los datos y la tecnolog\u00eda","text":"<p>Que los datos son el elemento fundamental en cualquier proceso y/o sistema de toma de decisiones no es algo nuevo. Sin embargo, los datos no siempre han estado al alcance de los expertos y no siempre ha sido posible ni sencillo procesarlos seg\u00fan las necesidades concretas de cada caso de aplicaci\u00f3n.</p> <p>La informaci\u00f3n, por tanto, siempre ha sido poder y el gran reto ha sido y sigue siendo extraer informaci\u00f3n a partir de datos para generar conocimiento. Para ello, es necesario contar con dos factores que deben estar alineados: datos y tecnolog\u00eda.</p> <p>Obtener datos no ha sido siempre una tarea f\u00e1cil. Esto es debido principalmente a que la gran cantidad de sensores disponibles en la actualidad, que permiten registrar magnitudes de cualquier proceso, no exist\u00eda como a d\u00eda de hoy. Adem\u00e1s, los sensores existentes en esta \u00e9poca (finales del siglo XX y comienzos del siglo XXI) no estaban ampliamente extendidos, ya que sus prestaciones estaban lejos de las que ofrecen hoy y sus precios no estaban al alcance de cualquier usuario. Por tanto, los procesos que se monitorizaban y de los cuales se recog\u00edan datos eran, sobretodo, procesos industriales realizados en grandes empresas. Por todos estos motivos, tradicionalmente se recurr\u00eda a modelos de simulaci\u00f3n que, a trav\u00e9s de la implementaci\u00f3n de un modelo matem\u00e1tico, permit\u00edan generar datos realistas de un proceso.</p> <p>Los datos generados mediante simulaci\u00f3n son conocidos como datos sint\u00e9ticos mientras que los datos provenientes de las lecturas de un sensor se conocen como datos reales.</p> <p>Pero con los datos no es suficiente. Es necesario tambi\u00e9n contar con la tecnolog\u00eda necesaria para su procesamiento. Generar, almacenar y procesar todos estos datos no es una tarea trivial, y plantea una serie de problemas tecnol\u00f3gicos a resolver.</p> <ul> <li>Primer problema tecnol\u00f3gico a resolver, el almacenamiento. Algunas soluciones propuestas pasan por los sistemas de informaci\u00f3n distribuida, entendidos como un conjunto de ordenadores separados f\u00edsicamente y conectados en red destinados al almacenamiento de datos o por los sistemas de informaci\u00f3n en la nube, que permiten adquirir espacio de almacenamiento en servidores privados, dejando la gesti\u00f3n de estos servidores en manos del proveedor.</li> <li> <p>El segundo problema tecnol\u00f3gico es el procesamiento de los datos almacenados. Este aspecto cobra especial relevancia en funci\u00f3n del caso de aplicaci\u00f3n, pudiendo distinguirse entre procesamiento on-line (en l\u00ednea/stream) y procesamiento off-line (fuera de l\u00ednea/batch).</p> </li> <li> <p>\ud83d\udcbb En el caso del procesamiento on-line/stream processing, los datos son procesados a medida que son generados, ya que se requiere una respuesta en tiempo real. Por ejemplo, en un sistema de control del tr\u00e1fico que permite regular los sem\u00e1foros en funci\u00f3n del tr\u00e1fico actual, el sistema debe regular el sem\u00e1foro a medida que se van generando e interpretando los datos del tr\u00e1fico en un instante de tiempo dado.</p> </li> <li>\u274e Por otra parte, en el caso del procesamiento off-line/batch processing, no es necesario que los datos se procesen a medida que se generan. Por ejemplo, en un sistema de detecci\u00f3n del fraude bancario, comprobar si un cliente ha realizado alg\u00fan movimiento fraudulento es una tarea que puede llevarse a cabo off-line, por ejemplo, haciendo un an\u00e1lisis de los movimientos del cliente en un momento dado, sin tener por qu\u00e9 diagnosticar cada movimiento que este va realizando.</li> </ul> <p>En este sentido, la computaci\u00f3n distribuida, en donde m\u00faltiples m\u00e1quinas realizan el procesamiento optimizando el rendimiento o la computaci\u00f3n en la nube, que permite adquirir recursos de procesamiento al igual que se puede adquirir espacio de almacenamiento, son dos soluciones al problema del procesamiento.</p> <p>Otras alternativas son la programaci\u00f3n paralela y la programaci\u00f3n multi-procesador, que permiten, respectivamente, aprovechar el paralelismo de m\u00faltiples hilos de ejecuci\u00f3n dentro de un procesador y realizar el procesamiento dividi\u00e9ndolo en m\u00faltiples hilos en diferentes procesadores</p> <p>Question</p> <p>Piensa en procesos cotidianos que requieran un procesamiento on-line y en otros que requieran un procesamiento off-line.</p> <p>\u23f3 En la actualidad, la proliferaci\u00f3n de una gran cantidad de sensores con altas prestaciones y precios asequibles que permiten monitorizar y generar datos sobre cualquier proceso ha supuesto un incremento exponencial en la cantidad de datos generados. Es posible monitorizar casi cualquier proceso, incluyendo los dom\u00e9sticos como el consumo el\u00e9ctrico de un hogar, la presencia dentro del mismo o procesos cotidianos como la actividad f\u00edsica, entre otros muchos. Hoy, los datos llevan la delantera en la carrera entre datos y tecnolog\u00eda. Si bien es cierto que la tecnolog\u00eda ha experimentado grandes avances en los \u00faltimos a\u00f1os, la cantidad de datos generada no deja de crecer. Esto supone un reto permanente para la tecnolog\u00eda, que sigue evolucionando a nivel hardware con la aparici\u00f3n de arquitecturas con mayores posibilidades de procesamiento, almacenamiento y a nivel software, con la aparici\u00f3n de modelos de programaci\u00f3n que optimizan el procesamiento de los datos.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#3-los-datos-los-de-ayer-y-los-de-hoy","title":"3. Los datos: los de ayer y los de hoy","text":"<p>Al igual que la tecnolog\u00eda ha ido evolucionando para dar respuesta a la ingente cantidad de datos que ha comenzado a generarse, estos \u00faltimos tambi\u00e9n han experimentado una gran evoluci\u00f3n. Esta evoluci\u00f3n, o revoluci\u00f3n, no est\u00e1 \u00fanicamente relacionada con la cantidad de datos (como se expuso en el anterior apartado) sino tambi\u00e9n con el tipo y el formato de los mismos.</p> <p>\ud83d\udcbe Tradicionalmente, el tipo y formato de datos con el que se ha trabajado para extraer informaci\u00f3n y conocimiento a partir de ellos era ciertamente limitado. En muchas ocasiones se trataba de ficheros de datos estructurados de forma tabular, donde cada fila del conjunto de datos representaba una instancia del mismo y cada columna una variable o atributo de la instancia. El formato de archivo que se manejaba sol\u00edan ser formatos de hojas de c\u00e1lculo (.xlsx, .ods, .numbers etc) o ficheros separados por comas (.csv). Muy pocos eran los procesos en los que se trabajaba con otros tipos de datos como texto, im\u00e1genes, audio e incluso v\u00eddeos, ya que los formatos de estos tipos de datos eran limitados hace unos a\u00f1os, su procesamiento m\u00e1s complejo y la tecnolog\u00eda para ello a\u00fan en desarrollo.</p> <p>Aunque a d\u00eda de hoy tambi\u00e9n se sigue trabajando con archivos de datos en forma de hojas de c\u00e1lculo y/o archivos tradicionales para generar conocimiento a partir de ellos, las posibilidades actuales son pr\u00e1cticamente ilimitadas.</p> <ul> <li> <p>\u270f\ufe0f En cuanto al texto, las t\u00e9cnicas de inteligencia artificial y procesamiento del lenguaje natural hacen posible la extracci\u00f3n de conocimiento a partir de grandes vol\u00famenes de textos, que pueden provenir de p\u00e1ginas web, archivos .pdf, redes sociales, etc.</p> </li> <li> <p>\ud83d\udcf9 El desarrollo de hardware con mejores prestaciones y los nuevos modelos de programaci\u00f3n permiten procesar en la actualidad grandes cantidades de im\u00e1genes, audios y v\u00eddeos con una gran variedad de t\u00e9cnicas de inteligencia artificial en tiempos razonables.</p> </li> <li> <p>\u26ab Finalmente, han aparecido nuevos tipos y formatos de datos, como por ejemplo, aquellos datos generados a partir de grafos, los cuales se tratar\u00e1n en pr\u00f3ximas secciones y cap\u00edtulos con m\u00e1s detenimiento. Estos datos se corresponden, por ejemplo, con datos geogr\u00e1ficos obtenidos a partir de mapas como los generados en aplicaciones como Google Maps u Open Street Maps o datos de seguimiento y actividad en redes sociales de gran valor en campa\u00f1as publicitarias entre otros muchos.</p> </li> </ul> <p>Question</p> <p>Haz una b\u00fasqueda y elabora un listado con distintos tipos de datos y los formatos de almacenamiento m\u00e1s utilizados con los que se trabaja en ciencia de datos y big data.</p> <p>Los diferentes tipos y formatos de datos, los de ayer y los de hoy, son la materia b\u00e1sica fundamental en cualquier proceso de extracci\u00f3n de informaci\u00f3n y de conocimiento. Despu\u00e9s, las metodolog\u00edas empleadas para ello y arquitecturas hardware sobre las que se realice el procesamiento de los mismos, permitir\u00e1n definir procesos y metodolog\u00edas de big data, aplicadas a un \u00e1mbito concreto.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#4-soluciones-big-data","title":"4. Soluciones Big Data","text":"<p>En esta nueva era tecnol\u00f3gica en la que nos hayamos inmersos, a diario se generan enormes cantidades de datos, del orden de petabytes (m\u00e1s de un mill\u00f3n de gigabytes) en muy cortos peri\u00f3dos de tiempo. Hoy en d\u00eda, cualquier dispositivo como puede ser un reloj, un coche, un smartphone, etc est\u00e1 conectado a Internet generando, enviando y recibiendo una gran cantidad de datos. Tanto es as\u00ed, que se estima que el 90 % de los datos disponibles en el mundo ha sido generado en los \u00faltimos a\u00f1os. Sin lugar a dudas, esta y las pr\u00f3ximas generaciones ser\u00e1n las generaciones del big data.</p> <p>Esta realidad descrita anteriormente demanda la capacidad de enviar y recibir datos e informaci\u00f3n a gran velocidad, as\u00ed como la capacidad de almacenar tal cantidad de datos y procesarlos en tiempo real. As\u00ed pues, la gran cantidad de datos disponibles junto con las herramientas, tanto hardware como software, que existen a disposici\u00f3n para analizarlos se conoce como big data.</p> <p>\u270c\ufe0f No existe una definici\u00f3n precisa del t\u00e9rmino big data, ni tampoco un t\u00e9rmino en castellano que permita denominar este concepto. A veces se usan en castellano los t\u00e9rmino datos masivos o grandes vol\u00famenes de datos para hacer referencia al big data. Por este motivo, a menudo el concepto de big data es definido en funci\u00f3n de las caracter\u00edsticas que poseen los datos y los procesos que forman parte de este nuevo paradigma de computaci\u00f3n. Esto es lo que se conoce como \u270c\ufe0f las Vs del big data \u270c\ufe0f.</p> <p>Algunos autores coinciden en que big data son datos cuyo volumen es demasiado grande como para procesarlos con las tecnolog\u00edas y t\u00e9cnicas tradicionales, requiriendo nuevas arquitecturas hardware, modelos de programaci\u00f3n y algoritmos para su procesamiento. Adem\u00e1s, se trata de datos que se presentan en una gran variedad de estructuras y formatos: datos sint\u00e9ticos, provenientes de sensores, num\u00e9ricos, textuales, im\u00e1genes, audio, v\u00eddeo... Finalmente, se trata de datos que requieren ser procesados a gran velocidad para poder extraer valor y conocimiento de ellos. Esta concepci\u00f3n se conoce como las tres Vs del big data (ver figura1.2).</p> Figura 1.2: Definici\u00f3n de big data en base a \u201cLas tres Vs del big data\". (Fuente: Researchgate) <p>Otros autores ampl\u00edan las caracter\u00edsticas que han de tener los datos que forman parte del big data, incluyendo \u201cotras Vs\u201d como lo son:</p> <ul> <li>\u270c\ufe0f Volatilidad, referida al tiempo durante el cual los datos recogidos son v\u00e1lidos y a durante cu\u00e1nto tiempo deber\u00e1n ser almacenados.</li> <li>\u270c\ufe0f Valor, referido a la utilidad de los datos obtenidos para extraer conocimiento y tomar decisiones a partir de ellos.</li> <li>\u270c\ufe0f Validez, referida a lo precisos que son los datos para el uso que se pretende darles. El uso de datos validados permitir\u00e1 ahorrar tiempo en etapas como la limpieza y el preprocesamiento de los datos.</li> <li>\u270c\ufe0f Veracidad, relacionada con la confiabilidad del origen del cual provienen los datos con los que se trabajar\u00e1 as\u00ed como la incertidumbre o el ruido que pudiera existir en ellos.</li> <li>\u270c\ufe0f Variabilidad, frente a la variedad de estructuras y formatos, hace referencia a la complejidad del conjunto de datos, es decir, al n\u00famero de variables que contiene. Estas caracter\u00edsticas, unidas a las tres Vs descritas anteriormente, se conocen como las ocho Vs del big data (ver figura1.3).</li> </ul> Figura 1.3: Definici\u00f3n de big data en base a \u201cLas ocho Vs del big data\u201d. (Fuente: Linkedin) <p>Dado que no existe una definici\u00f3n uniforme para el t\u00e9rmino big data, muchos autores definen el t\u00e9rmino en funci\u00f3n de aquellas caracter\u00edsticas que consideran m\u00e1s relevantes, por lo que es com\u00fan encontrar \u201clas cinco Vs del big data\u201d, \u201clas siete Vs del big data\u201d o \u201clas diez Vs del big data\u201d seg\u00fan cada autor, apareciendo distintos t\u00e9rminos para describir el big data, como tambi\u00e9n pueden ser visualizaci\u00f3n o vulnerabilidad, entre otros. </p> <p>Son muchas las soluciones a nivel hardware y software, que se han propuesto a los problemas derivados del almacenamiento y el procesamiento de big data. A continuaci\u00f3n, se describen los fundamentos de tres de ellas, las cuales ser\u00e1n desarrolladas a nivel te\u00f3rico, tecnol\u00f3gico y pr\u00e1ctico en los siguientes cap\u00edtulos.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#5-almacenes-de-datos","title":"5. Almacenes de datos","text":"<p>\ud83d\udcbe Tradicionalmente hablando, cuando nos referimos a almacenes de datos, podemos hablar de las bases de datos relacionales son colecciones de datos integrados, almacenados en un soporte secundario no vol\u00e1til y con redundancia controlada. La definici\u00f3n de los datos y la estructura de la base de datos debe estar basada en un modelo de datos que permita captar las interrelaciones y restricciones existentes en el dominio que se pretende modelizar. A su vez, un Sistema Gestor de Bases de Datos (SGBD) se compone de una colecci\u00f3n de datos estructurados e interrelacionados (una base de datos) as\u00ed como de un conjunto de programas para acceder a dichos datos.</p> <p>\ud83d\udcbe Las bases de datos tradicionales, siguiendo la definici\u00f3n anterior, est\u00e1n basadas generalmente en sistemas relacionales u objeto-relacionales. Para el acceso, procesamiento y recuperaci\u00f3n de los datos, se sigue el modelo Online Transaction Processing (OLTP). Una transacci\u00f3n es una interacci\u00f3n completa con un sistema de base de datos, que representa una unidad de trabajo. As\u00ed pues, una transacci\u00f3n representa cualquier cambio que se produzca en una base de datos.</p> <p>\ud83d\udcbe El modelo OLTP, traducido al castellano como procesamiento de transacciones en l\u00ednea, permite gestionar los cambios de la base de datos mediante la inserci\u00f3n, actualizaci\u00f3n y eliminaci\u00f3n de informaci\u00f3n de la misma a trav\u00e9s de transacciones b\u00e1sicas que son procesadas en tiempos muy peque\u00f1os.</p> <p>\ud83d\udcbe Con respecto a la recuperaci\u00f3n de informaci\u00f3n de la base de datos, se utilizan operadores cl\u00e1sicos (concatenaci\u00f3n, proyecci\u00f3n, selecci\u00f3n, agrupamiento...) para realizar consultas b\u00e1sicas y sencillas (realizadas, mayoritariamente, en lenguaje SQL y extensiones del mismo).</p> <p>\ud83d\udcbe Finalmente, las opciones de visualizaci\u00f3n de los datos recuperados son limitadas, mostr\u00e1ndose fundamentalmente los resultados de forma tabular y requiriendo un procesamiento adicional y m\u00e1s complejo en caso de querer presentar datos complejos.</p> <p>\ud83d\udd01 La revoluci\u00f3n en la generaci\u00f3n, almacenamiento y procesamiento de los datos, as\u00ed como la irrupci\u00f3n del big data, han puesto a prueba el modelo de funcionamiento, rendimiento y escalabilidad de las bases de datos relacionales tradicionales. En la actualidad, se requiere de soluciones integradas que a\u00fanen datos y tecnolog\u00eda para almacenar y procesar grandes cantidades de datos con diferentes estructuras y formatos con el objetivo de facilitar la consulta, el an\u00e1lisis y la toma de decisiones sobre los mismos. En este sentido, la inteligencia de negocio, m\u00e1s conocida por el t\u00e9rmino ingl\u00e9s business intelligence, investiga en el dise\u00f1o y desarrollo de este tipo de soluciones. La inteligencia de negocio puede definirse como la capacidad de una empresa de estudiar sus acciones y comportamientos pasados para entender d\u00f3nde ha estado la empresa, determinar la situaci\u00f3n actual y predecir o cambiar lo que suceder\u00e1 en el futuro, utilizando las soluciones tecnol\u00f3gicas m\u00e1s apropiadas para optimizar el proceso de toma de decisiones.</p> <p>Estas nuevas soluciones requerir\u00e1n un modelo de procesamiento diferente a OLTP. Esto es as\u00ed, ya que el objetivo perseguido por la inteligencia de negocio est\u00e1 menos orientado al \u00e1mbito transaccional y m\u00e1s enfocado al \u00e1mbito anal\u00edtico. Las nuevas soluciones utilizan el modelo Online analytical processing (OLAP).</p> <p>La principal diferencia entre OLTP y OLAP estriba en que mientras que el primero es un sistema de procesamiento de transacciones en l\u00ednea, el segundo es un sistema de recuperaci\u00f3n y an\u00e1lisis de datos en l\u00ednea. Por tanto, OLAP complementa a SQL aportando la capacidad de analizar datos desde distintas variables y dimensiones, mejorando el proceso de toma de decisiones. Para ello, OLAP permite realizar c\u00e1lculos y consolidaciones entre datos de distintas dimensiones, creando modelos que no presentan limitaciones conceptuales ni f\u00edsicas, presentando y visualizando la informaci\u00f3n de forma flexible, esto es, en diferentes formatos.</p> <p>Los sistemas OLAP est\u00e1n basados, generalmente, en sistemas o interfaces multidimensionales que proporcionan facilidades para la transformaci\u00f3n de los datos, permitiendo obtener nuevos datos m\u00e1s combinados y agregados que los obtenidos mediante las consultas simples realizadas por OLTP. Al contrario que en OLTP, las unidades de trabajo de OLAP son m\u00e1s complejas que en OLTP y consumen m\u00e1s tiempo.</p> <p>Finalmente, en cuanto a la visualizaci\u00f3n de los mismos, los sistemas OLAP permiten la visualizaci\u00f3n y el an\u00e1lisis multidimensional a partir de diferentes vistas de los datos, presentando los resultados en forma matricial y con mayores posibilidades est\u00e9ticas y visuales. La tabla 1.1 muestra un resumen con las principales diferencias entre los sistemas OLTP y OLAP.</p> <p> Bases de datos relacionales(OLTP) Soluciones Business Intelligence(OLAP) Concepto Sistema de procesamiento de transacciones en l\u00ednea Sistema de recuperaci\u00f3n y an\u00e1lisis de datos en l\u00ednea Funciones Gesti\u00f3n de transacciones: inserci\u00f3n, actualizaci\u00f3n, eliminaci\u00f3n... An\u00e1lisis de datos para dar soporte a la toma de decisiones Procesamiento Transacciones cortas Procesamientos de an\u00e1lisis complejos Tiempo Las transacciones requieren poco tiempo de ejecuci\u00f3n Los an\u00e1lisis requieren mayor tiempo de ejecuci\u00f3n Consultas Simples, utilizando operadores b\u00e1sicos tradicionales Complejas, permitiendo analizar los datos desde m\u00faltiples dimensiones Visualizaci\u00f3n B\u00e1sica. Muestra los datos en forma tabular Muestra los datos en forma matricial. Mayores posibilidades gr\u00e1ficas <p>Tabla 1.1: Tabla resumen y comparativa entre OLTP y OLAP </p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#51-sistemas-de-ayuda-a-la-decision","title":"5.1. Sistemas de ayuda a la decisi\u00f3n","text":"<p>En una empresa u organizaci\u00f3n, los datos generados a diario son, principalmente, aquellos derivados de las operaciones rutinarias de la empresa. Estos datos, tradicionalmente, se almacenaban en bases de datos relacionales y su manipulaci\u00f3n se correspond\u00eda con transacciones realizadas sobre la base de datos. Sin embargo, el objetivo de cualquier organizaci\u00f3n es seleccionar esos datos para realizar estudios y an\u00e1lisis que permitan generar informes que, a su vez, permitan a la empresa extraer informaci\u00f3n para tomar decisiones estrat\u00e9gicas que conduzcan a la organizaci\u00f3n al \u00e9xito.</p> <p>El crecimiento exponencial de los datos manejados por una organizaci\u00f3n ha hecho que los computadores sean las \u00fanicas herramientas capaces de procesar estos datos para obtener informaci\u00f3n y ofrecer ayuda en la toma de decisiones. En este contexto, aparecen los sistemas de ayuda a la decisi\u00f3n o Decision Support Systems (DSS) que ayudan a quienes ocupan puestos de gesti\u00f3n a tomar decisiones o elegir entre diferentes alternativas. </p> <p>Sistema de ayuda a la decisi\u00f3n</p> <p>\ud83d\udcc4 Sistema de ayuda a la decisi\u00f3n: Conjunto de t\u00e9cnicas y herramientas tecnol\u00f3gicas desarrolladas para procesar y analizar datos para ofrecer soporte en la toma decisiones a quienes ocupan puestos de gesti\u00f3n o direcci\u00f3n en una organizaci\u00f3n. Para ello, el sistema combina los recursos de los gestores junto con los recursos computacionales para optimizar el proceso de toma de decisiones.</p> <p>Mientras que las bases de datos relacionales han sido tradicionalmente el componente del back-end en el dise\u00f1o de sistemas de ayuda a la decisi\u00f3n, los almacenes de datos se han convertido en una opci\u00f3n mucho m\u00e1s competitiva como elemento back-end al mejorar el rendimiento de \u00e9stas.</p> <p>Los campos de aplicaci\u00f3n de los almacenes de datos no se reducen \u00fanicamente al \u00e1mbito empresarial, sino que cubren multitud de dominios como las ciencias naturales, demograf\u00eda, epidemiolog\u00eda o educaci\u00f3n, entre otros muchos. La propiedad com\u00fan a todos estos campos y que hace de los almacenes de datos una adecuada soluci\u00f3n en estos \u00e1mbitos es la necesidad de almacenamiento y herramientas de an\u00e1lisis que permitan obtener en tiempos razonables informaci\u00f3n y conocimiento \u00fatiles para mejorar el proceso de toma de decisiones.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#52-almacenes-de-datos-concepto","title":"5.2. Almacenes de datos: Concepto","text":"<p>La aparici\u00f3n de los almacenes de datos est\u00e1 ligada, principalmente, a una serie de retos que es necesario abordar para convertir los datos transaccionales con los que trabaja una base de datos relacional en informaci\u00f3n para generar conocimiento y dar soporte al proceso de toma de decisiones</p> <ul> <li> <p>Accesibilidad: Desde cualquier dispositivo, a cualquier tipo de usuario y a gran cantidad de informaci\u00f3n que no puede ser almacenada de forma centralizada. La accesibilidad, en este sentido, debe hacer frente al problema de la escalabilidad del sistema y de los datos que este maneja.</p> </li> <li> <p>Integraci\u00f3n: Referente a la gesti\u00f3n de datos heterog\u00e9neos, con distintos formatos, y provenientes de distintos \u00e1mbitos de la organizaci\u00f3n. Una correcta integraci\u00f3n debe garantizar a su vez la correcci\u00f3n y completitud de los datos integrados.</p> </li> <li> <p>Consultas mejoradas: Permitiendo incluir operadores avanzados y dar soporte a herramientas y procedimientos que posibiliten obtener el m\u00e1ximo partido de los datos existentes. De este modo, ser\u00e1 posible obtener informaci\u00f3n precisa para realizar un an\u00e1lisis eficiente.</p> </li> <li> <p>Representaci\u00f3n multidimensional: Proporciona herramientas para analizar de forma multi-dimensional los datos del sistema, incluyendo datos de diferentes unidades de la organizaci\u00f3n con el objetivo de proporcionar herramientas de an\u00e1lisis y visualizaci\u00f3n multi-dimensional para mejorar el proceso de toma de decisiones.</p> </li> </ul> <p>Almac\u00e9n de datos (Data Warehouse)</p> <p>\ud83d\udcda Por tanto, un almac\u00e9n de datos, m\u00e1s conocido por el t\u00e9rmino data warehouse (en ingl\u00e9s), es una soluci\u00f3n de business intelligence que combina tecnolog\u00edas y componentes con el objetivo de ayudar al uso estrat\u00e9gico de los datos por parte de una organizaci\u00f3n. Esta soluci\u00f3n debe proveer a la empresa, de forma integrada, de capacidad de almacenamiento de una gran cantidad de datos as\u00ed como de herramientas de an\u00e1lisis de los mismos que, frente al procesamiento de transacciones, permita transformar los datos en informaci\u00f3n para ponerla a disposici\u00f3n de la organizaci\u00f3n y optimizar el proceso de toma de decisiones.</p> <p>\ud83d\udcc4 O bien, m\u00e1s resumidamente, seg\u00fan W. Inmon (conocido por ser el \u201cpadre\u201d del concepto de Almac\u00e9n de datos (Data Warehouse)): Colecci\u00f3n de datos orientados a temas, integrados, variante en el tiempo y no vol\u00e1til que da soporte al proceso de toma de decisiones de la direcci\u00f3n.</p> <p>Para entender correctamente esta definici\u00f3n, es necesario ahondar en las caracter\u00edsticas que incluye la misma.</p> <ul> <li> <p>Orientados a temas: Es decir, no orientado a procesos (transacciones), sino a entidades de mayor nivel de abstracci\u00f3n como \u201cart\u00edculo\u201d o \u201cpedido\u201d.</p> </li> <li> <p>Integrados: Almacenados en un formato uniforme y consistente, lo que implica depurar o limpiar los datos para poder integrarlos.</p> </li> <li> <p>Variante en el tiempo: Asociados a un instante de tiempo (mes, trimestre, a\u00f1o...)</p> </li> <li> <p>No vol\u00e1tiles: Se trata de datos persistentes que no cambian una vez se incluyen en el almac\u00e9n de datos.</p> </li> </ul> <p>El dise\u00f1o y funcionamiento de los almacenes de datos se basa en el sistema de procesamiento anal\u00edtico en-l\u00ednea, OLAP. Este sistema se encarga del an\u00e1lisis, interpretaci\u00f3n y toma de decisiones acerca del negocio, en contraposici\u00f3n a los sistemas de procesamiento de transacciones en l\u00ednea, OLTP.</p> <p>As\u00ed pues, los sistemas OLTP est\u00e1n dirigidos por la tecnolog\u00eda y orientados a automatizar las operaciones del d\u00eda a d\u00eda de la organizaci\u00f3n, mientras que los sistemas OLAP est\u00e1n dirigidos por el negocio y proporcionan herramientas para tomar decisiones a largo plazo, mejorando la estrategia y la competitividad de la organizaci\u00f3n. La tabla 1.2 muestra una comparativa entre las principales caracter\u00edsticas de las bases de datos operacionales (OLTP) y los almacenes de datos (OLAP).</p> <p> Caracter\u00edstica BBDD Operacionales(OLTP) Almac\u00e9n Datos(OLAP) Objetivo Depende de la aplicaci\u00f3n Toma de decisiones Usuarios Miles Cientos Trabajo con... Transacciones predefinidas Consultas y an\u00e1lisis espec\u00edficos Acceso Lectura y escritura a cientos de registros Principalmente lecutra. Miles de registros Datos Detallados, num\u00e9ricos y alfanum\u00e9ricos Agregados, principalmente num\u00e9ricos Integraci\u00f3n En funci\u00f3n de la aplicaci\u00f3n Basados en temas, con mayor nivel de abstracci\u00f3n Calidad Medida en t\u00e9rminos de integridad Medida en t\u00e9rminos de consistencia Temporalidad Datos Solo datos actuales Datos actuales e hist\u00f3ricos Actualizaciones Continuas Peri\u00f3dicas Modelo Normalizado Desnormalizado, multidimensional Optimizaci\u00f3n Para acceso OLTP a parte de la BBDD Para acceso OLAP a gran parte de la BBDD <p>Tabla 1.2. Diferencias entre BBDD Operacionales y Almacenes de Datos </p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#53-almacenes-de-datos-arquitectura","title":"5.3 Almacenes de datos: Arquitectura","text":"<p>Las arquitecturas disponibles para el dise\u00f1o de almacenes de datos se basan, principalmente, en garantizar que el sistema cumpla una serie de propiedades esenciales para su \u00f3ptimo funcionamiento</p> <ul> <li> <p>Separaci\u00f3n: De los datos transaccionales y los datos estrat\u00e9gicos que sirven como punto de partida a la toma de de decisiones.</p> </li> <li> <p>Escalabilidad: A nivel hardware y software, para actualizarse y garantizar el correcto funcionamiento del sistema a medida que el n\u00famero de datos y usuarios aumenta.</p> </li> <li> <p>Extensiones: Permitiendo integrar e incluir nuevas aplicaciones sin necesidad de redise\u00f1ar el sistema completo.</p> </li> <li> <p>Seguridad: Monitorizando el acceso a los datos estrat\u00e9gicos guardados en el almac\u00e9n de datos.</p> </li> </ul> <p>Almac\u00e9n de datos</p> <p>\ud83d\udcc4 Las arquitecturas de almacenes de datos se clasifican, fundamentalmente, en dos tipos: arquitecturas orientadas a la estructura y arquitecturas orientadas a la empresa.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#531-arquitecturas-orientadas-a-la-estructura","title":"5.3.1 Arquitecturas orientadas a la estructura","text":"<p>Las arquitecturas orientadas a la estructura reciben su nombre debido a que est\u00e1n dise\u00f1adas poniendo especial \u00e9nfasis en el n\u00famero de capas y elementos que componen la arquitectura del sistema de almac\u00e9n de datos. De acuerdo con este criterio, es posible distinguir las siguientes arquitecturas.</p> <p>Arquitectura de una capa</p> <p>El objetivo principal de esta arquitectura, poco utilizada en la pr\u00e1ctica, es minimizar la cantidad de datos almacenados eliminando para ello los datos redundantes. La figura1.4 muestra un esquema de este tipo de arquitectura. En ella, el almac\u00e9n de datos creado es virtual, existiendo un middleware que interpreta los datos operacionales y ofrece una vista multidimensional de ellos.</p> <p>El principal inconveniente de esta arquitectura es que su simplicidad hace que el sistema no cumpla la propiedad de separaci\u00f3n, ya que los procesos de an\u00e1lisis se realizan sobre los datos operacionales.</p> Figura 1.4. Almac\u00e9n de datos. Arquitectura de una capa. (Fuente: UCLM) <p>Arquitectura de dos capas</p> <p>Fue dise\u00f1ada con el objetivo de solucionar el problema de la separaci\u00f3n que presentaba la arquitectura de una capa. Este esquema consigue subrayar la separaci\u00f3n entre los datos disponibles y el almac\u00e9n de datos a trav\u00e9s de los siguientes componentes (ver figura1.5:</p> <ul> <li> <p>Capa de origen (fuente): Se corresponde con los or\u00edgenes y fuentes de los datos heterog\u00e9neos que se pretenden incorporar al almac\u00e9n de datos.</p> </li> <li> <p>Puesta a punto: Proceso por el cual se utilizan herramientas de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL) para extraer, limpiar, filtrar, validar y cargar datos en el almac\u00e9n de datos.</p> </li> <li> <p>Capa de almac\u00e9n de datos: Almacenamiento centralizado de la informaci\u00f3n en el almac\u00e9n de datos, el cual puede ser utilizado para crear data marts o repositorios de metadatos.</p> </li> <li> <p>An\u00e1lisis: Conjunto de procesos a partir de los cuales los datos son analizados de forma eficiente y flexible, generando informes y simulando escenarios hipot\u00e9ticos para dar soporte a la toma de decisiones.</p> </li> </ul> <p>Data mart</p> <p>\ud83d\udccb Data mart es un subconjunto o agregaci\u00f3n de los datos almacenados en un almac\u00e9n de datos primario que incluye informaci\u00f3n relevante sobre un \u00e1rea espec\u00edfica del negocio.</p> Figura 1.5. Almac\u00e9n de datos. Arquitectura de dos capas. (Fuente: UCLM) <p>Arquitectura de tres capas</p> <p>Este tercer tipo de arquitectura incluye una capa llamada de datos reconciliados o almac\u00e9n de datos operativos. Con esta capa, los datos operativos obtenidos tras la limpieza y depuraci\u00f3n son integrados y validados, proporcionando un modelo de datos de referencia para toda la organizaci\u00f3n.</p> <p>De este modo, el almac\u00e9n de datos no se nutre de los datos de origen directamente, sino de los datos reconciliados generados, los cuales tambi\u00e9n son utilizados para realizar de forma m\u00e1s eficiente tareas operativas, como la realizaci\u00f3n de informes o la alimentaci\u00f3n de datos a procesos operativos.</p> <p>Esta capa de datos reconciliados tambi\u00e9n puede implementarse de forma virtual en una arquitectura de dos capas, ya que se define como una vista integrada y coherente de los datos de origen. La figura1.6 muestra de forma gr\u00e1fica este tipo de arquitectura</p> Figura 1.6. Almac\u00e9n de datos. Arquitectura de tres capas. (Fuente: UCLM)"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#532-arquitecturas-orientadas-a-la-empresa","title":"5.3.2. Arquitecturas orientadas a la empresa","text":"<p>Esta clasificaci\u00f3n distingue cinco tipos de arquitecturas que combinan las capas mencionadas en la primera clasificaci\u00f3n para dise\u00f1ar almacenes de datos.</p> <p>1. Arquitectura de data marts independientes</p> <p>Arquitectura preliminar en la que los distintos data marts son dise\u00f1ados de forma independiente y construidos de forma no integrada. Suele utilizarse en los inicios de implementaci\u00f3n de proyectos de almacenes de datos y reemplazada a medida que el proyecto va creciendo.</p> <p>2. Arquitectura en bus</p> <p>Similar a la anterior, asegura la integraci\u00f3n l\u00f3gica de los data marts creados, ofreciendo una visi\u00f3n amplia de los datos de la empresa y permitiendo realizar an\u00e1lisis rigurosos de los procesos que en ella se llevan a cabo.</p> <p>3. Arquitectura hub-and-spoke (centro y radio)</p> <p>Esta arquitectura es muy utilizada en almacenes de datos de tama\u00f1os medio y grande. Su dise\u00f1o pone especial \u00e9nfasis en garantizar la escalabilidad del sistema y permitir a\u00f1adir extensiones al mismo.</p> <p>Para ello, los datos se almacenan de forma at\u00f3mica y normalizada en una capa de datos reconciliados que alimenta a los data marts construidos que contienen, a su vez, los datos agregados de forma multidimensional. Los usuarios acceden a los data marts, si bien es cierto que tambi\u00e9n pueden hacer consultas directamente sobre los datos reconciliados.</p> <p>4. Arquitectura centralizada</p> <p>Se trata de un caso particular de la arquitectura hub-and-spoke. En ella, la capa de datos reconciliados y los data marts se almacenan en un \u00fanico repositorio f\u00edsico.</p> <p>5. Arquitectura federada</p> <p>Se trata de un tipo de arquitectura muy utilizada en entornos din\u00e1micos, cuando se pretende integrar almacenes de datos o data marts existentes con otros para ofrecer un entorno \u00fanico e integrado de soporte a la toma de decisiones. De esta forma, cada almac\u00e9n de datos y cada data mart es integrado virtual o f\u00edsicamente con lo dem\u00e1s. Para ello, se utilizan una serie de t\u00e9cnicas y herramientas avanzadas como son las ontolog\u00edas, consultas distribuidas e interoperatividad de metadatos, entre otras.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#6-data-lake","title":"6. Data Lake","text":""},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#61-concepto","title":"6.1 Concepto","text":"<p>Un data lake o lago de datos es un repositorio centralizado que permite almacenar, compartir, gobernar y descubrir todos los datos estructurados y no estructurados de una organizaci\u00f3n a cualquier escala. Es el lugar en el que se vuelcan los datos en bruto.</p> <p>Los data lakes no requieren un esquema predefinido, se pueden almacenar y procesar datos sin esquema y en cualquier formato sin la necesidad de conocer c\u00f3mo se van a explotar en el futuro. Esta caracter\u00edstica evita que sean necesarios complejos procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) (que explicaremos en el pr\u00f3ximo punto) de limpieza y preparaci\u00f3n.</p> <p>Entre las caracter\u00edsticas m\u00e1s importantes de los data lakes se encuentra su flexibilidad en almacenar diferentes tipos de datos, que proporciona la agilidad necesaria para los procesos de ingesta. Tambi\u00e9n es muy importante que proporcione suficiente trazabilidad, y de esta manera poder determinar los cambios que han sufrido los datos en los procesos de transformaci\u00f3n o ingesta.</p> Figura 1.7. Data Lake. (Fuente: AprenderBigData.com)"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/1_dis_const_soluc.html#62-data-lake-vs-data-warehouse","title":"6.2 Data Lake vs Data Warehouse","text":"<p>Los data lake y los data warehouse se utilizan de forma generalizada para el almacenamiento de big data, pero, aunque ambos son almacenes de datos, estos no son t\u00e9rminos intercambiables. Un data lake o \"lago de datos\" es un gran conjunto de datos en bruto, que todav\u00eda no tiene una finalidad definida. En cambio, un data warehouse o \"almac\u00e9n de datos\" es un dep\u00f3sito de datos que ya est\u00e1n estructurados y filtrados y han sido procesados para un prop\u00f3sito concreto.</p> Figura 1.8. Data Warehouse vs Data Lake 1 <p>A menudo se confunden estos dos tipos de almacenamiento de datos, pero son mucho m\u00e1s diferentes de lo que puede parecer a simple vista. De hecho, lo \u00fanico que tienen en com\u00fan es que contienen grandes cantidades de datos. Es importante realizar la distinci\u00f3n, ya que los data lake y los data warehouse atienden a diferentes prop\u00f3sitos, por lo que requieren un enfoque diferente para ser optimizados adecuadamente.</p> <p>As\u00ed, un data lake almacena datos sin procesar y que todav\u00eda no tienen una finalidad determinada. Sus usuarios finales son los cient\u00edficos de datos y su accesibilidad es elevada. Adem\u00e1s, en un data lake, justamente por esta f\u00e1cil accesibilidad, se pueden actualizar los datos r\u00e1pidamente.</p> Figura 1.9. Data Warehouse vs Data Lake 2. (Fuente: Huawei) <p>Por su lado, un data warehouse cuenta con datos procesados y que ya se est\u00e1n usando, por lo que tienen una finalidad concreta.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html","title":"UD 1 Gesti\u00f3n de Soluciones - Procedimientos y Mecanismos de Ingesta de Datos","text":"<p>En esta secci\u00f3n se profundizar\u00e1 en cada una de las etapas necesarias para dise\u00f1ar e implementar un almac\u00e9n de datos. Para ello, en primer lugar se describir\u00e1 el proceso de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL), fundamental para construir y alimentar un almac\u00e9n de datos. Despu\u00e9s, se desarrollar\u00e1n dos de los dise\u00f1os m\u00e1s extendidos de almac\u00e9n de datos: el dise\u00f1o en estrella y el dise\u00f1o en copo de nieve.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#1-el-proceso-de-extraccion-transformacion-y-carga-etl","title":"1. El proceso de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL)","text":"<p>El proceso ETL es el encargado de extraer, limpiar e integrar los datos provenientes de las fuentes de datos para alimentar el almac\u00e9n de datos. Este proceso tambi\u00e9n es el encargado de alimentar la capa de datos reconciliados en la arquitectura de tres capas. El proceso ETL tiene lugar cuando se puebla el almac\u00e9n de datos y se lleva a cabo cada vez que el almac\u00e9n de datos se actualiza. A continuaci\u00f3n, se describen detalladamente cada una de las fases de las que consta este proceso</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#11-extraccion","title":"1.1 \u2b06\ufe0f Extracci\u00f3n","text":"<p>Etapa que consiste en la lectura de los datos de las distintas fuentes de las que provienen. Cuando un almac\u00e9n de datos se rellena por primera vez, se suele utilizar la t\u00e9cnica de extracci\u00f3n est\u00e1tica, la cual consiste en extraer una instant\u00e1nea de los datos operacionales. A partir de entonces, se utiliza la extracci\u00f3n incremental para actualizar peri\u00f3dicamente los datos del almac\u00e9n de datos, recogiendo los cambios aplicados desde la \u00faltima extracci\u00f3n. Para ello, se utiliza el registro mantenido por el SGBD que, por ejemplo, asocia una marca de tiempo (timestamp) a los datos operacionales para registrar cuando fueron modificados y agilizar el proceso de extracci\u00f3n.</p> <p>En la actualidad, existe una gran cantidad de conjuntos de datos o data sets p\u00fablicos, conocidos bajo el nombre de Open Data, que abarcan una gran cantidad de dominios y con los que es posible trabajar para construir soluciones big data.  </p> <p>Open Data</p> <p>\ud83d\udccb Open Data: Se trata de datos que han sido generados por una fuente en particular, que abarcan un dominio tem\u00e1tico o disciplinar y tienen atributos, dentro de los cuales est\u00e1 la frecuencia de actualizaci\u00f3n. Adem\u00e1s, cuentan con una licencia espec\u00edfica que indica las condiciones de reutilizaci\u00f3n de los mismos.</p> <p>La fuente de los datos es en muchos de los casos el estado nacional, provincial, municipal u organizaciones comerciales. En otras ocasiones, la fuente de los datos es fruto del estudio o medici\u00f3n por parte de particulares. Los atributos de los conjuntos de datos deben especificar c\u00f3mo fueron obtenidos, incluyendo fechas de obtenci\u00f3n, actualizaci\u00f3n y validez, as\u00ed como el p\u00fablico involucrado, la metodolog\u00eda de recogida o muestreo, etc.</p> <p>Algunas de las fuentes m\u00e1s utilizadas en la actualidad para la obtenci\u00f3n de datos abiertos provienen de los centros nacionales e internacionales de estad\u00edsticas, como son el Instituto Nacional de Estad\u00edstica de Espa\u00f1a (INE) <sup>1</sup> , eurostat <sup>2</sup> , la oficina europea de estad\u00edsticas, la Organizaci\u00f3n Mundial de la Salud (OMS) <sup>3</sup> ... Por otra parte, existen repositorios p\u00fablicos de datos abiertos y a disposici\u00f3n de los usuarios como UCI4 <sup>4</sup> o Kaggle <sup>5</sup> , que es una comunidad de cient\u00edficos de datos donde empresas y organizaciones suben sus datos y plantean problemas que son resueltos por los miembros de la comunidad. La figura 1.8 muestra las fuentes de datos que se acaban de mencionar.</p> Figura 1.11: Fuentes para la obtenci\u00f3n de datos abiertos. (Fuente: Propia)"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#12-transformacion","title":"1.2 \ud83d\udd04 Transformaci\u00f3n","text":"<p>La etapa de transformaci\u00f3n es la fase clave para transformar los datos operativos en datos con un formato espec\u00edfico para alimentar un almac\u00e9n de datos. En esta etapa, los datos se limpian y se transforman, a\u00f1adi\u00e9ndoles contexto y significado. En caso de implementar un almac\u00e9n de datos siguiendo una arquitectura de tres capas, el proceso de transformaci\u00f3n es el encargado de obtener la capa de datos reconciliados. Si bien es cierto que algunos autores separan la limpieza y la transformaci\u00f3n de los datos en dos etapas distintas, en este cap\u00edtulo se considerar\u00e1n ambas dentro de la fase de transformaci\u00f3n.</p> <p>Transformaci\u00f3n</p> <p>\ud83d\udcc4 La etapa de transformaci\u00f3n engloba todos los procesos de limpieza y manipulaci\u00f3n de los datos, con el objetivo de transformar los datos operativos propios de sistemas relacionales (OLTP) en datos preparados para ser incluidos dentro del almac\u00e9n de datos (OLAP).</p> <p>La limpieza de los datos o data cleaning engloba todos aquellos procedimientos necesarios para detectar y resolver situaciones problem\u00e1ticas con los datos de partida que pudieran suponer problemas potenciales a la hora de analizarlos. As\u00ed pues, los datos de partida pueden ser incompletos, es decir, pueden contener atributos sin valor o valores agregados, incorrectos que incluyan errores o valores sin ning\u00fan significado, lo cual es com\u00fan cuando los datos se introducen manualmente en el sistema, o inconsistentes cuando los cambios no son propagados a todos los m\u00f3dulos del sistema, los rangos de un determinado atributo son cambiantes, existen datos duplicados...</p> <p>Otra tarea muy importante que se debe abordar en la fase de limpieza es la gesti\u00f3n de los datos perdidos. Se trata de datos que no est\u00e1n disponibles para algunas de las variables en alguna instancia. Esto puede ser debido, por ejemplo, a que no se registraran las modificaciones sufridas por los datos, se produjera un mal funcionamiento del equipo, los datos nunca fueron rellenados o no exist\u00edan en el momento en que fueron completados, se eliminaron por ser inconsistentes con la informaci\u00f3n registrada...</p> <p>En general, se distinguen dos tipos de situaciones cuando existen valores perdidos: datos perdidos completamente aleatorios y datos perdidos de forma no completamente aleatoria. En el segundo caso, puede ser interesante intentar analizar la raz\u00f3n de la p\u00e9rdida de los datos, la cual puede ser indicativa. En muchas ocasiones, los valores perdidos tienen relaci\u00f3n con un subconjunto de variables predictoras y no se encuentran aleatoriamente distribuidos por todas ellas. Por todo ello, las aproximaciones m\u00e1s comunes a la hora de gestionar datos perdidos son:</p> <ul> <li> <p>Eliminaci\u00f3n de instancias que contengan valores perdidos: consiste en establecer un porcentaje umbral de tal forma que, si una instancia contiene un n\u00famero de valores perdidos que supera este porcentaje, la instancia se descarta. Se trata de una t\u00e9cnica \u00fatil cuando el conjunto de datos es grande y el n\u00famero de instancias con valores perdidos no es muy elevado. No obstante, esta estrategia se debe utilizar con precauci\u00f3n, ya que puede suponer una excesiva p\u00e9rdida de informaci\u00f3n si el conjunto de datos no es muy grande, el n\u00famero de instancias con valores perdidos es alto o el porcentaje umbral es muy peque\u00f1o.</p> </li> <li> <p>Asignaci\u00f3n de valores fijos: consiste en asignar un valor fijo a todos los valores perdidos de todas las variables. Este valor puede ser el n\u00famero 0 o incluso un valor desconocido Unknown o no num\u00e9rico (NaN, Not a Number) en funci\u00f3n del lenguaje de programaci\u00f3n utilizado.</p> </li> <li> <p>Asignaci\u00f3n de valores de referencia: asigna un valor de referencia a los valores perdidos para cada variable. Estos valores de referencia suelen ser medidas de centralizaci\u00f3n como la media o la mediana de los valores de cada variable.</p> </li> <li> <p>Imputaci\u00f3n de valores perdidos: consiste en la aplicaci\u00f3n de t\u00e9cnicas m\u00e1s sofisticadas, como pueden ser t\u00e9cnicas estad\u00edsticas o de aprendizaje autom\u00e1tico para predecir o averiguar los valores que se han perdido.</p> </li> </ul> <p>Finalmente, la etapa de limpieza de datos tambi\u00e9n se encarga de la detecci\u00f3n de valores an\u00f3malos o outliers. Se trata de valores que se han introducido de forma err\u00f3nea o bien a una deformaci\u00f3n en la distribuci\u00f3n de valores. El proceso de detecci\u00f3n de anomal\u00edas consiste, fundamentalmente, en dos etapas: en primer lugar, asumir que existe un modelo generador de datos, como podr\u00eda ser una distribuci\u00f3n de probabilidad. En segundo lugar, considerar que las anomal\u00edas representan un modelo generador distinto, que no coincide con el original. Existen multitud de t\u00e9cnicas para detectar y descartar o imputar valores an\u00f3malos, como lo son t\u00e9cnicas estad\u00edsticas basadas en la desviaci\u00f3n y el rango intercuart\u00edlico o t\u00e9cnicas de aprendizaje autom\u00e1tico.</p> <p>Despu\u00e9s de la etapa de limpieza, comienza la etapa de transformaci\u00f3n/manipulaci\u00f3n. En ella, los datos se preparan para su carga en el almac\u00e9n de datos. Para ello, se transformar\u00e1n los datos provenientes de sistemas transaccionales OLTP en datos preparados para su an\u00e1lisis OLAP. A continuaci\u00f3n, se muestran algunos de los procesos de transformaci\u00f3n m\u00e1s comunes:</p> <ul> <li> <p>Estandarizaci\u00f3n de c\u00f3digos y formatos de representaci\u00f3n: incluye tareas como transformar la informaci\u00f3n EBCDIC a formato ASCII o Unicode, convertir n\u00fameros cardinales en ordinales o viceversa, homogeneizaci\u00f3n y tratamiento de fechas, expandir codificaciones a\u00f1adiendo textos descriptivos, unificaci\u00f3n de c\u00f3digos (sexo, estado civil, etc) y est\u00e1ndares (medidas, monedas, etc). </p> </li> <li> <p>Conversiones y combinaciones de campos: realizaci\u00f3n de agregaciones y c\u00e1lculos simples (importe neto, beneficio, realizaci\u00f3n de cuentas y promedios), c\u00e1lculos derivados con valores num\u00e9ricos y textuales... </p> </li> <li> <p>Correcciones: en caso de que no se pudiera hacer en el origen o en la etapa de limpieza, se deben corregir errores tipogr\u00e1ficos, datos sin sentido ni significado, resolver conflictos de dominio de variables, aclaraci\u00f3n de datos ambiguos y asignaci\u00f3n de valores a datos nulos.</p> </li> <li> <p>Integraci\u00f3n de varias fuentes: implica la resoluci\u00f3n de claves y utilizaci\u00f3n de diccionarios de datos y repositorios de metadatos para el dise\u00f1o del almac\u00e9n de datos.</p> </li> <li> <p>Eliminaci\u00f3n de datos y/o registros duplicados: frecuente cuando se trabaja con distintas fuentes de datos provenientes de diferentes dimensiones o departamentos de la organizaci\u00f3n. Para ello, es muy \u00fatil el uso de funciones de b\u00fasqueda y agrupamiento aproximado.</p> </li> <li> <p>Escalado y centrado: para reducir los efectos adversos de contar con datos dispersos, es muy \u00fatil utilizar t\u00e9cnicas para escalar y centrar los datos. Una posible transformaci\u00f3n consiste en restar a cada valor de cada instancia el valor medio de la variable correspondiente y despu\u00e9s dividir los valores por la desviaci\u00f3n est\u00e1ndar. De esta forma, los valores se escalan y se centran entorno al cero. Esta transformaci\u00f3n tambi\u00e9n puede hacerse entorno al valor de la media o la mediana. Tambi\u00e9n es posible escalar los valores en el intervalo [0\u22121] o aplicar el logaritmo, para reducir la dispersi\u00f3n de los datos y mejorar la visualizaci\u00f3n.</p> </li> <li> <p>Discretizaci\u00f3n: la aplicaci\u00f3n de muchas t\u00e9cnicas de aprendizaje autom\u00e1tico requiere que los valores de las variables sean discreto, en lugar de continuos. La discretizaci\u00f3n es el proceso por el cual se divide un intervalo de valores continuos en tantos fragmentos como etiquetas o valores discretos se quieran asignar, sustituyendo los valores originales por la etiqueta o valor discreto correspondiente.</p> </li> <li> <p>Selecci\u00f3n de caracter\u00edsticas: permite reducir el coste computacional de trabajar con todas las variables del conjunto de datos cuando, en muchas ocasiones, existen variables que no aportan informaci\u00f3n para el aprendizaje. Adem\u00e1s de reducir el coste computacional, se reduce el n\u00famero de dimensiones del conjunto de datos, mejorando la visualizaci\u00f3n y mejorando el rendimiento de los modelos no solo en t\u00e9rminos de tiempo, sino tambi\u00e9n de rendimiento, puesto que existen m\u00e9todos de aprendizaje cuyo rendimiento es peor cuando se ejecutan utilizando variables no significativas.</p> </li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#13-carga","title":"1.3 \ud83d\udd00 Carga","text":"<p>Se trata de la \u00faltima fase de cara a incluir datos en el almac\u00e9n de datos. La carga inicial de los datos puede requerir bastante tiempo al cargar de forma progresiva todos los datos hist\u00f3ricos, por lo que es normal realizarla en horas de baja carga de los sistemas. Una vez que el almac\u00e9n de datos ha sido inicialmente cargado, las sucesivas cargas de datos se pueden realizar de dos formas:</p> <ul> <li> <p>Refresco: El almac\u00e9n de datos se reescribe completamente, de forma que se reemplazan los datos antiguos. El refresco es utilizado habitualmente junto con la extracci\u00f3n est\u00e1tica y suele ser una estrategia muy utilizada para la carga inicial del almac\u00e9n de datos, aunque puede tambi\u00e9n realizarse a posteriori.</p> </li> <li> <p>Actualizaci\u00f3n: Se a\u00f1aden al almac\u00e9n de datos solamente aquellos datos nuevos que se pretenden incluir, sin eliminar ni modificar los datos ya existentes. Esta t\u00e9cnica es utilizada frecuentemente junto con la extracci\u00f3n incremental para actualizar regularmente el almac\u00e9n de datos. Se trata de una estrategia de carga compleja de dise\u00f1ar, ya que requiere que sea eficiente computacionalmente. Para ello, se requieren mecanismos de detecci\u00f3n del cambio como aquellos basados en la marca de tiempo (timestamp) o la utilizaci\u00f3n de tablas de log, entre otros.</p> </li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#14-frameworks-etl","title":"1.4 \u2b06\ufe0f\ud83d\udd04\ud83d\udd00 Frameworks ETL","text":"<p>Recientemente, han aparecido m\u00faltiples frameworks que ofrecen herramientas tecnol\u00f3gicas para dar un soporte integrado al proceso ETL. A continuaci\u00f3n, se describen algunos de los m\u00e1s utilizados:</p> <ul> <li> <p>Bubbles: Se trata de un framework ETL escrito en Python, aunque es posible utilizarlo desde otros lenguajes. Ofrece un marco de trabajo sencillo, donde el proceso ETL se describe como una secuencia de nodos conectados que representan los datos y las distintas operaciones que se pueden realizar con ellos. De esta forma, es posible construir un grafo que implemente el proceso ETL completo para un conjunto de datos.</p> </li> <li> <p>Apache Camel: Este framework escrito en lenguaje Java y de acceso abierto se enfoca en facilitar la integraci\u00f3n entre distintas fuentes de datos, haciendo el proceso m\u00e1s accesible a los desarrolladores, ofreciendo distintas herramientas para dar soporte al proceso ETL.</p> </li> <li> <p>Keetle: Es el framework de Pentaho para dar soporte al proceso ETL. Pentaho es una suite o conjunto de programas para construir soluciones de inteligencia de negocio y Keetle es su entorno de trabajo ETL. Similar a Bubbles, ofrece un entorno de trabajo sencillo para implementar un proceso ETL a trav\u00e9s de nodos y conexiones entre ellos. Adem\u00e1s, Keetle es de acceso abierto.</p> </li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#2-diseno-en-estrella","title":"2. Dise\u00f1o en Estrella","text":"<p>A la hora de dise\u00f1ar un almac\u00e9n de datos, existen dos alternativas ampliamente utilizadas: el dise\u00f1o en estrella, que promueve el dise\u00f1o directo de estructuras l\u00f3gicas sobre el modelo relacional, y el dise\u00f1o en copo de nieve, como variante del dise\u00f1o en estrella.</p> <p>El proceso de desarrollo de un almac\u00e9n de datos siguiendo el dise\u00f1o en estrella puede estructurarse seg\u00fan las siguientes fases:</p> <ol> <li> <p>Elegir un proceso de negocio a modelar: cualquier proceso de negocio puede ser modelado como un almac\u00e9n de datos. No obstante, ser\u00e1n de especial inter\u00e9s aquellos procesos necesarios para la toma de decisiones y que involucran a diferentes unidades de la organizaci\u00f3n.</p> </li> <li> <p>Escoger la granularidad del proceso de negocio: Los datos almacenados en el almac\u00e9n de datos deben expresarse siempre al mismo nivel de detalle. Por este motivo es necesario escoger un nivel de granularidad al comienzo del dise\u00f1o del almac\u00e9n de datos.</p> </li> <li> <p>Selecci\u00f3n de medidas/hechos: indican qu\u00e9 se necesita medir o evaluar enel proceso de negocio con el fin de dar respuesta a las necesidades de informaci\u00f3n y toma de decisiones por las cuales se pretende dise\u00f1ar el almac\u00e9nde datos. Por ejemplo, en el \u00e1mbito log\u00edstico las medidas podr\u00edan ser las unidades aceptadas odevueltas, mientras que en el \u00e1mbito del control de calidad podr\u00edan ser lasunidades producidas, defectuosas, costo de la producci\u00f3n.</p> </li> <li> <p>Elegir las dimensiones que se aplicar\u00e1n a cada medida o hecho: las dimensiones especifican las distintas propiedades de las medidas o hechos que se pretenden almacenar e integrar dentro del almac\u00e9n de datos. Por ejemplo, si las medidas seleccionadas est\u00e1n relacionadas con las ventas de una determinada empresa, las dimensiones podr\u00edan ser: fecha de la venta, vendedor ,cliente, producto...</p> </li> </ol> <p>A la hora de dise\u00f1ar el almac\u00e9n de datos siguiendo el dise\u00f1o en estrella, se crea una tabla central, tambi\u00e9n llamada tabla de hechos, tabla factual o fact table. Esta tabla es la que posee los datos (medidas o hechos) sobre las diferentes combinaciones de las dimensiones. En algunas ocasiones, un dise\u00f1o en estrella presenta m\u00e1s de una tabla de hechos. Los hechos introducidos pueden ser de distintos tipos: completamente aditivos (total de ventas de un departamento), no aditivos (m\u00e1rgen de beneficios expresado como porcentaje) o semiaditivos (como datos intermedios que pueden agregarse con datos de otras dimensiones).</p> <p>Rodeando a la tabla de hechos, se encuentra una tabla por cada una de las dimensiones definidas. La clave primaria de la tabla de hechos se crea combinando las claves primarias de sus dimensiones relacionadas, de forma que est\u00e1 compuesta por las claves ajenas de las tablas de dimensiones que relaciona. De esta forma, la tabla de hechos presenta una relaci\u00f3n del tipo \u201cmuchos a muchos\u201d entre todas las tablas de dimensiones que relaciona, a la vez que una relaci\u00f3n \u201cmuchos a uno\u201d con cada tabla de dimensi\u00f3n por separado. Este esquema con forma de estrella da nombre a este tipo de dise\u00f1o.</p> <p>Ejemplo</p> <p>Sea un almac\u00e9n de datos en el que se pretende almacenar las ventas llevadas a cabo en una organizaci\u00f3n. La tabla central de hechos representa los datos de cada venta, mientras que las dimensiones que rodean a la tabla central recogen datos sobre los productos vendidos, la fecha de la venta, el canal de distribuci\u00f3n y el lugar de entrega. La figura1.12 muestra un ejemplo de este esquema de almac\u00e9n de datos.</p> Figura 1.12: Ejemplo de almac\u00e9n de datos dise\u00f1ado en estrella. (Fuente: UCLM) <p>Ventajas e incovenientes</p> <p>Entre las ventajas del dise\u00f1o en estrella, destaca ser un dise\u00f1o f\u00e1cil de modificar que proporciona tiempos r\u00e1pidos de respuesta, simplificando la navegaci\u00f3n de los medatadatos. Adem\u00e1s, este dise\u00f1o permite simular vistas de los datos que obtendr\u00e1n los usuarios finales y facilita la interacci\u00f3n con otras herramientas.</p> <p>Sin embargo, el dise\u00f1o en estrella presenta algunos inconvenientes que surgen, por ejemplo, cuando se combinan dimensiones con distintos niveles de detalle. Adem\u00e1s, cuando se pretende limitar los niveles de una dimensi\u00f3n se debe a\u00f1adir un campo de nivel o utilizar un modelo de tipo constelaci\u00f3n, donde se incluyen diferentes estrellas que almacenan tablas de hechos agregadas, lo cual a\u00f1ade complejidad al esquema. </p> <p>Resumen</p> <p>\ud83d\udcc4 El dise\u00f1o en estrella permite crear un almac\u00e9n de datos con una estructura centralizada. El dise\u00f1o se compone de una tabla de hechos central, que recoge los valores de las medidas del proceso de negocio que se pretende modelar. Rodeando a la tabla de hechos, se incluyen tantas tablas como dimensiones se hayan especificado en el modelo, las cuales se relacionan entre s\u00ed a trav\u00e9s de la tabla de hechos.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/2_proced_mecan_ingesta_datos.html#3-diseno-en-copo-de-nieve","title":"3. Dise\u00f1o en Copo de Nieve","text":"<p>Otro de los dise\u00f1os m\u00e1s extendidos a la hora de implementar un almac\u00e9n de datos es el dise\u00f1o en copo de nieve. De hecho, el dise\u00f1o de copo de nieve es considerado una variante o derivado del dise\u00f1o en estrella y, por tanto, puede construirse a partir de este siguiendo las mismas etapas que se describieron en la secci\u00f3n anterior. </p> <p>Este dise\u00f1o es muy utilizado cuando se dispone de tablas de dimensi\u00f3n con una gran cantidad de atributos. En esta circunstancia, el dise\u00f1o de copo de nieve permite normalizar las tablas de dimensi\u00f3n, ofreciendo un mejor rendimiento cuando las consultas realizadas sobre el almac\u00e9n de datos requieren del uso de operadores de agregaci\u00f3n. De esta forma, la tabla de hechos deja de ser la \u00fanica tabla que presenta relaciones con las dem\u00e1s, apareciendo nuevas relaciones que se dan entre las tablas normalizadas que componen las tablas de dimensiones. Este esquema, que incluye ramificaciones en cada dimensi\u00f3n que se corresponden con las tablas necesarias para su normalizaci\u00f3n, guarda un gran parecido con la estructura de un copo de nieve, lo cual da nombre a este dise\u00f1o.</p> <p>La diferencia entre los dise\u00f1os en estrella y copo de nieve radica fundamentalmente en la modelizaci\u00f3n de las tablas de dimensiones. Para obtener un dise\u00f1o en copo de nieve, basta con partir de un dise\u00f1o en estrella en el que, tras un proceso de normalizaci\u00f3n, se obtienen varias tablas de dimensi\u00f3n a partir de una tabla desnormalizada. Dentro del dise\u00f1o de copo de nieve, es posible distinguir entre copo de nieve parcial, en el que solo algunas de las dimensiones son normalizadas y copo de nieve completo, en el que todas las dimensiones del esquema son normalizadas.</p> <p>Siguiendo con el ejemplo anterior, en el que se mostraba un almac\u00e9n de datos para representar las ventas realizadas por una organizaci\u00f3n, la figura1.13 muestra un dise\u00f1o para un almac\u00e9n de datos siguiendo un esquema de copo de nieve parcial, en el que las dimensiones geogr\u00e1fica y producto han sido normalizadas. </p> Figura 1.13: Ejemplo de almac\u00e9n de datos dise\u00f1ado en copo de nieve. (Fuente: UCLM) <p>Ventajas e Inconvenientes</p> <p>La normalizaci\u00f3n de las tablas de dimensiones proporciona al dise\u00f1o de copo de nieve la mejora de la eficiencia en la realizaci\u00f3n de consultas complejas que requieren el uso de operadores de agregaci\u00f3n. Sin embargo, este dise\u00f1o es conceptualmente m\u00e1s dif\u00edcil de implementar, ya que incluye un mayor n\u00famero de tablas y requiere de realizar muchos joins entre ellas, lo que incrementa los tiempos de consulta.</p> <p>En ocasiones, se requiere dise\u00f1ar un almac\u00e9n de datos que contenga m\u00e1s de una tabla de hechos. Para ello, el esquema o dise\u00f1o en constelaci\u00f3n permite crear un almac\u00e9n de datos de forma similar al dise\u00f1o en estrella, con la diferencia de que este modelo incluye m\u00e1s de una tabla de hechos. Adem\u00e1s, cada tabla de hechos puede vincularse con todas o solo algunas tablas de dimensiones. Este esquema contribuye a la reutilizaci\u00f3n de las tablas de dimensiones, las cuales se pueden relacionar con m\u00e1s de una tabla de hechos.</p> <p>Resumen</p> <p>\ud83d\udcc4 El dise\u00f1o en copo de nieve permite crear un almac\u00e9n de datos a partir de un dise\u00f1o en estrella. Para ello, las tablas de dimensiones se normalizan, generando m\u00e1s de una tabla para cada una de las dimensiones, mejorando la eficiencia de consultas complejas que requieren el uso de operadores avanzados.</p> <ol> <li> <p>https://www.ine.es \u21a9</p> </li> <li> <p>https://ec.europa.eu/eurostat \u21a9</p> </li> <li> <p>https://www.who.int/es/data/gho/publications/world-health-statistics \u21a9</p> </li> <li> <p>https://archive.ics.uci.edu/ \u21a9</p> </li> <li> <p>https://www.kaggle.com \u21a9</p> </li> </ol>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html","title":"UD 1 Gesti\u00f3n de Soluciones - Formato de Datos Adecuado para el Almacenamiento","text":"<p>Este cap\u00edtulo profundiza en las soluciones de almacenamiento y gesti\u00f3n de datos propias de las bases de datos NoSQL para dar soluci\u00f3n al almacenamiento de datos para los sistemas Big Data.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#1-teorema-de-cap","title":"1. Teorema de CAP","text":"<p>Para entender las diferencias entre estos tipos de bases de datos, piense en el teorema CAP, una serie de principios aplicados a los sistemas distribuidos que almacenan el estado. En la siguiente figura se muestran las tres propiedades del teorema CAP.</p> Figura 1.14: Teorema de CAP. (Fuente: Microsoft) <p>El teorema indica que los sistemas de datos distribuidos ofrecer\u00e1n un equilibrio entre consistencia, disponibilidad y tolerancia a particiones. Adem\u00e1s, cualquier almac\u00e9n de datos solo puede garantizar dos de las tres propiedades:</p> <ul> <li>Consistencia(C). Es la capacidad del sistema que garantiza devolver el \u00faltimo valor que ha sido escrito a cualquier operaci\u00f3n de lectura. Por ejemplo, si actualizamos un registro con valor 2 a valor 3, las bases de datos y sistemas consistentes deber\u00e1n devolver a todos los lectores el nuevo valor 3 a partir de ese momento. Cualquier otro resultado es considerado como una violaci\u00f3n de la consistencia.</li> <li>Disponibilidad(A). Capacidad del sistema de garantizar el procesamiento de cualquier operaci\u00f3n de lectura sin producir ning\u00fan error..</li> <li>Tolerancia a la partici\u00f3n(T). Capacidad del sistema de garantizar su funcionamiento en caso de partici\u00f3n de la red o incomunicaci\u00f3n entre sus nodos. Para ello, el retraso o la p\u00e9rdida de mensajes de comunicaci\u00f3n no deber\u00e1n impedir que el sistema funcione y responda a las peticiones.El sistema es tolerante a las particiones si los mensajes que se pierden o se retrasan no impiden su funcionamiento.</li> </ul> Figura 1.15: Teorema de CAP 2. (Fuente: medium.baqend.com) <p>El teorema usa estas definiciones para afirmar que no es posible ofrecer las tres garant\u00edas de forma simult\u00e1nea. O lo que es lo mismo, en cada instante se debe sacrificar alguna de ellas para garantizar las otras dos.</p> Figura 1.16: Teorema de CAP 3. (Fuente: medium.baqend.com) <p>Como aplicaci\u00f3n pr\u00e1ctica, todos los sistemas distribuidos como las bases de datos sufren fallos e interrupciones de red y es necesario que sigan funcionando. Por esta raz\u00f3n, los sistemas deben elegir entre ser consistentes o disponibles y balancear estas dos garant\u00edas.</p> <p>Podemos observarlo en la figura anterior, donde cuando se produce una partici\u00f3n de red, se debe renunciar a la coherencia o a la disponibilidad. Para la consistencia, se bloquea la respuesta hasta que llegue ACK, sin embargo, para la disponibilidad, la respuesta se da antes de que la replicaci\u00f3n sea exitosa.</p> Figura 1.17: Teorema de CAP 4. (Fuente: medium.baqend.com) <p>Si un sistema favorece la consistencia frente a la disponibilidad en el caso de una partici\u00f3n de red, ser\u00e1 CP, mientras que si favorece la disponibilidad en este caso ser\u00e1 AP. Los sistemas ACID como las bases de datos relacionales, eligen la consistencia sobre disponibilidad, mientras que los sistemas BASE eligen la disponibilidad.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#2-bases-de-datos-nosql","title":"2. Bases de datos NoSQL","text":"<p>El t\u00e9rmino NoSQL proviene del ingl\u00e9s y est\u00e1 formado por las palabras \"No\" y \"SQL\". El significado literal del t\u00e9rmino no debe confundir al lector ni alejarlo de su correcto significado. NoSQL no engloba solamente un conjunto de bases de datos y sistemas de almacenamiento que no utilizan SQL para su gesti\u00f3n. El significado de NoSQL ampl\u00eda lo anterior, siendo el acr\u00f3nimo de \"Not only SQL\". Por tanto, NoSQL es una categor\u00eda general de sistemas de bases de datos que, adem\u00e1s de SQL, utilizan otra serie de tecnolog\u00edas para almacenar y gestionar los datos. El t\u00e9rmino NoSQL fue acu\u00f1ado en 1998 por Carlo Strozzy y retomado en el a\u00f1o 2009 por Eric Evans, quien aprovech\u00f3 este t\u00e9rmino para referirse a esta nueva generaci\u00f3n de familias de bases de datos como \"Big Data\".</p> Figura 1.18: Ecosistema NoSQL. (Fuente: medium.baqend.com)"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#21-nosql-vs-relacional-sql","title":"2.1 NoSQL vs Relacional (SQL)","text":"<p>Las bases de datos relacionales o RDBMS se usan generalmente para almacenar datos estructurados, en los que su formato, esquema, tipos y relaciones son fijas y sufren pocos cambios. Cada columna, define un aspecto del registro.</p> <p>Las transacciones realizadas en los sistemas de bases de datos relacionales garantizan la consistencia y escalabilidad de las operaciones a trav\u00e9s de una serie de caracter\u00edsticas especificadas en el modelo ACID.</p> <ul> <li> <p>Atomicidad (A): Garantiza que una transacci\u00f3n se ha realizado completamente o no, evitando que algunas operaciones se ejecuten a medias.</p> </li> <li> <p>Consistencia (C): Tambi\u00e9n conocida como integridad, asegura que cualquier transacci\u00f3n realizada desde un estado seguro en la base de datos conducir\u00e1 a esta a otro estado tambi\u00e9n seguro o v\u00e1lido.</p> </li> <li> <p>Aislamiento (I: Isolation): Permite que el resultado en el estado del sistema sea el mismo, independientemente del modo de ejecuci\u00f3n de las transacciones (secuencial o concurrente).</p> </li> <li> <p>Durabilidad (D): O persistencia, garantiza que a pesar de que se produzca un fallo en el sistema, los cambios realizados por una transacci\u00f3n persistir\u00e1n.</p> </li> </ul> <p>Algunos ejemplos de bases de datos relacionales o SQL son SQL Server, MySQL, PostgreSQL, DB2 y Oracle.</p> <p>Por otro lado, las bases de datos NoSQL han venido para quedarse. Simplemente son capaces de aportar unas caracter\u00edsticas de escalabilidad y flexibilidad que las bases de datos tradicionales no han podido satisfacer de forma sencilla. Alternativamente, las bases de datos NoSQL siguen el modelo conocido como BASE, m\u00e1s flexible que ACID en aquellos sistemas de bases de datos que no se adhieren estrictamente al modelo relacional.</p> <ul> <li> <p>Disponibilidad b\u00e1sica (Basic Availability): El sistema siempre ofrece una respuesta a una petici\u00f3n de datos, aunque los datos sean inconsistentes, est\u00e9n en fase de cambio o incluso se produzca un fallo.</p> </li> <li> <p>Soft-state (S): La consistencia de este tipo de bases de datos es eventual, por lo que el estado del sistema cambia a lo largo del tiempo, aunque no haya habido entradas de datos.</p> </li> <li> <p>Eventual Consistency (E): Cuando se dejan de recibir datos, el sistema se vuelve consistente. As\u00ed pues, los datos se propagan a todo el sistema, pero este contin\u00faa recibiendo datos sin evaluar la consistencia de los mismos para cada transacci\u00f3n antes de avanzar a la siguiente.</p> </li> </ul> <p>Info</p> <p>La Computaci\u00f3n o Inform\u00e1tica ubicua (ubicomp) es un concepto en ingenier\u00eda de software y las ciencias de la computaci\u00f3n. Es entendida como la integraci\u00f3n de la inform\u00e1tica en el entorno de la persona, de forma que los ordenadores no se perciban como objetos diferenciados, apareciendo en cualquier lugar y en cualquier momento. Es el concepto que determina por primera vez que el hombre no se debe adaptar a la m\u00e1quina, sino la m\u00e1quina al hombre.</p> <p>En la actualidad, las aplicaciones web modernas, el auge de la computaci\u00f3n ubicua y el Big Data, presentan desaf\u00edos muy diferentes a los que presentan los sistemas de informaci\u00f3n tradicionales, implementados por medio de sistemas de bases de datos relacionales. Algunos desaf\u00edos son: el procesamiento masivo de datos, la alta frecuencia de lecturas y escrituras, los cambios din\u00e1micos y frecuentes en el esquema de datos, la escalabilidad a costes razonables, gesti\u00f3n de datos temporales...En este contexto, en los \u00faltimos a\u00f1os han aparecido multitud de enfoques, modelos y tecnolog\u00edas para aportar soluciones a los desaf\u00edos mencionados anteriormente. En este cap\u00edtulo, se profundiza en las bases de datos XML, bases de datos documentales y orientadas a grafos.</p> <p>Base de datos NoSQL</p> <p>\ud83d\udcc4 Base de datos NoSQL: Sistema de bases de datos no relacional que utiliza distintas tecnolog\u00edas para la gesti\u00f3n y almacenamiento distribuido de datos masivos sin un esquema estricto subyacente. Esta familia de base de datos permite incrementar la disponibilidad y escalabilidad del sistema, mejorando su rendimiento.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#22-tipos-de-bases-de-datos-nosql","title":"2.2 Tipos de bases de datos NoSQL","text":"Figura 1.19: Tipos BBDD NoSQL. (Fuente: medium.baqend.com)"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#221-documentales","title":"2.2.1 Documentales","text":"<p>Estas bases de datos almacenan datos autocontenidos llamados documentos. Cada documento puede definir su propio esquema. Estos documentos generalmente tienen formato JSON (aunque tambi\u00e9n existen otros como YAML o BSON, o incluso como un fichero de texto). Asignan a cada documento un identificador \u00fanico y no es necesario definir relaciones entre ellos.</p> Figura 1.20: NoSQL Documentales. (Fuente: medium.baqend.com) <p>Algunos ejemplos de Base de datos NoSQL de tipo documental son MongoDB, CouchDB y DynamoDB.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#222-clave-valor","title":"2.2.2 Clave-valor","text":"<p>Estas bases de datos tampoco tienen un esquema de datos predefinido. Se asimilan al concepto de array o map persistente y a las bases de datos con dos columnas, la primera es la clave y la segunda el valor del registro. Los valores pueden tomar el tipo de dato que sea necesario, incluso JSON.</p> <p>La ventaja de estas bases de datos es su gran velocidad de lectura y de escritura debido a su estructura. La desventaja es su poca capacidad para filtrar la informaci\u00f3n en las consultas.</p> Figura 1.21: NoSQL clave valor. (Fuente: medium.baqend.com) <p>Algunos ejemplos de Base de datos NoSQL de tipo clave-valor son DynamoDB, Redis.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#223-tabular","title":"2.2.3 Tabular","text":"<p>Estas bases de datos orientadas a columnas organizan los datos de esta forma en vez de en filas y almacenan cada columna de forma separada. De esta forma, es muy sencillo y eficiente a\u00f1adir nuevas columnas y realizar operaciones de b\u00fasqueda sobre ellas. Estas bases de datos usan el concepto de espacio de claves que contiene todas las familias de columnas. Dentro de una familia de columnas, el n\u00famero de columnas de cada fila puede variar</p> <p>Son muy populares en sistemas de anal\u00edtica ya que funcionan de forma similar a lo que lo har\u00edan las tablas en bases de datos relacionales. Adem\u00e1s, aportan una excelente escalabilidad y una compresi\u00f3n eficiente de los datos. Por otro lado, estas bases de datos son poco eficientes al consultar filas individuales y registros repartidos en varias columnas.</p> Figura 1.22: NoSQL Tabular. (Fuente: medium.baqend.com) <p>Algunos ejemplos de Base de datos NoSQL de tipo tabular son Cassandra, HBase, Druid o BigTable.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#224-grafo","title":"2.2.4 Grafo","text":"<p>En estas bases de datos, los registros son almacenados con una estructura en forma de grafo. Las relaciones interconectan los datos entre s\u00ed y los distribuyen.</p> <p>Su estructura consiste en dos componentes principales. Los nodos son los datos en s\u00ed mismos y los enlaces o aristas explican las relaciones entre dos nodos. Estos enlaces tambi\u00e9n pueden contener informaci\u00f3n como la naturaleza de la relaci\u00f3n o incluso direcci\u00f3n para describir el flujo.</p> Figura 1.23: NoSQL Grafo. (Fuente: medium.baqend.com) <p>Algunos ejemplos de Base de datos NoSQL de tipo grafo son Neo4J, DEX, InfiniteGraph.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#3-bases-de-datos-xml","title":"3. Bases de datos XML","text":"<p>El lenguaje XML (eXtensible Markup Language) es un lenguaje de marcas extensible derivado del lenguaje SGML (Standard Generalized Markup Language). Se trata de un lenguaje ideado para la definici\u00f3n y gesti\u00f3n de documentos que permite representar datos estructurados y semi-estructurados. En la actualidad, XML se ha convertido en un formato est\u00e1ndar para la comunicaci\u00f3n entre aplicaciones, facilitando su integraci\u00f3n. Entre las principales ventajas de XML destacan:</p> <ul> <li> <p>Datos autodocumentados: Gracias a la posibilidad de definir cualquier tipo de etiquetas, no es necesario conocer el esquema para entender el significado de los datos. </p> </li> <li> <p>Extensi\u00f3n: Permite adaptar el lenguaje f\u00e1cilmente a cualquier dominio. Anidamiento: La definici\u00f3n de estructuras anidadas dota a los documentos de gran flexibilidad para transferir y consultar informaci\u00f3n. </p> </li> <li> <p>Flexibilidad: Los documentos XML no tienen un formato estricto ni r\u00edgido, permitiendo a\u00f1adir o ignorar informaci\u00f3n en ellos. </p> </li> </ul> <p>Estructura de un documento XML</p> <p>La estructura de un documento XML est\u00e1 compuesta de elementos. Un elemento es una porci\u00f3n de documento delimitado por un par de etiquetas &lt; &gt; (apertura) y &lt; /&gt; (cierre) entre las cuales se incluye un texto llamado contexto del elemento. Un elemento sin contexto puede abreviarse mediante el uso de una \u00fanica etiquete . La estructura de un XML se forma anidando elementos para especificar la estructura del documento, formando una estructura de \u00e1rbol. El fragmento de c\u00f3digo 3.1 representa la estructura b\u00e1sica de un documento XML que representa un listado de asignaturas.</p> <p><pre><code>&lt;Asignaturas_Primero&gt;\n    &lt;Asignatura&gt;\n        &lt;titulo&gt; C\u00e1lculo &lt;/titulo&gt;\n        &lt;tipo&gt; Anual &lt;/tipo&gt;\n        &lt;profesor&gt;\n            &lt;nombre&gt; Ana &lt;/nombre&gt;\n            &lt;apellido&gt; Sanz &lt;/apellido&gt;\n        &lt;/profesor&gt;\n        &lt;profesor&gt;\n            &lt;nombre&gt; Roberto &lt;/nombre&gt;\n            &lt;apellido&gt; Hern\u00e1ndez &lt;/apellido&gt;\n        &lt;/profesor&gt;\n    &lt;/Asignatura&gt;\n    &lt;Asignatura&gt;\n        &lt;titulo&gt; F\u00edsica &lt;/titulo&gt;\n        &lt;tipo&gt; Semestral &lt;/tipo&gt;\n        &lt;profesor&gt;\n            &lt;nombre&gt; Carmelo &lt;/nombre&gt;\n            &lt;apellido&gt; Moya &lt;/apellido&gt;\n        &lt;/profesor&gt;\n    &lt;/Asignatura&gt;\n&lt;/Asignaturas_Primero&gt;\n</code></pre> Listado 1.1: Listado de Asignaturas</p> <p>Los elementos de un documento pueden contener atributos. Los atributos se incluyen en la definici\u00f3n del elemento, dentro de la etiqueta de apertura. Sint\u00e1cticamente, se representan mediante un par nombre = valor. A diferencia de un subelemento, un atributo solo puede aparecer una vez en una etiqueta. El uso de atributos y su diferencia con respecto a los elementos no siempre es evidente. A nivel de documento, los atributos se introducen para a\u00f1adir texto que no se visualizar\u00e1 en el documento, mientras que los subelementos formar\u00e1n parte del contenido del documento. A nivel de datos, la diferencia es insustancial, puesto que la misma informaci\u00f3n se puede representar utilizando atributos o subelementos. Por ejemplo, el subelemento &lt; tipo &gt; de una asignatura, podr\u00eda representarse como un atributo del elemento asignatura. El listado 1.2 representa la asignatura de C\u00e1lculo del ejemplo anterior, incluyendo el t\u00edtulo de la asignatura como atributo de la misma.</p> <p><pre><code>&lt;Asignaturas_Primero&gt;\n    &lt;Asignatura&gt;\n        &lt;titulo&gt; C\u00e1lculo tipo = 'anual' &lt;/titulo&gt;\n        &lt;tipo&gt; Anual &lt;/tipo&gt;\n        &lt;profesor&gt;\n            &lt;nombre&gt; Ana &lt;/nombre&gt;\n            &lt;apellido&gt; Sanz &lt;/apellido&gt;\n        &lt;/profesor&gt;\n        &lt;profesor&gt;\n            &lt;nombre&gt; Roberto &lt;/nombre&gt;\n            &lt;apellido&gt; Hern\u00e1ndez &lt;/apellido&gt;\n        &lt;/profesor&gt;\n    &lt;/Asignatura&gt;\n&lt;/Asignaturas_Primero&gt;\n</code></pre> Listado 1.2: Uso de atributos para la definici\u00f3n de asignaturas</p> <p>Dado que una de las principales aplicaciones de XML es el intercambio de informaci\u00f3n entre organizaciones, y ante la flexibilidad del lenguaje que permite definir cualquier nombre para una etiqueta, es posible que existan conflictos entre los nombres de \u00e9stas. Para ello, se recurre a los espacios de nombres, los cuales permiten a las organizaciones especificar nombres de etiquetas \u00fanicos. De esta forma, se a\u00f1ade al nombre de la etiqueta o atributo un identificador de recursos universal. El listado muestra de forma resumida un ejemplo de utilizaci\u00f3n del espacio de nombres para la definici\u00f3n de asignaturas.</p> <p><pre><code>&lt;Asignatura xmlns:AS=\"http://www.listado-asignaturas.com\"&gt;\n    &lt;AS:titulo&gt; C\u00e1lculo tipo = 'anual' &lt;/AS:titulo&gt;\n    &lt;AS:tipo&gt; Anual &lt;/AS:tipo&gt;\n    &lt;AS:profesor&gt;\n        &lt;AS:nombre&gt; Ana &lt;/AS:nombre&gt;\n        &lt;AS:apellido&gt; Sanz &lt;/AS:apellido&gt;\n    &lt;/AS:profesor&gt;\n    &lt;AS:profesor&gt;\n        &lt;AS:nombre&gt; Roberto &lt;/AS:nombre&gt;\n        &lt;AS:apellido&gt; Hern\u00e1ndez &lt;/AS:apellido&gt;\n    &lt;/AS:profesor&gt;\n&lt;/Asignatura&gt;\n</code></pre> Listado 1.3: Uso de un espacio de nombres</p> <p>Al igual que ocurre en las bases de datos relacionales, las bases de datos XML presentan una serie de lenguajes de definici\u00f3n de tipos de documentos XML. Concretamente, a continuaci\u00f3n se profundizaran en DTD y XML Schema. Seguidamente, se estudiar\u00e1n los lenguajes XPath, XSLT y XQuery, como lenguajes de manipulaci\u00f3n de datos XML para la definici\u00f3n y ejecuci\u00f3n de consultas.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#31-definition-type-document-dtd","title":"3.1 Definition Type Document (DTD)","text":"<p>En XML, se dice que un documento est\u00e1 bien formado si cumple con la sintaxis del lenguaje: correcta definici\u00f3n de elementos y atributos, correcto anidamiento...Como se puede apreciar, este concepto alude a la dimensi\u00f3n sint\u00e1ctica de los documentos.</p> <p>Por otra parte, se dice que un documento XML es v\u00e1lido cuando est\u00e1 bien formado y cumple con la estructura de documento que ha sido dada a trav\u00e9s de un lenguaje de definici\u00f3n de datos. Esta definici\u00f3n de la estructura de los documentos XML es opcional, si bien es cierto que es recomendable, especialmente cuando se trabaja con documentos complejos y con un volumen grande de ellos. La definici\u00f3n de tipos de documento o DTD permite definir la estructura de un documento XML. Para ello, la DTD especifica la estructura de los elementos y atributos de un documento: qu\u00e9 elementos pueden aparecer, qu\u00e9 atributos, qu\u00e9 subelementos, multiplicidad de ellos... Existen multitud de validadores on-line de documentos XML y sus correspondientes DTDs y/o XML Schemas<sup>1</sup>.</p> <p>La declaraci\u00f3n de una DTD dentro de un documento XML puede implementarse de dos formas: en primer lugar, de manera interna. En este caso, se a\u00f1ade la DTD despu\u00e9s del pre\u00e1mbulo XML y justo antes del documento propiamente dicho. El listado 1.4 muestra un ejemplo de declaraci\u00f3n interna de una DTD.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\" ?&gt;\n&lt;!DOCTYPE Elemento_Raiz [Aqu\u00ed la DTD]&gt;\n&lt;!-------Aqu\u00ed va el documento XML-----&gt;\n</code></pre> Listado 1.4: Declaraci\u00f3n Interna DTD</p> <p>En segundo lugar, la DTD puede declararse de forma externa, definiendo la misma en un fichero .DTD aparte que despu\u00e9s es enlazado en el documento. El listado 1.5 muestra un ejemplo de declaraci\u00f3n externa de una DTD.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\" ?&gt;\n&lt;!DOCTYPE Elemento_Raiz SYSTEM \"Mi_DTD.dtd\"&gt;\n&lt;!-------Aqu\u00ed va el documento XML------&gt;\n</code></pre> Listado 1.5: Declaraci\u00f3n Externa DTD</p> <p>Siguiendo con el ejemplo del listado 1.1, el listado 1.6 muestra el archivo .dtd que validar\u00eda la estructura del documento de asignaturas.</p> <p><pre><code>&lt;\u00a1 DOCTYPE Listado_Asignaturas[\n&lt;! ELEMENT Asignaturas_Primero (Asignatura+)&gt;\n&lt;! ELEMENT Asignatura (titulo, tipo, profesor+)&gt;\n&lt;! ELEMENT titulo (#PCDATA)&gt;\n&lt;! ELEMENT tipo (#PCDATA)&gt;\n&lt;! ELEMENT profesor (nombre, apellido)&gt;\n&lt;! ELEMENT nombre (#PCDATA)&gt;\n&lt;! ELEMENT apellido (#PCDATA)&gt;\n</code></pre> Listado 1.6: DTD Listado de Asignaturas</p> <p>En el listado anterior, se define en primer lugar el tipo de documento \"Listado_Asignaturas\". A continuaci\u00f3n, y especificados entre corchetes, se describen los elementos que contendr\u00e1 el documento, comenzando por el elemento ra\u00edz \"Asignaturas_Primero\". Este elemento estar\u00e1 formado por uno o m\u00e1s elementos \"Asignatura\" (especificado mediante el operador +). Seguidamente, el elemento \"Asignatura\" viene dado por un conjunto de tres subelementos: t\u00edtulo, tipo y profesor. Adem\u00e1s, cada asignatura puede tener uno o m\u00e1s profesores. Los elementos t\u00edtulo y tipo vienen dados por cadenas de caracteres (denotadas con la instrucci\u00f3n #PCDATA) mientras que el elemento profesor estar\u00e1 formado por dos subelementos llamados \"nombre\" y \"apellido\" ambos definidos mediante cadenas de caracteres.</p> <p>La definici\u00f3n de atributos para un elemento se hace a continuaci\u00f3n de la definici\u00f3n del mismo utilizando &lt;!ATTLIST &gt;. El listado 1.7 muestra la DTD anterior en caso de que el tipo de asignatura sea considerado un atributo. </p> <p><pre><code>&lt;\u00a1 DOCTYPE Listado_Asignaturas[\n&lt;! ELEMENT Asignaturas_Primero (Asignatura+)&gt;\n&lt;! ELEMENT Asignatura (titulo, profesor+)&gt;\n&lt;! ATTLIST tipo id #CDATA REQUIRED&gt;\n&lt;! ELEMENT titulo (#PCDATA)&gt;\n&lt;! ELEMENT tipo (#PCDATA)&gt;\n&lt;! ELEMENT profesor (nombre, apellido)&gt;\n&lt;! ELEMENT nombre (#PCDATA)&gt;\n</code></pre> Listado 1.7: DTD Listado de Asignaturas con atributos</p> <p>Finalmente, la tabla 3.1a muestra un resumen de definici\u00f3n de elementos y atributos en una DTD.</p> Tipos ELEMENTO #PCDATA Cadena de caracteres EMPTY Elemento sin contexto ANY Cualquier tipo Operadores ELEMENTO | Disyunci\u00f3n + Uno o m\u00e1s * Cero o m\u00e1s ? Cero o uno <p>Tabla 3.1a: Descripci\u00f3n DTD de elementos y atributos XML</p> <p>Y la tabla 3.1b muestra un resumen de definici\u00f3n de atributos en una DTD.</p> Tipos ATRIBUTOS CDATA Cadena de caracteres ID Identificador IDREF Referencia a un ID Operadores ELEMENTO #REQUIRED Obligatorio #IMPLIED Opcional #FIXED Valor fijo #DEFAULT Valor por defecto <p>Tabla 3.1a: Descripci\u00f3n DTD de atributos XML</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#32-xml-schema","title":"3.2 XML Schema","text":"<p>Se trata de un lenguaje de definici\u00f3n de la estructura de documentos XML m\u00e1s sofisticado que DTD. Aunque es m\u00e1s complejo, soluciona muchos de los inconvenientes de las DTD. De hecho, XML Schema es un superconjunto del lenguaje DTD especificado mediante la sintaxis de XML. Entre otros aspectos, XML Schema soporta no solo el tipo cadena de caracteres (soportado por DTD), sino tambi\u00e9n tipos num\u00e9ricos, tipos complejos como listas y adem\u00e1s permite al usuario definir sus propios tipos as\u00ed como extender tipos de datos complejos mediante un mecanismo similar a la herencia. Adem\u00e1s, XML Schema soporta el concepto de espacios de nombre, permitiendo reutilizar elementos de un esquema en otros.</p> <p>Un XML Schema se especifica en un documento aparte del documento XML con extensi\u00f3n .xsd. Dentro de este archivo, que sigue las reglas sint\u00e1cticas de cualquier documento XML, se especifican las definiciones de cada elemento del esquema, qu\u00e9 atributos incluye as\u00ed como sus tipos de datos v\u00e1lidos asociados tanto a los elementos como a los atributos. De esta forma, XML Schema permite definir un esquema para validar documentos XML, de forma m\u00e1s sofisticada a como lo hace DTD. El listado 1.8 muestra un XML Schema para el ejemplo del listado de asignaturas.</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;\n    &lt;xs:element name=\"Asignaturas_Primero\"&gt;\n        &lt;xs:complexType&gt;\n            &lt;xs:sequence&gt;\n                &lt;xs:element maxOccurs=\"unbounded\" name=\"Asignatura\"&gt;\n                    &lt;xs:complexType&gt;\n                        &lt;xs:sequence&gt;\n                            &lt;xs:element name=\"titulo\" type=\"xs:string\" /&gt;\n                            &lt;xs:element name=\"tipo\" type=\"xs:string\" /&gt;\n                            &lt;xs:element maxOccurs=\"unbounded\" name=\"profesor\"&gt;\n                                &lt;xs:complexType&gt;\n                                    &lt;xs:sequence&gt;\n                                        &lt;xs:element name=\"nombre\" type=\"xs:string\" /&gt;\n                                        &lt;xs:element name=\"apellido\" type=\"xs:string\" /&gt;\n                                    &lt;/xs:sequence&gt;\n                                &lt;/xs:complexType&gt;\n                            &lt;/xs:element&gt;\n                        &lt;/xs:sequence&gt;\n                    &lt;/xs:complexType&gt;\n                &lt;/xs:element&gt;\n            &lt;/xs:sequence&gt;\n        &lt;/xs:complexType&gt;\n    &lt;/xs:element&gt;\n&lt;/xs:schema&gt;\n</code></pre> Listado 1.8: XML Schema: Listado de asignaturas</p> <p>El listado anterior comienza definiendo un XML Schema en la segunda l\u00ednea de c\u00f3digo. Cualquier XML Schema deber\u00e1 incluir esta l\u00ednea de c\u00f3digo. El atributo attributeFormDefault = \"unqualified\" indica que no es necesario utilizar el prefijo del espacio de nombres para definir los atributos del esquema, mientras que el atributo elementFormDefault = \"qualified\" indica que no es necesario utilizar el prefijo del espacio de nombre para definir los elementos del esquema.</p> <p>A continuaci\u00f3n, se define el elemento ra\u00edz \"Asignaturas_Primero\" el cual se trata de un tipo complejo, puesto que no se define con un tipo de datos b\u00e1sico como entero, car\u00e1cter, etc. Este elemento est\u00e1 formado por una secuencia de elementos del tipo \"Asignatura\" (el n\u00famero m\u00e1ximo de elementos Asignatura no est\u00e1 limitado ya que maxOccurs = \"unbounded\". Los elementos Asignatura, a su vez, est\u00e1n formados de una secuencia de elementos \"nombre\", \"tipo\" (ambos de tipo cadena de caracteres) y un conjunto de elementos de tipo \"profesor\" no acotados, el cual se compone a su vez de los elementos \"nombre\" y \"apellido\", ambos de tipo cadena de caracteres.</p> <p>XML Schema permite representar la cardinalidad de un elemento, definiendo el n\u00famero m\u00ednimo y m\u00e1ximo de ocurrencias mediante los atributos minOccurs y maxOccurs. Adem\u00e1s, tambi\u00e9n es posible a\u00f1adir restricciones de forma que se limiten los valores de un elemento o atributo, se permita elegir entre una lista de posibles...Finalmente, XML Schema dispone de los contenedores sequence para definir una secuencia ordenada de subelementos, chocie para definir la elecci\u00f3n entre posibles grupos de elementos o elementos y all para definir un conjunto no ordenado de elementos. Al igual que suced\u00eda con DTD, existen distintas herramientas on-line que no solo permiten validar un documento XML a partir de su esquema XSD, sino que tambi\u00e9n permiten generar el esquema XSD a partir del archivo XML<sup>2</sup>.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#33-xpath","title":"3.3 XPath","text":"<p>En cualquier sistema de bases de datos, adem\u00e1s de los lenguajes de definici\u00f3n de datos, los lenguajes de manipulaci\u00f3n de datos son esenciales para poder realizar consultas y recuperar datos, realizar actualizaciones, etc.</p> <p>XPath es un lenguaje de manipulaci\u00f3n de datos XML. La principal caracter\u00edstica de este lenguaje es que interpreta un documento XML como una estructura de datos de tipo \u00e1rbol, donde los nodos y sus respectivas hojas se corresponden con los elementos y subelementos del documento XML. De esta forma, y utilizando comandos que permiten recorrer el documento de forma an\u00e1loga a los comandos que se utilizan en una terminal de comandos para recorrer el sistema de archivos de un computador, es posible realizar consultas. La tabla 3.2 muestra un listado de los principales operadores utilizados en XPath.</p> Operador Significado / Navegaci\u00f3n entre nodos del documento. Selecciona los nodos del nivel inferior. // Navegaci\u00f3n entre nodos del documento. Selecciona los nodos del nivel inferior de cualquier nodo especificado a continuaci\u00f3n . Selecciona el nodo actual .. Selecciona el nodo padre o nodo de nivel superior | Expresa alternativas @ Selecciona un atributo de un nodo * Selecciona todos los nodos [] Agrupa otros operadores <p>Tabla 3.2: XPath: Operadores b\u00e1sicos</p> <p>Con estos operadores, veamos c\u00f3mo realizar algunas consultas de ejemplo sobre el fragmento XML que especifica un listado de asignaturas.</p> <p>Question</p> <p>Consulta 1. Recuperar los nombres de todas las asignaturas.\"</p> <pre><code>/Asignaturas_Primero/Asignatura/titulo/text()\n</code></pre> <p>Question</p> <p>Consulta 2. \u00bfCu\u00e1ntas asignaturas son impartidas por m\u00e1s de un profesor?\"</p> <pre><code>/Asignaturas_Primero/Asignatura[count(profesor)&gt;1]\n</code></pre> <p>Question</p> <p>Consulta 3. \u00bfQui\u00e9nes son los profesores que imparten la asignatura de f\u00edsica?\"</p> <pre><code>/Asignaturas_Primero/Asignatura[titulo=\"Fisica\"]/profesor/nombre\n</code></pre> <p>Es posible utilizar XPath de forma on-line a trav\u00e9s de visualizadores<sup>3</sup> o, por ejemplo, instalando el plug-in XPatherizer de Notepad++, en caso de utilizar este \u00faltimo editor de c\u00f3digo XML.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#33-xslt","title":"3.3 XSLT","text":"<p>Otro lenguaje de manipulaci\u00f3n de datos XML es el lenguaje XSLT. El lenguaje XSLT (XML StylesSheet Transformations) proviene del lenguaje XSL (XML StylesSheet). Este \u00faltimo permite especificar las opciones de formato de un documento XML en un archivo separado, aislando as\u00ed la especificaci\u00f3n del contenido y la presentaci\u00f3n de un archivo XML. Podr\u00eda decirse que XSL es a XML lo que CSS es a HTML. El lenguaje XSLT permite especificar las opciones e formato de un documento XML, pudiendo transformar este documento en otros formatos como HTML, PDF, etc. Dada la potencia de este lenguaje, que es capaz de transformar un archivo XML en otro que tambi\u00e9n pudiera ser XML, se utiliza frecuentemente como lenguaje de consulta.</p> <p>Las transformaciones XSLT se definen por medio de plantillas, que permiten a su vez recuperar contenido del documento XML a trav\u00e9s de la utilizaci\u00f3n de expresiones XPath. La principal caracter\u00edstica de XSLT es que la aplicaci\u00f3n de plantillas se realiza de forma recursiva, lo cual recibe el nombre de recursividad estructural. El listado 1.9 presenta un ejemplo de aplicaci\u00f3n de una transformaci\u00f3n XSLT por medio de una plantilla.</p> <p><pre><code>&lt;xsl: template match = /Asignaturas_Primero/Asignatura &gt;\n    &lt;titulo_asignatura&gt;\n        &lt;xsl:value-of select = titulo/&gt;\n    &lt;/titulo_asignatura&gt;\n&lt;/xsl:template&gt;\n</code></pre> Listado 1.9: XSLT: Ejemplo de plantilla</p> <p>Tal y como se puede observar en el fragmento anterior, una transformaci\u00f3n o consulta XSLT se define mediante dos \u00f3rdenes b\u00e1sicas: match que especifica una expresi\u00f3n XPath para seleccionar uno o m\u00e1s nodos y value \u2212 of select que devuelve los valores especificados de los nodos que se han obtenido como resultado de la expresi\u00f3n XPath. Merece la pena destacar que cualquier texto o etiqueta del archivo XSLT que no est\u00e9 en el espacio de nombre se copia a la salida sin cambios. Esto quiere decir, que el resultado del fragmento anterior no son \u00fanicamente los t\u00edtulos de las asignaturas, sino estos mismos delimitados por una etiqueta de apertura y cierre llamada &lt; titulo_asignatura &gt;. De esta forma, a partir de un documento XML se ha generado otro con el resultado de la consulta. A modo de resumen, la tabla 3.3 define los principales constructores XSLT.</p> Constructor XSLT Significado &lt; xsl:template match = \"expresion XPath\" &gt; Selecciona nodos a devolver Devuelve todos los nodos que no coinciden con alguna otra plantilla Selecciona el nodo actual A\u00f1ade un atributo al elemento precedente Aplica la recursividad estructural Define una clave Devuelve los nodos que coincidan con el valor especificado. Especifica operaciones de combinaci\u00f3n (JOINS) Devuelve los nodos resultados, ordenados por un elemento o atributo <p>Tabla 3.3: Principales Constructores XSLT</p> <p>Para trabajar con XSLT, se aconseja la descarga del software XML Copy Editor https://xml-copy-editor.sourceforge.io.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#34-xquery","title":"3.4 XQuery","text":"<p>Es un lenguaje de consulta de prop\u00f3sito general sobre datos XML estandarizado por el consorcio W3C. XQuery procede del lenguaje de programaci\u00f3n Quilt y, por este motivo, toma prestadas muchas caracter\u00edsticas de otros lenguajes como XPath o SQL. De hecho, la sintaxis de XQuery es mucho m\u00e1s similar a la de SQL, al contrario que ocurr\u00eda con lenguajes como XPath o XSLT. </p> <p>Para definir una consulta en XQuery se construye una expresi\u00f3n, denominada \"FLWR\"(flower, flor en ingl\u00e9s). A continuaci\u00f3n, se describen cada una de las sentencias de las que se compone la expresi\u00f3n:</p> <ul> <li> <p>FOR: Obtiene una serie de variables cuyos valores son el resultado de una expresi\u00f3n XPath. Es el equivalente a la cl\u00e1usula FROM del lenguaje SQL.</p> </li> <li> <p>LET: Define y renombra expresiones complejas para ser utilizadas en el resto de la consulta, reduciendo la complejidad y aliviando la sintaxis. Es una cl\u00e1usula opcional.</p> </li> <li> <p>WHERE: En esta cl\u00e1usula, llamada igualmente en SQL, se especifican condiciones sobre los resultados obtenidos en la cl\u00e1usula FOR.</p> </li> <li> <p>RETURN: Especifica y define el resultado que se va a obtener de la consulta. Se trata del equivalente a la cl\u00e1usula SELECT en SQL.</p> </li> </ul> <p>Siguiendo la sintaxis anteriormente mencionada, el listado 1.10 muestra un ejemplo para obtener un listado con las asignaturas anuales.</p> <p><pre><code>for $x in /Asignaturas_Primero/Asignatura\nlet $nombre_asignatura:= $x/titulo\nwhere $x/tipo='anual'\nreturn &lt;asignatura_anual&gt; $nombre_asignatura &lt;/asignatura_anual&gt;\n</code></pre> Listado 1.10: XQuery: Ejemplo de consulta</p> <p>En el listado anterior,la cl\u00e1usula for permitir\u00e1 recorrer todos los elementos asignatura. Por su parte, let define una expresi\u00f3n en la que se asigna a nombre_asignatura el t\u00edtulo de la asignatura que se est\u00e1 recorriendo en cada momento. La cl\u00e1usula where impone el criterio de que el tipo de asignatura sea anual y, finalmente, result define como resultado un fragmento de c\u00f3digo XML donde el nombre de la asignatura anual (seg\u00fan la expresi\u00f3n indicada en let) aparecer\u00e1 como resultado encerrado entre dos etiquetas XML llamadas asignatura_anual. Tal y como se puede apreciar, XQuery tambi\u00e9n permite generar nuevos documetnos XML a partir de consultas.</p> <p>Al igual que SQL, XQuery dispone de multitud de funciones para la consulta de datos. As\u00ed, es posible utilizar la funci\u00f3n distinct() para eliminar duplicados en los resultados as\u00ed como funciones de agregaci\u00f3n como sum() o count() adem\u00e1s de poder especificar funciones definidas por el usuario. XQuery tambi\u00e9n permite utilizar \"joins\" incluyendo en la cl\u00e1usula where el operador de combinaci\u00f3n al igual que en SQL. Por otra parte, si bien es cierto que XQuery no posee el operador group by de SQL, este puede implementarse mediante la creaci\u00f3n de consultas anidadas, las cuales s\u00ed que son soportadas por el lenguaje.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#4-bases-de-datos-documentales-mongodb","title":"4. Bases de datos documentales: MongoDB","text":"<p>Los sistemas de bases de datos documentales u orientados a documentos son sistemas de bases de datos NoSQL que almacenan datos, los cuales se estructuran en forma de documentos. En los \u00faltimos a\u00f1os, han proliferado una gran variedad de sistemas de este tipo como pueden ser Cassandra<sup>4</sup>, CouchDB<sup>5</sup>, Riak<sup>6</sup> o MongoDB<sup>7</sup>, sobre el cual trata esta secci\u00f3n.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#41-caracteristicas-funciones-y-metodo","title":"4.1 Caracter\u00edsticas, funciones y m\u00e9todo","text":"<p>Las bases de datos documentales son capaces de almacenar informaci\u00f3n en diferentes formatos sin una estructura definida. En cualquier caso, lo habitual es que los documentos empleen un formato de archivo, mientras que los datos contenidos s\u00ed utilicen una estructura fija.</p> <p>A pesar de que su estructura es completamente distinta, estas bases de datos permiten realizar las mismas operaciones b\u00e1sicas que las bases de datos relacionales, esto es, a\u00f1adir, actualizar o eliminar informaci\u00f3n, adem\u00e1s de realizar las pertinentes consultas por parte del usuario.</p> <p>A diferencia de las bases de datos relacionales, en las bases de datos orientadas a documentos no es necesario recorren todas las columnas de una tabla a la hora de realizar una consulta. En lugar de ello se asigna un identificador \u00fanico a cada documento, de manera que a la hora de hacer una consulta se comprueba el mismo documento. Este identificador puede ser de diferentes tipos, por ejemplo una ruta completa o una cadena de caracteres.</p> <p>Gracias a este funcionamiento, las bases de datos documentales se emplean para el almacenamiento y consulta de datos semiestructurados, los cu\u00e1les no tienen un esquema previamente definido y que ser\u00edan dif\u00edciles (o incluso imposibles) de gestionar en la tradicionales bases relacionales.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#42-ventajas-y-inconvenientes","title":"4.2 Ventajas y Inconvenientes","text":""},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#ventajas","title":"Ventajas","text":"<ul> <li>Permiten almacenar y consultar informaci\u00f3n semiestructurada sin una estructura definida.</li> <li>Son un modelo muy flexible que puede albergar numerosos tipos de datos.</li> <li>Simplifican las tareas de adici\u00f3n o actualizaci\u00f3n de datos. La mayor\u00eda de aplicaciones web o m\u00f3viles est\u00e1n sometidas a cambios constantes. Gracias a las bases de datos documentales se pueden a\u00f1adir nuevos datos o modelos de an\u00e1lisis de manera mucho m\u00e1s flexible.</li> <li>Aseguran una escritura r\u00e1pida, dando prioridad a la disponibilidad de la escritura sobre la consistencia de los datos. Esto permite asegurar la rapidez incluso en casos de fallos en el hardware o en la red, que en otras bases de datos supondr\u00eda retrasos en la modificaci\u00f3n de los datos y repercutir\u00eda negativamente en su coherencia.</li> <li>Garantizan un buen rendimiento. La mayor\u00eda de bases de datos documentales cuentan con potentes motores de b\u00fasqueda y avanzadas propiedades de indexaci\u00f3n, lo que asegura una mayor rapidez a la hora de consultar la informaci\u00f3n.</li> <li>Tienen una gran escalabilidad y son uno de los mejores m\u00e9todos para el almacenamiento de grandes vol\u00famenes de informaci\u00f3n.</li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#desventajas","title":"Desventajas","text":"<ul> <li>No utilizan el lenguaje SQL como lenguaje principal de consulta, aunque s\u00ed lo pueden usar de apoyo. Es decir, al contrario que las bases relacionales, no existe un lenguaje estandarizado para la creaci\u00f3n de estas bases de datos.</li> <li>No siempre pueden garantizar las propiedades ACID de atomicidad, consistencia, integridad y durabilidad.</li> <li>No tienen una gran comunidad detr\u00e1s y existen mucha menos informaci\u00f3n acerca de estas bases de datos.</li> <li>Los \u00edndices pueden ocupar mucha memoria RAM, sobre todo en las bases documentales que manejan un gran volumen de datos.</li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#43-estructura-y-uso","title":"4.3 Estructura y uso","text":"<p>Como ya sabemos, las bases de datos documentales almacenan datos semiestructurados sin un esquema predefinido. La informaci\u00f3n se almacena en documentos que incluyen todas las caracter\u00edsticas del registro, y las consultas se realizan en base a estos documentos. Esto las diferencia de las bases relacionales que organizan la informaci\u00f3n en tablas y las consultas se realizan en base a los campos (columnas) de dichas tablas.</p> <p>Por otro lado, no utilizan el lenguaje SQL. Por el contrario, emplean otro tipo de formatos como pueden ser el JSON o XML. El formato m\u00e1s habitual es JSON (Javascript Object Notation), un lenguaje que resulta m\u00e1s compacto y legible que el SQL.</p> <p>En base a esta estructura y caracter\u00edsticas, las bases de datos documentales se usan principalmente para almacenar grandes cantidades de datos, o cuando se trata de informaci\u00f3n poco estructurada.</p>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#5-bases-de-datos-orientadas-a-grafos","title":"5. Bases de datos orientadas a grafos","text":"<p>Tal y como se expuso anteriormente, en los \u00faltimos a\u00f1os han aparecido nuevos tipos de datos como aquellos que vienen dados en forma de grafo. Algunos de estos datos son los provenientes de interacciones en redes sociales, datos geogr\u00e1ficos expresados en forma de mapas, etc.</p> <p>Un grafo es un ente matem\u00e1tico compuesto por un conjunto de nodos o v\u00e9rtices y un conjunto de enlaces o aristas. Matem\u00e1ticamente puede ser expresado por medio de la ecuaci\u00f3n siguiente</p> <pre><code>G = {V, E}\n</code></pre> <p>Donde V representa el conjunto de nodos o v\u00e9rtices y E representa el conjunto de enlaces o aristas. As\u00ed pues, sea el grafo de la figura 1.18 que representa un conjunto de ciudades conectadas por autov\u00edas, el conjunto V de nodos ser\u00eda V = {La Coru\u00f1a, Madrid, San Sebasti\u00e1n, Barcelona, Valencia, Sevilla, C\u00e1diz}, mientras que el conjunto de enlaces E vendr\u00eda dado por</p> <pre><code>E = {A-1, A-2, A-3, A-4, A-4-I, A-4-II, A-6, A-7}\n</code></pre> Figura 1.24: Grafo que representa las principales autov\u00edas de Espa\u00f1a. (Fuente: Propia) <p>Una base de datos orientada a grafos es, por tanto, un sistema de bases de datos que implementa m\u00e9todos de creaci\u00f3n, lectura, actualizaci\u00f3n y eliminaci\u00f3n de datos en un modelo expresado en forma de grafo. Existen dos aspectos fundamentales en este tipo de sistemas: el primero de ellos hace referencia al almacenamiento de los datos. En una base de datos orientada a grafos, los datos pueden almacenarse siguiendo el modelo relacional, lo que implica mapear la estructura del grafo a una estructura relacional, o bien, almacenarse de forma nativa utilizando modelos de datos propios para almacenar estructuras de tipo grafo. La ventaja de mapear los grafos a una estructura relacional radica en que la gesti\u00f3n y consulta de los datos se realizar\u00e1 de forma tradicional a trav\u00e9s de un backend conocido como, por ejemplo, MySQL. Por su parte, la ventaja del almacenamiento nativo de grafos radica es que existen modelos de datos e implementaciones que aseguran y garantizan el buen rendimiento y la escalabilidad del sistema.</p> <p>El segundo aspecto importante es el procesamiento de los datos. En este sentido, tambi\u00e9n es posible distinguir el procesamiento no nativo de los grafos, lo cual implica el tratamiento de estos datos siguiendo las t\u00e9cnicas tradicionales utilizadas en los lenguajes de modificaci\u00f3n de datos de las bases de datos relacionales, o el procesamiento nativo de los datos de grafos, el cual es beneficioso porque optimiza los recorridos del grafo cuando se realizan consultas aunque, en ocasiones, invierta demasiado tiempo y memoria en consultas que no requieren de recorridos complejos.</p> <p>Si bien es cierto que cualquier dominio puede ser modelado como un grafo, esta no es una raz\u00f3n de peso como para cambiar o migrar de un esquema y modelo de datos bien conocido e investigado, como el esquema relacional, a un modelo orientado a grafos. Sin embargo, la motivaci\u00f3n para requerir de sistemas de bases de datos espec\u00edficos, orientados a trabajar con este tipo de datos, radica en tres aspectos principales:</p> <ul> <li> <p>\u26ab Rendimiento: Los sistemas de bases de datos orientados a grafos optimizan el rendimiento de las consultas sobre este tipo de datos con respecto a las bases de datos relacionales. Si bien es cierto que las consultas en el modelo relacional se vuelven m\u00e1s complejas y el rendimiento disminuye a medida que el conjunto de datos crece, esto no es as\u00ed cuando se trabaja con grafos, donde complejidad y rendimiento permanecen constantes. Esto es as\u00ed, ya que las consultas se localizan en una porci\u00f3n del grafo y, por tanto, solo es necesario explorar la parte del grafo afectada para resolver la consulta.</p> </li> <li> <p>\u26ab Flexibilidad: Los sistemas de bases de datos orientados a grafos son m\u00e1s flexibles que los sistemas de bases de datos relacionales. Mientras que los primeros requieren la modelizaci\u00f3n exhaustiva a priori de todo el dominio, esto no es necesario en los segundos, donde la naturaleza aditiva de los grafos permite a\u00f1adir nuevos nodos, relaciones, etiquetas y subgrafos a uno dado sin necesidad de modificar todo el modelo de datos, facilitando la implementaci\u00f3n y reduciendo el riesgo a corromper el modelo de datos. </p> </li> <li>\u26ab Agilidad: A partir de las dos caracter\u00edsticas anteriores, es posible deducir que el trabajo con sistemas de bases de datos orientados a grafos es m\u00e1s \u00e1gil que la gesti\u00f3n de estos datos a trav\u00e9s de sistemas de bases de datos relacionales. Esta r\u00e1pida gesti\u00f3n permitir\u00e1 utilizar tecnolog\u00eda alineada con las nuevas metodolog\u00edas de desarrollo de software \u00e1gil, permitiendo dise\u00f1ar y desarrollar software que utilice este tipos de datos.</li> </ul> <p>Existen distintos lenguajes de marcado de grafos, a partir de los cuales es posible generar archivos con formato de grafo para ser tratados por una base de datosde este tipo. Algunos de los lenguajes m\u00e1s utilizados son GraphML, eXtensibleGraph Markup and Modeling Language (XGMML), Graph Exchange Language (GXL) o Graph Modelling Language (GML), entre otros. La mayor\u00eda deellos son variantes o extensiones del lenguaje XML para el modelado de grafos.</p> <p>En la actualidad, GraphML es uno de los lenguajes m\u00e1s extendidos para el modelado de datos en forma de grafo. Se trata de un lenguaje sencillo, general, extensible y robusto que est\u00e1 compuesto por un n\u00facleo del lenguaje que define las propiedades estructurales del grafo y un mecanismo flexible de extensiones que permite a\u00f1adir informaci\u00f3n espec\u00edfica del mismo. La notaci\u00f3n es muy similar a XML y se profundizar\u00e1 en ella en cap\u00edtulos posteriores. A modo de ejemplo, el grafo mostrado en la figura 1.18 podr\u00eda representarse en GraphML seg\u00fan se muestra en el siguiente listado.</p> <p><pre><code>&lt;graph id=\"Grafo_Autovias\" edgedefault=\"undirected\"&gt;\n  &lt;node id=\"La Coru\u00f1a\"/&gt;\n  &lt;node id=\"San Sebasti\u00e1n\"/&gt;\n  &lt;node id=\"Madrid\"/&gt;\n  &lt;node id=\"Barcelona\"/&gt;\n  &lt;node id=\"Valencia\"/&gt;\n  &lt;node id=\"Sevilla\"/&gt;\n  &lt;node id=\"Cadiz\"/&gt;\n  &lt;edge id = \"A-6\" source=\"La Coru\u00f1a\" target=\"Madrid\"/&gt;\n  &lt;edge id = \"A-1\" source=\"Madrid\" target=\"San Sebasti\u00e1n\"/&gt;\n  &lt;edge id = \"A-2\" source=\"Madrid\" target=\"Barcelona\"/&gt;\n  &lt;edge id = \"A-7\" source=\"Barcelona\" target=\"Valencia\"/&gt;\n  &lt;edge id = \"A-3\" source=\"Madrid\" target=\"Valencia\"/&gt;\n  &lt;edge id = \"A-4\" source=\"Madrid\" target=\"Sevilla\"/&gt;\n  &lt;edge id = \"A-4-I\" source=\"Sevilla\" target=\"C\u00e1diz\"/&gt;\n  &lt;edge id = \"A-4-II\" source=\"Madrid\" target=\"C\u00e1diz\"/&gt;\n&lt;/graph&gt;\n</code></pre> Listado 1.11: Grafo Autov\u00edas</p> <p>Como se puede apreciar en el fragmento de c\u00f3digo, en primer lugar se define un grafo cuyo identificador es Grafo_Autov\u00edas. El tipo de aristas que incluye este grafo son aristas no dirigidas, puesto que los enlaces del grafo no presentan flechas que indiquen direcci\u00f3n. Por tanto, los enlaces son bidireccionales. En caso de definir un grafo dirigido, el valor del atributo edgedefault ser\u00eda directed. A continuaci\u00f3n, se definen los distintos nodos que contendr\u00e1 el grafo y despu\u00e9s las aristas o enlaces, definiendo el identificador de cada una de ellas as\u00ed como su origen y destino. Al tratarse de un grafo no dirigido, origen y destino son intercambiables, no siendo as\u00ed en caso de que se trate de un grafo dirigido.</p> <p>Los sistemas de bases de datos orientados a grafos han proliferado mucho en los \u00faltimos a\u00f1os, existiendo numerosas tecnolog\u00edas que dan soporte a ellos, como pueden ser Microsoft Infinite Graph, Titan, OrientDB, FlockDB, AllegroGraph o Neo4j.</p> <p>Aunque existen diferentes tipos de modelos de datos basados en grafos, cada sistema de base de datos elegir\u00e1 uno de ellos para representar el contenido de la base de datos. Cualquiera de estos modelos est\u00e1 inspirado en la estructura matem\u00e1tica de un grafo para modelizar un conjunto de datos. Matem\u00e1ticamente, un grafo es un conjunto de nodos o v\u00e9rtices, que representan entidades de un dominio, y un conjunto de aristas o enlaces, que representan las relaciones o interacciones que se producen entre las entidades. La figura1.19 muestra un ejemplo de grafo que representa las relaciones de seguimiento entre distintos canales en YouTube.</p> Figura 1.25: Ejemplo de grafo relaciones de seguimiento de canales Youtube. (Fuente: Propia) <p>Uno de los modelos de datos basado en grafos m\u00e1s popular es el modelo de grafo de propiedades etiquetadas. Un grafo representado mediante este modelo, debe cumplir las siguientes caracter\u00edsticas principales:</p> <ol> <li>Debe contener un conjunto de nodos y aristas o relaciones entre dichos nodos.</li> <li>Los nodos contienen propiedades, definidas a trav\u00e9s de pares \u201cclave-valor\u201d.</li> <li>Los nodos pueden estar etiquetados con una o m\u00e1s etiquetas.</li> <li>Las relaciones entre los nodos est\u00e1n nombradas y son dirigidas, teniendo siempre un nodo de inicio y otro nodo de fin.</li> <li>Las relaciones del grafo tambi\u00e9n pueden contener propiedades. Este modelo, a pesar de su simplicidad, es sencillo de entender y permite modelar cualquier problema y/o conjunto de datos.</li> </ol>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#51-caracteristicas-principales","title":"5.1 Caracter\u00edsticas principales","text":"<p>El aumento del uso de los sistemas de bases de datos orientados a grafos y el auge que est\u00e1n alcanzando en los \u00faltimos a\u00f1os se debe, principalmente, a una serie de caracter\u00edstica que confieren a estos sistemas una serie de ventajas importantes sobre los sistemas de bases de datos tradicionales y otros sistemas NoSQL.</p> <ul> <li>Almacenamiento: Las bases de datos orientadas a grafo pueden ser de almacenamiento nativo, en cuyo caso los datos se almacenan internamente con estructura de grafo. No obstante, existen bases de datos de grafos que mapean los datos para adptarlos a una estructura relacional.</li> <li>Procesamiento: El almacenamiento nativo permite obtener una estructura de datos que no requiere \u00edndices de adyacencia para referenciar los nodos del grafo, mejorando el rendimiento en la ejecuci\u00f3n de consultas.</li> <li>Rendimiento: La ejecuci\u00f3n de consultas sobre datos estructurados en forma de grafo proporciona escalabilidad al sistema de bases de datos, ya que aunque el n\u00famero de datos aumente, las consultas se ejecutan sobre la porci\u00f3n del grafo de inter\u00e9s.</li> <li>Flexibilidad: Los modelos de datos basados en grafos permiten r\u00e1pidamente a\u00f1adir nuevos nodos, enlaces y subgrafos a un modelo existente con facilidad y sin p\u00e9rdida de funcionalidad ni rendimiento.</li> <li>Rapidez: La naturaleza libre de esquema de las bases de datos orientadas a grafos, juntos con las APIs desarrolladas para su manipulaci\u00f3n y los lenguajes de consulta, hacen que estos sistemas sean \u00e1giles y r\u00e1pidos, integr\u00e1ndolos con las metodolog\u00edas de desarrollo de software iterativas e incrementales.</li> </ul>"},{"location":"UD1%20-%20Gesti%C3%B3n%20de%20soluciones/3_form_datos_almacenamiento.html#52-areas-de-aplicacion","title":"5.2 \u00c1reas de Aplicaci\u00f3n","text":"<p>Las caracter\u00edsticas de las bases de datos orientadas a grafos desarrolladas en el apartado anterior confieren a esta tecnolog\u00eda un sinf\u00edn de aplicaciones reales en las que su uso mejora significativamente a las tecnolog\u00edas tradicionales. A continuaci\u00f3n, se muestran algunos ejemplos de aplicaci\u00f3n donde, actualmente, este tipo de bases de datos es el m\u00e1s ampliamente utilizado.</p> <ul> <li> <p>Redes sociales: El an\u00e1lisis de redes sociales es, a d\u00eda de hoy, una de las principales fuentes de informaci\u00f3n de cualquier dominio. El an\u00e1lisis de redes sociales mediante bases de datos orientadas a grafos permite identificar relaciones expl\u00edcitas e impl\u00edcitas entre usuarios y grupos de usuarios, as\u00ed como identificar la forma en la que ellos interact\u00faan pudiendo inferir el comportamiento de un usuario en base a sus conexiones. En una red social, dos usuarios presentan una conexi\u00f3n expl\u00edcita si est\u00e1n directamente conectados. Esto ocurre, por ejemplo, entre dos usuarios de Facebook que son amigos o dos compa\u00f1eros de trabajo de la misma empresa en LinkedIn. Por otra parte, una relaci\u00f3n impl\u00edcita es aquella que se produce entre dos usuarios a trav\u00e9s de un intermediario, como puede ser otro usuario, un post donde han hecho un comentario, un \u201clike\u201d o un art\u00edculo que ambos han comprado.</p> </li> <li> <p>Sistemas de recomendaci\u00f3n: Estos sistemas permiten modelar en forma de grafo las relaciones que se establecen entre personas o usuarios y cosas, como pueden ser productos, servicios, contenido multimedia o cualquier otro concepto relevante en funci\u00f3n del dominio de aplicaci\u00f3n. Las relaciones se establecen en funci\u00f3n del comportamiento de los usuarios al comprar, consumir contenido, puntuarlo o evaluarlo etc. De esta forma, los sistemas de recomendaci\u00f3n identifican recursos de inter\u00e9s para un usuario espec\u00edfico y pueden predecir su comportamiento al comprar un producto o contratar un servicio.</p> </li> <li> <p>Informaci\u00f3n geogr\u00e1fica: Se trata del caso de aplicaci\u00f3n m\u00e1s popular de la teor\u00eda de grafos. Las aplicaciones de las bases de datos orientadas a grafos en informaci\u00f3n geogr\u00e1fica van desde calcular rutas \u00f3ptimas entre dos puntos en cualquier tipo de red (red de carreteras, de ferrocarril, a\u00e9rea, log\u00edstica...) hasta encontrar todos los puntos de inter\u00e9s en un \u00e1rea concreta, encontrar el centro de una regi\u00f3n u obtener la intersecci\u00f3n entre dos o m\u00e1s regiones, entre otras muchas. As\u00ed, las bases de datos orientadas a grafos permiten modelar estos casos de aplicaci\u00f3n como grafos dirigidos sobre los cuales operar para obtener los resultados deseados.</p> </li> </ul> <p>Aunque en este apartado se hayan analizado las principales \u00e1reas de aplicaci\u00f3n, son muchos los dominios de aplicaci\u00f3n donde las bases de datos orientadas a grafos se est\u00e1n convirtiendo en una soluci\u00f3n predominante, como lo son la gesti\u00f3n de datos maestros, gesti\u00f3n de redes y centros de datos o control de acceso entre otros muchos.</p> <ol> <li> <p>https://www.liquid-technologies.com/online-xsd-validator y http://xmlvalidator.new-studio.org \u21a9</p> </li> <li> <p>https://www.freeformatter.com, https://www.liquid-technologies.com/online-xsd-validator y https://extendsclass.com/xml-schema-validator.html \u21a9</p> </li> <li> <p>https://download.cnet.com/XPath-Visualizer/3000-7241_4-75804649.html, https://www.freeformatter.com/xpath-tester.html \u21a9</p> </li> <li> <p>https://cassandra.apache.org/ \u21a9</p> </li> <li> <p>http://couchdb.apache.org \u21a9</p> </li> <li> <p>https://riak.com \u21a9</p> </li> <li> <p>https://www.mongodb.com/es \u21a9</p> </li> </ol>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/index.html","title":"UD 2 - Procesado y Presentaci\u00f3n Datos Almacenados","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/index.html#resultados-de-aprendizaje","title":"Resultados de Aprendizaje","text":"<ul> <li>RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/index.html#contenido-y-criterios-de-evaluacion","title":"Contenido y Criterios de Evaluaci\u00f3n","text":"Contenido Criterios de Evaluaci\u00f3n MongoDB - Neo4j RA5075.1d - Se han procesado los datos almacenados.  RA5075.1e - Se han presentado los resultados y las soluciones al cliente final en una forma f\u00e1cil de interpretar."},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html","title":"UD 2 - Procesado y Presentaci\u00f3n Datos Almacenados - MongoDB","text":"<p>El nombre MongoDB proviene de la palabra inglesa \"homongous\" que significa enorme. MongoDB es, por tanto, un sistema de base de datos NoSQL, open source, orientado a documentos y escrito en lenguaje C++. Este sistema de bases de datos est\u00e1 disponible no solo para m\u00faltiples plataformas y sistemas operativos (Windows, Linux, OS X) sino tambi\u00e9n como servicio empresarial en la nube y se puede integrar con otros servicios como, por ejemplo, Amazon Web Services (AWS).</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#1-caracteristicas","title":"1. Caracter\u00edsticas","text":"<p>Tal y como se ha presentado anteriormente, MongoDB es un sistema de bases de datos documental u orientado a documentos. Esta orientaci\u00f3n hace que los datos se almacenen de forma estructurada en forma de documentos, los cuales se acoplan sin problema en los tipos de datos utilizados por los lenguajes de programaci\u00f3n. Adem\u00e1s, esta concepci\u00f3n hace que una base de datos MongoDB disponga de un esquema din\u00e1mico y f\u00e1cilmente modificable.</p> <p>En este apartado se pretende poner de relieve las principales caracter\u00edsticas de MongoDB que hacen que sea en la actualidad uno de los sistemas de bases de datos NoSQL m\u00e1s utilizados tanto en el \u00e1mbito acad\u00e9mico como en el profesional. </p> <ul> <li> <p>Alto rendimiento: Gracias a la definici\u00f3n de los documentos y la creaci\u00f3n de \u00edndices que hace que las lecturas y escrituras se realicen de forma m\u00e1s r\u00e1pida.</p> </li> <li> <p>Alta disponibilidad: MongoDB dispone de servidores replicados con restablecimiento autom\u00e1tico maestro.</p> </li> <li> <p>F\u00e1cil escalabilidad: Permitiendo distribuir colecciones de documentos entre diferentes m\u00e1quinas de forma muy sencilla.</p> </li> <li> <p>Indexaci\u00f3n: Similar al concepto de \u00edndice en una base de datos relacional, permite crear \u00edndices e \u00edndices secundarios para mejorar el rendimiento de las consultas.</p> </li> <li> <p>Consultas ad-hoc: Al igual que en una base de datos relacional, MongoDB da soporte a la b\u00fasqueda por campos, consultas de rangos, uso de expresiones regulares... A todo lo anterior, se a\u00f1ade la posibilidad de ejecutar y devolver una funci\u00f3n JavaScript definida por el programador.</p> </li> <li> <p>Replicaci\u00f3n: Siguiendo el modelo maestro-esclavo. El maestro puede ejecutar comandos de lectura y escritura mientras que el esclavo solo tiene acceso de lectura y la posibilidad de realizar copias de seguridad. En caso de que el maestro caiga, el esclavo puede elegir un nuevo maestro para mantener el servicio de replicaci\u00f3n.</p> </li> <li> <p>Balanceo de carga: MongoDB es capaz de ejecutarse en m\u00faltiples servidores, pudiendo balancear la carga y/o duplicar los datos para mantener el correcto funcionamiento del sistema aunque se produzca un fallo hardware.</p> </li> <li> <p>Almacenamiento de archivos: Todo lo anterior, facilita que este sistema de base de datos pueda ser usado con un sistema de archivos GridFS con balanceo de carga y replicaci\u00f3n.</p> </li> <li> <p>Agregaci\u00f3n: Proporciona operadores de agregaci\u00f3n y la posibilidad de utilizar funciones MapReduce para el procesamiento de datos por lotes.</p> </li> <li> <p>Ejecuci\u00f3n de Javscript del lado del servidor: Es posible realizar consultas utilizando JavaScript, de forma que \u00e9stas son enviadas directamente a la base de datos para ser ejecutadas.</p> </li> </ul> <p>Todas estas caracter\u00edsticas hacen que, en la actualidad, MongoDB sea uno de los principales sistemas de bases de datos NoSQL elegidos en multitud de aplicaciones como: almacenamiento y registro de eventos, comercio electr\u00f3nico, juegos, aplicaciones m\u00f3viles, almacenes de datos, gesti\u00f3n de estad\u00edsticas en tiempo real y cualquier aplicaci\u00f3n que requiera llevar a cabo anal\u00edticas sobre grandes vol\u00famenes de datos.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#2-conceptos-basicos","title":"2. Conceptos b\u00e1sicos","text":"<p>Aunque MongoDB es un sistema de bases de datos NoSQL con todo lo que ello implica, tambi\u00e9n ofrece toda la funcionalidad de la que disponen las bases de datos relacionales. Sin embargo, la estructura de una base de datos MongoDB difiere de la de una base de datos relacional.</p> <p>En un servidor MongoDB es posible crear tantas bases de datos como se desee. El concepto de base de datos es en este caso equivalente al de base de datos en los sistemas relacionales. Una vez creada, la base de datos estar\u00e1 compuesta por una o m\u00e1s colecciones. El t\u00e9rmino colecci\u00f3n en el \u00e1mbito NoSQL es el equivalente al concepto de tabla en los sistemas relacionales.</p> <p>Cada colecci\u00f3n, por tanto, estar\u00e1 formada por un conjunto de documentos (o incluso ninguno, en cuyo caso la colecci\u00f3n estar\u00eda vac\u00eda). El concepto de documento en NoSQL se corresponde con el concepto de registro en una base de datos relacional. Un documento estar\u00e1 compuesto por una serie de campos, al igual que lo est\u00e1n los registros de una base de datos relacional.</p> <p>En el \u00e1mbito NoSQL, y m\u00e1s concretamente en MongoDB, cada documento viene dado por un archivo JSON http://www.json.org/ (BSON, en realidad, que veremos justo en el siguiente punto) en el cual, siguiendo una estructura clave-valor se especifican las caracter\u00edsticas de cada documento. El siguiente c\u00f3digo muestra un ejemplo de documento que define un barco.</p> <pre><code>{\n    name: 'USS Enterprise-D',\n    operator: 'Starfleet',\n    type: 'Explorer',\n    class: 'Galaxy',\n    crew: 750,\n    codes: [10,11,12]\n}\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#21-bson","title":"2.1 BSON","text":"<p>BSON es un formato de intercambio de datos usado principalmente para su almacenamiento y transferencia en la base de datos MongoDB. Es una representaci\u00f3n binaria de estructuras de datos y mapas. El nombre BSON est\u00e1 basado en el t\u00e9rmino JSON y significa Binary JSON (JSON Binario). Consulta su especificaci\u00f3n en su p\u00e1gina oficial https://bsonspec.org/</p> <p>BSON fue dise\u00f1ado para tener las siguientes caracter\u00edsticas</p> <ul> <li>Ligero: Mantener la sobrecarga espacial al m\u00ednimo es importante para cualquier formato de representaci\u00f3n de datos, especialmente cuando se utiliza a trav\u00e9s de la red.</li> <li>Transitable: BSON est\u00e1 dise\u00f1ado para ser recorrido f\u00e1cilmente. Esta es una propiedad vital en su papel como la principal representaci\u00f3n de datos para MongoDB.</li> <li>Eficiente: La codificaci\u00f3n de datos a BSON y la decodificaci\u00f3n desde BSON puede realizarse muy r\u00e1pidamente en la mayor\u00eda de lenguajes debido al uso de tipos de datos C.</li> </ul> <p>Un objeto BSON consiste en una lista ordenada de elementos. Cada elemento consta de un campo nombre, un tipo y un valor. Los nombres son de tipo String y los tipos pueden ser:</p> Figura BSON 1: Tipos de datos BSON. (Fuente: MongoDB) <p>Un ejemplo de un objeto BSON podr\u00eda ser</p> <pre><code>var mydoc = {\n               _id: ObjectId(\"5099803df3f4948bd2f98391\"),\n               name: { first: \"Alan\", last: \"Turing\" },\n               birth: new Date('Jun 23, 1912'),\n               death: new Date('Jun 07, 1954'),\n               contribs: [ \"Turing machine\", \"Turing test\", \"Turingery\" ],\n               views : NumberLong(1250000)\n            }\n</code></pre> <p>Donde los campos del ejemplo anterior tienen los siguientes tipos de datos:</p> <ul> <li>_id contiene un <code>ObjectId</code>.</li> <li>name contiene un documento embebido que contiene los campos first y last.</li> <li>birth y death mantienen valores del tipo <code>Date</code>.</li> <li>contribs contiene un <code>string</code>.</li> <li>views tiene un valor del tipo <code>NumberLong</code>.</li> </ul> <p>Adem\u00e1s, BSON tambi\u00e9n contiene expresiones que permiten la representaci\u00f3n de tipos de datos que no forman parte de la especificaci\u00f3n JSON. Puedes consultarlos todos en https://www.mongodb.com/docs/manual/reference/bson-types/</p> <p>De todos estos tipos de datos, en MongoDB debemos hacer mayor consideraci\u00f3n a estos particulares:</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#binary-data","title":"Binary Data","text":"<p>EL valor de un Binary Data <code>bindata</code> es un array de bytes. Un valor de <code>bindata</code> tiene un subtipo que indica c\u00f3mo interpretar los datos binarios. La siguiente tabla muestra los subtipos. </p> Figura BSON 2: Subtipos de datos BSON Binary Data. (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#objectids","title":"ObjectIds","text":"<p>Los ObjectIds <code>object</code> son peque\u00f1os, probablemente \u00fanicos, r\u00e1pidos de generar y ordenados. Los valores de ObjectId tienen una longitud de 12 bytes y constan de:</p> <ul> <li>4-byte timestamp, que representa la creaci\u00f3n del ObjectId, medida en segundos desde la \u00e9poca de Unix.</li> <li>Un valor aleatorio de 5 bytes generado una vez por proceso. Este valor aleatorio es exclusivo de la m\u00e1quina y del proceso.</li> <li>Un contador incremental de 3 bytes, inicializado a un valor aleatorio.</li> </ul> <p>Note</p> <p>En MongoDB, cada documento almacenado en una colecci\u00f3n requiere un campo _id \u00fanico que act\u00faa como clave principal. Si un documento insertado omite el campo _id, el controlador MongoDB genera autom\u00e1ticamente un ID de objeto para el campo _id.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#string","title":"String","text":"<p>BSON String <code>string</code> son UTF-8. En general, los controladores para cada lenguaje de programaci\u00f3n convierten del formato de cadena del lenguaje a UTF-8 al serializar y deserializar BSON. Esto hace posible almacenar la mayor\u00eda de los caracteres internacionales en cadenas BSON con facilidad. Adem\u00e1s, las consultas $regex de MongoDB admiten UTF-8 en la cadena de expresiones regulares.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#timestamp","title":"Timestamp","text":"<p>BSON tiene un tipo <code>timestamp</code> especial para uso interno de MongoDB y no est\u00e1 asociado con el est\u00e1ndar Tipo Date. Este tipo de marca de tiempo interna es un valor de 64 bits donde:</p> <ul> <li>los 32 bits m\u00e1s significativos son un valor <code>time_t</code>(segundos desde la \u00e9poca Unix)</li> <li>los 32 bits menos significativos son un <code>ordinal</code> incremental para operaciones dentro de un segundo determinado.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#22-document-limitations","title":"2.2 Document Limitations","text":"<p>Info</p> <p>Puedes consultar toda la informaci\u00f3n relacionada con documentos en https://www.mongodb.com/docs/manual/core/document/</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#limite-de-tamano-del-documento","title":"L\u00edmite de tama\u00f1o del documento","text":"<p>El tama\u00f1o m\u00e1ximo de documento BSON es de 16MB (ayuda a garantizar que un solo documento no pueda utilizar una cantidad excesiva de RAM ni ancho de banda). Para almacenar documentos que superen el tama\u00f1o m\u00e1ximo, MongoDB proporciona la API GridFS.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#orden-de-campos-del-documento","title":"Orden de campos del documento","text":"<p>A diferencia de los objetos JavaScript, los campos de un documento BSON est\u00e1n ordenados. Pero no asegura que el orden de los campos se respete</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#case-sensitive","title":"Case sensitive","text":"<p>Es sensible a las may\u00fasculas y min\u00fasculas</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#23-comandos-de-administracion","title":"2.3 Comandos de Administraci\u00f3n","text":"<p>Para la administraci\u00f3n del sistema de bases de datos, MongoDB pone a disposici\u00f3n de los usuarios las siguientes utilidades:</p> <ul> <li> <p>mongo: Se trata del shell interactivo de MongoDB que permite insertar, eliminar, actualizar datos y realizar consultas, adem\u00e1s de replicar la informaci\u00f3n, apagar servidores y ejecutar c\u00f3digo JavaScript.</p> </li> <li> <p>mongostat: Herramienta de l\u00ednea de comandos que muestra las estad\u00edsticas de una instancia en ejecuci\u00f3n de MongoDB</p> </li> <li> <p>mongotop: Herramienta de l\u00ednea de comandos que muestra la cantidad de tiempo empleado en la lectura y escritura de datos por parte de la instancia en ejecuci\u00f3n</p> </li> <li> <p>mongosniff: Herramienta de l\u00ednea de comandos que permite hacer un rastreo del tr\u00e1fico de la red que va desde y hacia MongoDB.</p> </li> <li> <p>mongoimport/mongoexport: Herramienta de l\u00ednea de comandos para importar y exportar contenido en o desde .json, .csv o .tsv entre otros.</p> </li> <li> <p>mongodump/mongorestore: Herramienta de l\u00ednea de comandos para la creaci\u00f3n de una exportaci\u00f3n binaria del contenido de la base de datos.</p> </li> </ul> <p>BSON =&gt; JSON</p> <p>Si necesitamos transformar un fichero BSON a JSON, usar\u00edamos el comando (bsondump)[https://www.mongodb.com/docs/database-tools/bsondump/]:</p> <pre><code>bsondump file.bson &gt; file.json\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#3-instalacion-y-uso","title":"3. Instalaci\u00f3n y uso","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#31-docker","title":"3.1 Docker","text":"<p>Vamos a explicar como levantar un contenedor MongoDB con Docker</p> <p>Docker engine vs Docker Desktop on Linux</p> <p>Linux, ya tenemos por defecto docker engine para la ejecuci\u00f3n de contenedores. Si instalamos docker desktop hay que tener en cuenta que docker desktop trabaja sobre un maquina virtual KVM. Esto significa que las im\u00e1genes desplegadas en docker engine no est\u00e1n disponibles en docker desktop y viceversa. Para ello podemos \"jugar\" con el contexto de docker.</p> <p>Para m\u00e1s informaci\u00f3n observa la documentaci\u00f3n oficial de docker: What is the difference between Docker Desktop for Linux and Docker Engine</p> <p> Figura Docker 1. Docker Desktop on Linux. (Fuente: Docker) </p> <p>Para m\u00e1s informaci\u00f3n observa los siguientes recursos:</p> <ul> <li>Why does Docker Desktop for Linux run a VM?</li> <li>Docker Desktop vs. Docker Engine en Ubuntu LTS. Video explicativo</li> </ul> <ol> <li>Instalar MondoDB en Docker</li> <li>Vamos a la imagen oficial de mondodb en docker</li> <li>Observa con detenimiento la informaci\u00f3n que nos facilita la imagen oficial para el despliegue y configuraci\u00f3n de los contenedores</li> <li> <p>Seguimos los pasos, que tienen este formato</p> <p><pre><code>docker run -d --network some-network --name some-mongo \\\n-e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\\n-e MONGO_INITDB_ROOT_PASSWORD=secret \\\nmongo:tag\n</code></pre> En mi caso voy a a\u00f1adir un bind mount para poder luego importar un fichero para el ejemplo. Por tanto ser\u00eda:</p> <p><pre><code>docker run -d --name mongodb-bda1 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=admin --mount type=bind,src=/home/jaime/BigData/BDA/,dst=/home/bda mongo:latest\n</code></pre> Tambi\u00e9n podemos crear el contenedor sin bind mount y copiarla directamente con <code>docker cp</code>:</p> <pre><code>docker run -d --name mongodb-bda1 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=admin mongo:latest\ndocker cp fichero mongo:/directorio\n</code></pre> </li> <li> <p>Para a\u00f1adir documentos, podemos importar datos de un dataset <code>mongoimport</code>, por ejemplo de kaggle, o restaurar una Base de datos <code>mongorestore</code> </p> Dataset restaurantes1.csvsampledata (muestra de datos de MongoDB, la misma de Mongo Atlas) <p>Descarga restaurantes1.csv</p> <pre><code>wget https://atlas-education.s3.amazonaws.com/sampledata.archive\n</code></pre> </li> <li> <p>Copiamos los archivos que vamos a importar/restaurar</p> restaurantes1.csvsampledata <pre><code>docker cp restaurantes1.csv mongodb-bda1:/tmp\n</code></pre> <pre><code>docker cp sampledata.archive mongodb-bda1:/tmp\n</code></pre> </li> <li> <p>Abre una terminal en el contenedor (puedes abrirla por Docker Desptop tambi\u00e9n)</p> <pre><code>docker exec -it mongodb-bda1 sh\n</code></pre> </li> <li> <p>Restauramos/importamos</p> Importamos restaurantes1.csvRestauramos sampledata <pre><code>mongoimport --db=db_ej1_restaurantes --collection=restaurantes --file=tmp/restaurantes1.csv --authenticationDatabase=admin --username=admin --password=admin\n</code></pre> <pre><code>mongorestore --archive=/tmp/sampledata.archive --authenticationDatabase=admin --username=admin --password=admin\n</code></pre> <pre><code>2023-10-13T19:20:03.814+0000    preparing collections to restore from\n2023-10-13T19:20:03.817+0000    reading metadata for sample_airbnb.listingsAndReviews from archive '/tmp/sampledata.archive'\n2023-10-13T19:20:03.817+0000    reading metadata for sample_analytics.accounts from archive '/tmp/sampledata.archive'\n2023-10-13T19:20:03.817+0000    reading metadata for sample_analytics.customers from archive '/tmp/sampledata.archive'\n...\n2023-10-13T19:20:14.010+0000    425367 document(s) restored successfully. 0 document(s) failed to restore.\n</code></pre> </li> <li> <p>Comprobamos. Entrar a la consola de mongo y autorizarnos</p> <pre><code>mongosh\nuse admin\ndb.auth(\"admin\",\"admin\")\n</code></pre> </li> <li> <p>Ver las bases de datos</p> <pre><code>show dbs\n</code></pre> <p> Figura Docker 2. Mostrar las Bases de datos con Mongo. (Fuente: Propia) </p> </li> <li> <p>Usar la Base de datos (use nombre_bd , tambi\u00e9n sirve para crear una Base de datos) </p> <pre><code>use db_ej1_restaurantes\n</code></pre> </li> <li> <p>Ver las colecciones de la base de datos que estamos usando</p> <pre><code>show collections\n</code></pre> </li> <li> <p>Mostramos los documentos</p> <pre><code>db.restaurantes.find()\n</code></pre> </li> </ol>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#32-soluciones-mongo","title":"3.2 Soluciones Mongo","text":"<p>En la actualidad, MongoDB se comercializa mediante tres productos:</p> <ul> <li>MongoDB Enterprise Advanced, versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada.</li> <li>MongoDB Community Edition, Software gratuito para trabajar on-premise, con versiones para diferentes sistemas operativos.</li> <li>Mongo Atlas, como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#mongodb-community-edition","title":"MongoDB Community Edition","text":"<p>Instalaci\u00f3n on-promise de MongoDB. Si deseas una instalaci\u00f3n en local puedes seguir las instrucciones de la documentaci\u00f3n oficial.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#33-mongo-atlas","title":"3.3 Mongo Atlas","text":"<p>Podemos hacer uso de una soluci\u00f3n gratuita cloud de Mongo Atlas ofrecida por MongoDB, con 512MB de RAM compartida y 5GB de almacenamiento</p> Figura Atlas 1: MongoDB Atlas. (Fuente: Propia) <p>Para ello seguimos todos los pasos: Getting Started MongoDB</p> Figura Atlas 2: MongoDB Atlas. Get Started. (Fuente: Propia) <ol> <li> <p>Creamos cuenta</p> </li> <li> <p>A\u00f1adir tu direcci\u00f3n ip a la lista de ips de acceso al cluster</p> <ul> <li> <p>Men\u00fa izquierdo =&gt; Network Access</p> </li> <li> <p>Add IP Address (Derecha)</p> </li> </ul> </li> </ol> Figura Atlas 3: MongoDB Atlas. A\u00f1adir IP. (Fuente: MongoDB) <ol> <li>Desplegamos cluster gratuito. Creamos una Organizaci\u00f3n, Proyecto y una Base de Datos del cluster</li> </ol> <ul> <li>Crear Organizaci\u00f3n</li> </ul> Figura Atlas 4: A\u00f1adir Organizaci\u00f3n. (Fuente: Propia) <ul> <li>Crear Proyecto</li> </ul> Figura Atlas 5: A\u00f1adir Proyecto. (Fuente: Propia) <ul> <li>Nueva Base de datos</li> </ul> Figura Atlas 6: A\u00f1adir Base de datos. (Fuente: Propia) <ul> <li>Creando cluster</li> </ul> Figura Atlas 7: Cluster. (Fuente: Propia) <ol> <li>Crear un usuario para la Base de Datos</li> </ol> Figura 8 Atlas: Nuevo usuario. (Fuente: Propia) <ol> <li> <p>Cargar contenido en nuestra Base de Datos. Vamos a usar uno de los Data Set de ejemplo facilitados por el propio MongoDB.</p> <ul> <li>Damos a la opci\u00f3n de los ... y Load Data Sample.</li> </ul> </li> </ol> Figura Atlas 9. Cargar datos a la Base de Datos. (Fuente: Propia) <ul> <li>Browser Collections</li> </ul> Figura Atlas 10: Browser Collections. (Fuente: Propia) <ol> <li>Conectarte a tu cluster. Hay varias formas de conexi\u00f3n a tu cluster.</li> </ol> <ul> <li> <p>A MongoDB driver, un controlador para comunicarse con su base de datos MongoDB mediante programaci\u00f3n. Soporta varios lenguajes de programaci\u00f3n.</p> </li> <li> <p>MongoDB Compass, una GUI para sus datos de MongoDB. Puede utilizar Compass para explorar, modificar y visualizar sus datos.</p> </li> <li> <p>The MongoDB Shell, una interfaz de l\u00ednea de comandos interactiva para MongoDB. Puedes usar mongosh para insertar e interactuar con datos en su cl\u00faster Atlas.</p> </li> <li> <p>MongoDB for VS Code. Para trabajar MongoDB en VS Code</p> </li> <li> <p>Atlas SQL. Para conectar f\u00e1cilmente a Atlas para visualizaci\u00f3n y an\u00e1lisis de datos</p> </li> </ul> Figura Atlas 11. Formas de conexi\u00f3n a Atlas. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#34-mongodb-shell","title":"3.4 MongoDB Shell","text":"<p>Para esta conexi\u00f3n, solo necesitamos usar el comando <code>mongosh</code> con las credenciales configuradas anteriormente. Siguiendo los pasos dentro de connect en la opci\u00f3n Mongo Shell que nos da Mongo Atlas (figura anterior) </p> Figura Atlas 12. Connect Mongo Shell. (Fuente: Propia) <p>Y escribiendo despu\u00e9s la contrase\u00f1a establecida nos conectamos a nuestro al servicio cloud</p> Figura Atlas 13. Connect mongosh. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#35-mongodb-para-vs-code","title":"3.5 MongoDB para VS Code","text":"<ol> <li>Para ello usamos la extensi\u00f3n <code>MongoDB for VS Code</code> </li> </ol> Figura MongoDB for VSCode 1. Extensi\u00f3n. (Fuente: Propia) <ol> <li>Damos a la opci\u00f3n de connect en nuestro MongoDB Atlas y seleccionamos MongoDB for VSCode. Seguimos los pasos que nos indican. Copiamos la url del punto 3 y le a\u00f1adimos nuestra contrase\u00f1a</li> </ol> Figura MongoDB for VSCode 2. Conectando con Mondo Atlas. (Fuente: Propia) <ol> <li>Vamos a nuestra extensi\u00f3n y le damos a connecting string y a\u00f1adimos la url con nuestra contrase\u00f1a.</li> </ol> Figura MongoDB for VSCode 3. Conectado con Mondo Atlas. (Fuente: Propia) <ol> <li>Seguimos los pasos y creamos un New Playground. Como test, ejecutamos (bot\u00f3n play arriba a la derecha) el playground creado por defecto. Observa el resultado, la salida en consola y los nuevos datos creados en nuestra Cloud Database.</li> </ol> Figura MongoDB for VSCode 4. Ejecutando playground. (Fuente: Propia) <ol> <li> <p>Observa el c\u00f3digo del playground. Presta especial atenci\u00f3n a las palabras reservadas <code>db, use, consolo.log(), print(), printjson()</code>. Para mas informaci\u00f3n de uso consulta la documentaci\u00f3n oficial</p> </li> <li> <p>Tambi\u00e9n puedes ejecutar una shell de mongoDB usando bot\u00f3n derecho sobre la conexi\u00f3n y pulsando Launch MongoDB Shell. Para ello debes tener en tu sistema MongoDB Shell</p> </li> </ol> Figura MongoDB for VSCode 5. MongoDB Shell en VSCode. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#36-mongodb-compass","title":"3.6 MongoDB Compass","text":"<p>MongoDB Compass es una potente GUI para consultar, agregar y analizar sus datos de MongoDB en un entorno visual. MongoDB Compass viene con 3 versiones diferentes:</p> <ul> <li>Compass: La full version de MongoDB Compass, con todas las caracter\u00edsticas y capacidades.</li> <li>Readonly Edition: Esta versi\u00f3n se limita estrictamente a operaciones de lectura, y se eliminan todas las capacidades de escritura y eliminaci\u00f3n (ideal para Analista de Datos).</li> <li>Isolated Edition: Esta versi\u00f3n deshabilita todas las conexiones de red excepto la conexi\u00f3n a la instancia de MongoDB.</li> </ul> <p>Info</p> <p>Compass es de uso gratuito y est\u00e1 disponible en origen, y puede ejecutarse en macOS, Windows y Linux.</p> <p>Puedes ver toda la documentaci\u00f3n y los pasos de instalaci\u00f3n en la documentaci\u00f3n oficial</p> <p>Para comprobar su funcionamiento, vamos a conectarnos a nuestra Base de Datos en Atlas:</p> <ol> <li>Instalamos siguiendo los pasos de la documentaci\u00f3n oficial.</li> <li>Vamos a las opciones de conexi\u00f3n y elegimos la opci\u00f3n compass. Copiamos el string de conexi\u00f3n a\u00f1adiendo nuestra contrase\u00f1a.</li> </ol> Figura MongoDB Compass 1. Conexi\u00f3n mediante Compass. (Fuente: Propia) <ol> <li>En Compass, abrimos la opci\u00f3n de conexi\u00f3n y copiamos nuestro string de conexi\u00f3n.</li> </ol> Figura MongoDB Compass 2. String de conexi\u00f3n. (Fuente: Propia) <ol> <li>Ya podemos usar la GUI de Compass.</li> </ol> Figura MongoDB Compass 3. GUI de Compass 1. (Fuente: Propia) <ol> <li>Visualizar las Base de Datos, Collections y Documentos. Debajo tambi\u00e9n se nos abre una conexi\u00f3n con <code>mongosh</code> para realizar las operaciones que queramos en terminal si lo deseamos (la cual podemos minimizar).</li> </ol> Figura MongoDB Compass 4. GUI de Compass 2. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#37-mongodb-tools","title":"3.7 MongoDB Tools","text":"<p>The MongoDB Database Tools son una colecci\u00f3n de utilidades de l\u00ednea de comandos para trabajar con una implementaci\u00f3n de MongoDB. Las herramientas de base de datos incluyen los siguientes binarios:</p> <ol> <li>Binary Import / Export:</li> </ol> <ul> <li><code>mongodump</code>: Crea una exportaci\u00f3n binaria del contenido de un demonio mongod.</li> <li><code>mongorestore</code>: Restaura datos de un volcado de base de datos mongodump en un mongod o mongos (Para un shared cluster, las instancias de _mongos proporcionan la interfaz entre las aplicaciones cliente y el cl\u00faster fragmentado.)_</li> <li><code>bsondump</code>: Convierte archivos BSON dump into JSON.</li> </ul> <ol> <li>Data Import / Export:</li> </ol> <ul> <li><code>mongoimport</code>: Importa el contenido desde un archivo externo E_xtended JSON, CSV, or TSV._</li> <li><code>mongoexport</code>:Produce una exportaci\u00f3n JSON o CSV de datos almacenados en un instancia de mongod.</li> </ul> <ol> <li>Diagnostic Tools:</li> </ol> <ul> <li><code>mongostat</code>: Proporciona una descripci\u00f3n general r\u00e1pida del estado de una instancia de mongod o mongos actualmente en ejecuci\u00f3n.</li> <li><code>mongotop</code>: Proporciona una descripci\u00f3n general del tiempo que una instancia de mongod dedica a leer y escribir datos.</li> </ul> <ol> <li>GridFS Tools:</li> </ol> <ul> <li><code>mongofiles</code>: admite la manipulaci\u00f3n de archivos almacenados en su instancia de MongoDB en objetos GridFS.</li> </ul> <p>Puedes optar por descargar estas herramientas en local o utilizar algunas de las opciones mostradas anteriormente. La instancia de MongoDB creada en Docker tambi\u00e9n tiene por defecto todas estas herramientas.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#4-operaciones-crud","title":"4. Operaciones CRUD","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#41-create-operations","title":"4.1 Create Operations","text":"<p>Las operaciones de creaci\u00f3n o inserci\u00f3n agregan nuevos documentos a una colecci\u00f3n. Si la colecci\u00f3n no existe actualmente, las operaciones de inserci\u00f3n crear\u00e1n la colecci\u00f3n.</p> <p>MongoDB proporciona los siguientes m\u00e9todos para insertar documentos en una colecci\u00f3n:</p> <ul> <li>db.collection.insertOne()</li> <li>db.collection.insertMany() </li> </ul> <p>En MongoDB, las operaciones de inserci\u00f3n tienen como objetivo una \u00fanica colecci\u00f3n. Todas las operaciones de escritura en MongoDB son at\u00f3micas al nivel de un solo documento.</p> Figura MongoDB Create Operations. (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#insertar-un-documento","title":"Insertar un documento","text":"<p><code>db.collection.insertOne()</code> inserta un \u00fanico documento dentro de una collection.</p> <p>Note</p> <p>El siguiente c\u00f3digo inserta un nuevo documento en la colecci\u00f3n de inventario. Si el documento no especifica un campo _id, MongoDB agrega el campo _id con un valor ObjectId al nuevo documento.</p> <pre><code>db.inventory.insertOne(\n   { item: \"canvas\", qty: 100, tags: [\"cotton\"], size: { h: 28, w: 35.5, uom: \"cm\" } }\n)\n</code></pre> <p><code>insertOne()</code> devuelve un documento que incluye el valor del campo _id del documento reci\u00e9n insertado. </p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#insertar-varios-documentos","title":"Insertar varios documentos","text":"<p><code>db.collection.insertMany()</code> puede insertar m\u00faltiples documentos a una collection. Pasa un array of documentos al m\u00e9todo.</p> <pre><code>db.inventory.insertMany([\n   { item: \"journal\", qty: 25, tags: [\"blank\", \"red\"], size: { h: 14, w: 21, uom: \"cm\" } },\n   { item: \"mat\", qty: 85, tags: [\"gray\"], size: { h: 27.9, w: 35.5, uom: \"cm\" } },\n   { item: \"mousepad\", qty: 25, tags: [\"gel\", \"blue\"], size: { h: 19, w: 22.85, uom: \"cm\" } }\n])\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#42-read-operations","title":"4.2 Read Operations","text":"<p>En primer lugar, antes de aprender como se realizan consultas en MongoDB, vamos a ver cuales son los operadores que facilita MongoDB para poder realizar todo tipo de consultas. </p> <p>Seguidamente veremos como realizar consultas.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#421-query-operators","title":"4.2.1 Query Operators","text":"<p>Mostraremos un resumen de todos los operadores (salvo aggregation operators que veremos m\u00e1s adelante cuando expliquemos agregaciones). Para m\u00e1s detalle, explicaci\u00f3n y ejemplo de cada operador, la puedes encontrar en la documentaci\u00f3n oficial</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#query-selectors","title":"Query Selectors","text":"<ol> <li>Comparison</li> </ol> Figura MongoDB Operators 1. Comparison Operators (Fuente: MongoDB) <ol> <li>Logical</li> </ol> Figura MongoDB Operators 2. Logical Operators _(Fuente: MongoDB)_ <ol> <li>Element</li> </ol> Figura MongoDB Operators 3. Element Operators (Fuente: MongoDB) <ol> <li>Evaluation</li> </ol> Figura MongoDB Operators 4. Evaluation Operators (Fuente: MongoDB) <ol> <li>Geospatial</li> </ol> Figura MongoDB Operators 5. Geospatial Operators (Fuente: MongoDB) <ol> <li>Array</li> </ol> Figura MongoDB Operators 6. Array Operators (Fuente: MongoDB) <ol> <li>Bitwise</li> </ol> Figura MongoDB Operators 7. Bitwise Operators (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#projection-operators","title":"Projection Operators","text":"Figura MongoDB Operators 8. Projection Operators (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#miscellaneous-operators","title":"Miscellaneous Operators","text":"Figura MongoDB Operators 9. Miscellaneous Operators (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#422-consultas-mondodb","title":"4.2.2 Consultas MondoDB","text":"<p>Para usar como ejemplo a\u00f1adimos los siguientes documentos. Primero nos aseguramos de tener la collection vac\u00eda.</p> <pre><code>db.inventory.deleteMany({})\n</code></pre> <pre><code>db.inventory.insertMany([\n   { item: \"journal\", qty: 25, size: { h: 14, w: 21, uom: \"cm\" }, status: \"A\" },\n   { item: \"notebook\", qty: 50, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"A\" },\n   { item: \"paper\", qty: 100, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"D\" },\n   { item: \"planner\", qty: 75, size: { h: 22.85, w: 30, uom: \"cm\" }, status: \"D\" },\n   { item: \"postcard\", qty: 45, size: { h: 10, w: 15.25, uom: \"cm\" }, status: \"A\" }\n]);\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#seleccionar-todos-los-documentos-de-un-collection","title":"Seleccionar todos los documentos de un Collection","text":"<p>Para seleccionar todos los documentos de la colecci\u00f3n, pasamos un documento vac\u00edo como par\u00e1metro de filtro de consulta al m\u00e9todo de b\u00fasqueda. Esta operaci\u00f3n utiliza un predicado de filtro de {}:</p> MongoshEquivalente en SQL <pre><code>db.inventory.find( {} )\n</code></pre> <pre><code>SELECT * FROM inventory\n</code></pre> <p>Para poder realizar cualquier tipo de consulta/b\u00fasqueda usaremos dentro de las {} los filtros/operadores (vistos en el punto anterior) para realizar todo tipo de consultas.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#especificando-una-condicion-de-igualdad","title":"Especificando una condici\u00f3n de igualdad","text":"<p>Para especificar condiciones de igualdad, utiliza la expresi\u00f3n <code>&lt;field&gt;:&lt;value&gt;</code> como filtro de la consulta del documento:</p> <pre><code>{ &lt;field1&gt;: &lt;value1&gt;, ... }\n</code></pre> <p>El siguiente ejemplo selecciona de la colecci\u00f3n de inventario todos los documentos cuyo estado es \"D\":</p> MongoshEquivalente en SQL <pre><code>db.inventory.find( { status: \"D\" } )\n</code></pre> <pre><code>SELECT * FROM inventory WHERE status = \"D\"\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#especificando-condiciones-mediante-operadores-de-consulta","title":"Especificando condiciones mediante operadores de consulta","text":"<p>Un filtro de consulta de un documento  puede utilizar los operadores de consulta para especificar condiciones de la siguiente forma:</p> <pre><code>{ &lt;field1&gt;: { &lt;operator1&gt;: &lt;value1&gt; }, ... }\n</code></pre> <p>El siguiente ejemplo recupera todos los documentos de la colecci\u00f3n donde el estado es \"A\" o \"D\":</p> MongoshEquivalente en SQL <p><pre><code>db.inventory.find( { status: { $in: [ \"A\", \"D\" ] } } )\n</code></pre> Podr\u00eda usarse tambi\u00e9n el operador <code>$or</code></p> <pre><code>SELECT * FROM inventory WHERE status in (\"A\", \"D\")\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#usando-condicion-and","title":"Usando condici\u00f3n <code>AND</code>","text":"<p>Una consulta compuesta puede especificar condiciones para m\u00e1s de un campo en los documentos de la colecci\u00f3n. Impl\u00edcitamente, una conjunci\u00f3n l\u00f3gica <code>AND</code> conecta las cl\u00e1usulas de una consulta compuesta de modo que la consulta selecciona los documentos de la colecci\u00f3n que coinciden con todas las condiciones.</p> <p>El siguiente ejemplo recupera todos los documentos de la colecci\u00f3n donde el estado es igual a \"A\" y la cantidad es menor que <code>$lt</code> 30:</p> MongoshEquivalente en SQL <pre><code>db.inventory.find( { status: \"A\", qty: { $lt: 30 } } )\n</code></pre> <pre><code>SELECT * FROM inventory WHERE status = \"A\" AND qty &lt; 30\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#usando-condicion-or","title":"Usando condici\u00f3n <code>OR</code>","text":"<p>Con el operador <code>$or</code>, puedes especificar una consulta compuesta que una cada cl\u00e1usula con una conjunci\u00f3n OR l\u00f3gica para que la consulta seleccione los documentos de la colecci\u00f3n que coincidan con al menos una condici\u00f3n.</p> <p>El siguiente ejemplo recupera todos los documentos de la colecci\u00f3n donde el estado es igual a \"A\" o la cantidad es menor que <code>$lt</code> 30:</p> MongoshEquivalente en SQL <pre><code>db.inventory.find( { $or: [ { status: \"A\" }, { qty: { $lt: 30 } } ] } )\n</code></pre> <pre><code>SELECT * FROM inventory WHERE status = \"A\" OR qty &lt; 30\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#especificando-condiciones-and-y-or","title":"Especificando condiciones <code>AND</code> y <code>OR</code>","text":"<p>En el siguiente ejemplo, el documento de consulta compuesta selecciona todos los documentos de la colecci\u00f3n donde el estado es igual a \"A\" y la cantidad es menor que <code>$lt</code> 30 o el elemento comienza con el car\u00e1cter p:</p> MongoshEquivalente en SQL <pre><code>db.inventory.find( {\n status: \"A\",\n $or: [ { qty: { $lt: 30 } }, { item: /^p/ } ]\n} )\n</code></pre> <p>Podr\u00eda usarse otro operador de filtro:</p> <pre><code>db.inventory.find( {\n status: \"A\",\n $or: [ { qty: { $lt: 30 } }, { item: { $regex: '^p' } } ]\n} )\n</code></pre> <pre><code>SELECT * FROM inventory WHERE status = \"A\" AND ( qty &lt; 30 OR item LIKE \"p%\")\n</code></pre> <p>Con esta introducci\u00f3n podemos tener una buena introducci\u00f3n a las query operations de MongoDB. Pero existen otros muchos tipos:</p> <ul> <li>Query on Embedded/Nested Documents</li> <li>Query an Array</li> <li>Query an Array of Embedded Documents</li> <li>Project Fields to Return from Query</li> <li>Query for Null or Missing Fields</li> </ul> <p>Puedes consultarlas en la documentaci\u00f3n oficial</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#43-update-operations","title":"4.3 Update Operations","text":"<p>Como en el caso de las consultas, vamos a ver primero los operadores de actualizaci\u00f3n y seguidamente como realizamos actualizaciones en MongoDB</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#431-update-operators","title":"4.3.1 Update Operators","text":"<p>Los siguientes modificadores est\u00e1n disponibles para su uso en operaciones de actualizaci\u00f3n.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#sintaxis","title":"Sintaxis","text":"<pre><code>{\n   &lt;operator1&gt;: { &lt;field1&gt;: &lt;value1&gt;, ... },\n   &lt;operator2&gt;: { &lt;field2&gt;: &lt;value2&gt;, ... },\n   ...\n}\n</code></pre> <p>Para m\u00e1s detalle, explicaci\u00f3n y ejemplo de cada operador, la puedes encontrar en la documentaci\u00f3n oficial. Update Operator</p> <ol> <li>Field update operators</li> </ol> Figura MongoDB Update Operators 1. Field (Fuente: MongoDB) <ol> <li>Array update operators</li> </ol> Figura MongoDB Update Operators 2. Array (Fuente: MongoDB) <ol> <li>Modifiers update operators</li> </ol> Figura MongoDB Update Operators 3. Modifiers (Fuente: MongoDB) <ol> <li>Bitwise update operators</li> </ol> Figura MongoDB Update Operators 4. Bitwise (Fuente: MongoDB)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#432-actualizaciones","title":"4.3.2 Actualizaciones","text":"<p>Para usar como ejemplo a\u00f1adimos los siguientes documentos. Primero nos aseguramos de tener la collection vac\u00eda.</p> <pre><code>db.inventory.deleteMany({})\n</code></pre> <pre><code>db.inventory.insertMany( [\n   { item: \"canvas\", qty: 100, size: { h: 28, w: 35.5, uom: \"cm\" }, status: \"A\" },\n   { item: \"journal\", qty: 25, size: { h: 14, w: 21, uom: \"cm\" }, status: \"A\" },\n   { item: \"mat\", qty: 85, size: { h: 27.9, w: 35.5, uom: \"cm\" }, status: \"A\" },\n   { item: \"mousepad\", qty: 25, size: { h: 19, w: 22.85, uom: \"cm\" }, status: \"P\" },\n   { item: \"notebook\", qty: 50, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"P\" },\n   { item: \"paper\", qty: 100, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"D\" },\n   { item: \"planner\", qty: 75, size: { h: 22.85, w: 30, uom: \"cm\" }, status: \"D\" },\n   { item: \"postcard\", qty: 45, size: { h: 10, w: 15.25, uom: \"cm\" }, status: \"A\" },\n   { item: \"sketchbook\", qty: 80, size: { h: 14, w: 21, uom: \"cm\" }, status: \"A\" },\n   { item: \"sketch pad\", qty: 95, size: { h: 22.85, w: 30.5, uom: \"cm\" }, status: \"A\" }\n] );\n</code></pre> <p>MongoDB proporciona los siguientes m\u00e9todos para actualizar documentos en una colecci\u00f3n:</p> <ul> <li>db.collection.updateOne(, , ) <li>db.collection.updateMany(, , ) <li>db.collection.replaceOne(, , ) <p>Para la actualizaci\u00f3n de documentos de una colletion usamos la siguiente sintaxis</p> <pre><code>{\n  &lt;update operator&gt;: { &lt;field1&gt;: &lt;value1&gt;, ... },\n  &lt;update operator&gt;: { &lt;field2&gt;: &lt;value2&gt;, ... },\n  ...\n}\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#update-a-single-document","title":"Update a Single Document","text":"<p>El siguiente ejemplo utiliza el m\u00e9todo <code>db.collection.updateOne()</code> en la colecci\u00f3n de inventario para actualizar el primer documento donde elemento es igual a \"paper\":</p> <pre><code>db.inventory.updateOne(\n   { item: \"paper\" },\n   {\n     $set: { \"size.uom\": \"cm\", status: \"P\" },\n     $currentDate: { lastModified: true }\n   }\n)\n</code></pre> <p>La operaci\u00f3n de actualizaci\u00f3n:</p> <ul> <li>utiliza el operador <code>$set</code> para actualizar el valor del campo s<code>ize.uom</code> a \"cm\" y el valor del campo de estado a \"P\",</li> <li>utiliza el operador <code>$currentDate</code> para actualizar el valor del campo lastModified a la fecha actual. Si el campo lastModified no existe, <code>$currentDate</code> crear\u00e1 el campo.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#update-multiple-documents","title":"Update Multiple Documents","text":"<p>El siguiente ejemplo utiliza el m\u00e9todo <code>db.collection.updateMany()</code> en la colecci\u00f3n de inventory para actualizar todos los documentos donde la cantidad es inferior a 50:</p> <pre><code>db.inventory.updateMany(\n   { \"qty\": { $lt: 50 } },\n   {\n     $set: { \"size.uom\": \"in\", status: \"P\" },\n     $currentDate: { lastModified: true }\n   }\n)\n</code></pre> <p>La operaci\u00f3n de actualizaci\u00f3n:</p> <ul> <li>utiliza el operador <code>$set</code> para actualizar el valor del campo <code>size.uom</code> a \"in\" y el valor del campo de estado a \"P\",</li> <li>utiliza el operador <code>$currentDate</code> para actualizar el valor del campo lastModified a la fecha actual. Si el campo lastModified no existe, <code>$currentDate</code> crear\u00e1 el campo.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#replace-a-document","title":"Replace a Document","text":"<p>Para reemplazar todo el contenido de un documento excepto el campo <code>_id</code>, pase un documento completamente nuevo como segundo argumento a <code>db.collection.replaceOne()</code>.</p> <p>Al reemplazar un documento, el documento de reemplazo debe consistir \u00fanicamente en pares de campo/valor; es decir, no incluir actualizar operadores expresiones.</p> <p>El documento de reemplazo puede tener campos diferentes a los del documento original. En el documento de reemplazo, puede omitir el campo <code>_id</code> ya que el campo <code>_id</code> es inmutable; sin embargo, si incluye el campo <code>_id</code>, debe tener el mismo valor que el valor actual.</p> <p>El siguiente ejemplo reemplaza el primer documento de la colecci\u00f3n de inventario donde elemento: \"papel\":</p> <pre><code>db.inventory.replaceOne(\n   { item: \"paper\" },\n   { item: \"paper\", instock: [ { warehouse: \"A\", qty: 60 }, { warehouse: \"B\", qty: 40 } ] }\n)\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#otros-ejemplos","title":"Otros ejemplos","text":"<pre><code>db.ships.updateOne({name : {$eq: 'USS-Enterprise-D'}}, {$set : {name: 'USS Something'}})\ndb.ships.updateOne({name : {$eq: 'USS Something'}}, {$set : {name: 'USS Prometheus', class: 'Prometheus'}})\ndb.ships.updateOne({name : {$eq: 'USS Prometheus'}}, {$unset : {operator: 1}})\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#433-metodos-adicionales","title":"4.3.3 M\u00e9todos adicionales","text":"<p>MongoDB tiene otros m\u00e9todos menos usados para la actualizaci\u00f3n de documentos en las colecciones:</p> <ul> <li>db.collection.findOneAndReplace()</li> <li>db.collection.findOneAndUpdate()</li> <li>db.collection.findAndModify()</li> <li>db.collection.bulkWrite()</li> </ul> <p>Puedes obtener m\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial. Update methods</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#44-delete-operations","title":"4.4 Delete Operations","text":"<p>Para usar como ejemplo a\u00f1adimos los siguientes documentos. Primero nos aseguramos de tener la collection vac\u00eda.</p> <pre><code>db.inventory.deleteMany({})\n</code></pre> <pre><code>db.inventory.insertMany( [\n   { item: \"journal\", qty: 25, size: { h: 14, w: 21, uom: \"cm\" }, status: \"A\" },\n   { item: \"notebook\", qty: 50, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"P\" },\n   { item: \"paper\", qty: 100, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"D\" },\n   { item: \"planner\", qty: 75, size: { h: 22.85, w: 30, uom: \"cm\" }, status: \"D\" },\n   { item: \"postcard\", qty: 45, size: { h: 10, w: 15.25, uom: \"cm\" }, status: \"A\" },\n] );\n</code></pre> <p>MongoDB proporciona los siguientes m\u00e9todos para borrar documentos en una colecci\u00f3n:</p> <ul> <li>db.collection.deleteMany()</li> <li>db.collection.deleteOne()</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#delete-all-documents","title":"Delete All Documents","text":"<p>Para eliminar todos los documentos de una colecci\u00f3n, pasamos un documento de filtro vac\u00edo {} al m\u00e9todo <code>db.collection.deleteMany()</code>.</p> <pre><code>db.inventario.deleteMany({})\n</code></pre> <p>El m\u00e9todo devuelve un documento con el estado de la operaci\u00f3n.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#delete-all-documents-that-match-a-condition","title":"Delete All Documents that Match a Condition","text":"<p>Podemos especificar criterios o filtros que identifiquen los documentos que se eliminar\u00e1n. Los filtros utilizan la misma sintaxis que las operaciones de lectura.</p> <p>Para especificar condiciones de igualdad, utilizamos expresiones <code>&lt;campo&gt;:&lt;valor&gt;</code> en el documento de filtro de consulta:</p> <pre><code>{ &lt;campo1&gt;: &lt;valor1&gt;,... }\n</code></pre> <p>Una consulta de documento con filtro podemos utilizar los operadores de consulta para especificar condiciones de la siguiente forma:</p> <pre><code>{ &lt;campo1&gt;: { &lt;operador1&gt;: &lt;valor1&gt; }, ... }\n</code></pre> <p>Entonces, para eliminar todos los documentos que coincidan con un criterio de eliminaci\u00f3n, pasamos un filtro por par\u00e1metro al m\u00e9todo <code>deleteMany()</code>.</p> <p>Lo vemos en el siguiente ejemplo, donde eliminamos todos los documentos de la colecci\u00f3n de inventario donde el campo de estado es \"A\":</p> <pre><code>db.inventory.deleteMany({ status: \"A\" })\n</code></pre> <p>El m\u00e9todo devuelve un documento con el estado de la operaci\u00f3n.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#delete-only-one-document-that-matches-a-condition","title":"Delete Only One Document that Matches a Condition","text":"<p>Para eliminar como m\u00e1ximo un \u00fanico documento que coincida con un filtro espec\u00edfico (aunque varios documentos puedan coincidir con el filtro especificado), utilizamos el m\u00e9todo <code>db.collection.deleteOne()</code>.</p> <p>En el siguiente ejemplo elimina el primer documento cuyo estado es \"D\":</p> <pre><code>db.inventory.deleteOne( {status: \"D\" } )\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#otros-ejemplos_1","title":"Otros Ejemplos","text":"<pre><code>db.ships.deleteOne({name : 'USS Prometheus'})\ndb.ships.deleteOne({name:{$regex:'^A*'}})\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#5-aggregations","title":"5. Aggregations","text":"<p>Las operaciones de agregaci\u00f3n procesan m\u00faltiples documentos y devuelven resultados calculados. Puede utilizar operaciones de agregaci\u00f3n para:</p> <ul> <li>Agrupar valores desde varios documentos a la vez.</li> <li>Realice operaciones sobre los datos agrupados para devolver un \u00fanico resultado.</li> <li>Analizar los cambios de datos a lo largo del tiempo.</li> </ul> <p>Para realizar operaciones de agregaci\u00f3n, puede utilizar:</p> <ul> <li>Aggregation pipelines, que son el m\u00e9todo preferido para realizar agregaciones.</li> <li>Single purpose aggregation methods, que son simples pero carecen de las capacidades de una Aggregation pipelines.</li> </ul> <p>Un proceso de agregaci\u00f3n consta de una o m\u00e1s etapas que procesan documentos:</p> <ul> <li>Cada etapa realiza una operaci\u00f3n en los documentos de entrada. Por ejemplo, una etapa puede filtrar documentos, agrupar documentos y calcular valores.</li> <li>Los documentos que salen de una etapa pasan a la siguiente etapa.</li> <li>Una Aggregation Pipelines puede devolver resultados por grupos de documentos. Por ejemplo, devuelva los valores total, promedio, m\u00e1ximo y m\u00ednimo.</li> </ul> <p>Para poder realizar estas agrupaciones de datos por etapas MongoDB proporciona una larga lista de Operadores</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#51-operator-aggregation-pipelines-stages","title":"5.1 Operator Aggregation Pipelines Stages","text":"<p>Hay una larga lista de operadores que te permiten establecer cada una de las etapas de los Aggregation Pipelines. En la documentaci\u00f3n oficial tienes la lista completa.</p> <p>A continuaci\u00f3n extraemos los m\u00e1s utilizados a modo de resumen:</p> <p> Estado Descripci\u00f3n $project Cambia la forma de cada documento en la secuencia, por ejemplo agregando nuevos campos o eliminando campos existentes. Por cada documento de entrada, genera un documento. $match Filtra el stream de documentos para permitir que solo los documentos coincidentes pasen sin modificaciones a la siguiente etapa del proceso. <code>$match</code> utiliza consultas est\u00e1ndar de MongoDB. Para cada documento de entrada, genera un documento (una coincidencia) o cero documentos (ninguna coincidencia). $group Agrupa los documentos de entrada por una expresi\u00f3n de identificador especificada y aplica las expresiones del acumulador, si se especifican, a cada grupo. Consume todos los documentos de entrada y genera un documento por cada grupo distinto. Los documentos de salida solo contienen el campo identificador y, si se especifica, campos acumulados. $sort Reordena el flujo de documentos seg\u00fan una clave de clasificaci\u00f3n especificada. S\u00f3lo cambia el orden; los documentos permanecen sin modificaciones. Por cada documento de entrada, genera un documento. $skip Omite los primeros n documentos donde n es el n\u00famero de omisi\u00f3n especificado y pasa los documentos restantes sin modificar a la canalizaci\u00f3n. Para cada documento de entrada, genera cero documentos (para los primeros n documentos) o un documento (si est\u00e1 despu\u00e9s de los primeros n documentos). $limit Pasa los primeros n documentos sin modificar al pipeline, donde n es el l\u00edmite especificado. Para cada documento de entrada, genera un documento (para los primeros n documentos) o cero documentos (despu\u00e9s de los primeros n documentos). $unwind Deconstruye un campo de matriz a partir de los documentos de entrada para generar un documento para cada elemento. Cada documento de salida reemplaza la matriz con un valor de elemento. Para cada documento de entrada, genera n documentos donde n es el n\u00famero de elementos de la matriz y puede ser cero para una matriz vac\u00eda. <p>Tabla 1: Operadores de aggregations m\u00e1s usados en MongoDB </p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#52-aggregation-pipelines","title":"5.2 Aggregation Pipelines","text":"<p>Una vez vista la informaci\u00f3n sobre Agreggation vamos a aprender a usarlo usando peque\u00f1os ejemplos. Para ello vamos a tomar como base la siguiente colecci\u00f3n <code>orders</code> de pedidos de pizzas. Recuerda asegurarte de tener la collection vac\u00eda.</p> <pre><code>db.orders.deleteMany({})\n</code></pre> <p>Info</p> <p>Aggregation Pipelines que se ejecutan con el m\u00e9todo <code>db.collection.aggregate()</code> no modifican los documentos de una colecci\u00f3n, a menos que la canalizaci\u00f3n contenga una etapa <code>$merge</code> o <code>$out</code>.</p> <pre><code>db.orders.insertMany( [\n   { _id: 0, name: \"Pepperoni\", size: \"small\", price: 19,\n     quantity: 10, date: ISODate( \"2021-03-13T08:14:30Z\" ) },\n   { _id: 1, name: \"Pepperoni\", size: \"medium\", price: 20,\n     quantity: 20, date : ISODate( \"2021-03-13T09:13:24Z\" ) },\n   { _id: 2, name: \"Pepperoni\", size: \"large\", price: 21,\n     quantity: 30, date : ISODate( \"2021-03-17T09:22:12Z\" ) },\n   { _id: 3, name: \"Cheese\", size: \"small\", price: 12,\n     quantity: 15, date : ISODate( \"2021-03-13T11:21:39.736Z\" ) },\n   { _id: 4, name: \"Cheese\", size: \"medium\", price: 13,\n     quantity:50, date : ISODate( \"2022-01-12T21:23:13.331Z\" ) },\n   { _id: 5, name: \"Cheese\", size: \"large\", price: 14,\n     quantity: 10, date : ISODate( \"2022-01-12T05:08:13Z\" ) },\n   { _id: 6, name: \"Vegan\", size: \"small\", price: 17,\n     quantity: 10, date : ISODate( \"2021-01-13T05:08:13Z\" ) },\n   { _id: 7, name: \"Vegan\", size: \"medium\", price: 18,\n     quantity: 10, date : ISODate( \"2021-01-13T05:10:13Z\" ) }\n] )\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#calcular-la-cantidad-total-del-pedido","title":"Calcular la cantidad total del pedido","text":"<p>Este ejemplo contiene dos etapas y devuelve la cantidad total del pedido de pizzas de tama\u00f1o \"medium\" agrupadas por nombre de pizza:</p> <pre><code>db.orders.aggregate( [\n\n   // Etapa 1: Filtramos los documentos por el tama\u00f1o de la pizza\n   {\n      $match: { size: \"medium\" }\n   },\n\n   // Etapa 2: Agrupe los documentos restantes por nombre de pizza y calcule la cantidad total\n   {\n      $group: { _id: \"$name\", totalQuantity: { $sum: \"$quantity\" } }\n   }\n\n] )\n</code></pre> <p>La etapa de <code>$match</code>:</p> <ul> <li>Filtra los documentos de pedido de pizza a pizzas de tama\u00f1o mediano.</li> <li>Pasa los documentos restantes a la etapa de <code>$group</code>.</li> </ul> <p>La fase de <code>$group</code>:</p> <ul> <li>Agrupa los documentos restantes por nombre de pizza.</li> <li>Utiliza <code>$sum</code> para calcular la cantidad total del pedido para cada <code>$name</code> de pizza. El total se almacena en el campo <code>totalQuantity</code> devuelto por la aggregation pipeline.</li> </ul> <p>La salida del ejemplo ser\u00eda:</p> <pre><code>[\n   { _id: 'Cheese', totalQuantity: 50 },\n   { _id: 'Vegan', totalQuantity: 10 },\n   { _id: 'Pepperoni', totalQuantity: 20 }\n]\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#calcular-el-valor-total-del-pedido-y-la-cantidad-promedio-del-pedido","title":"Calcular el valor total del pedido y la cantidad promedio del pedido","text":"<p>El siguiente ejemplo calcula el valor total del pedido de pizza y la cantidad promedio del pedido entre dos fechas:</p> <pre><code>db.orders.aggregate( [\n\n   // Etapa 1: Filtramos los documentos de pedidos de pizza por rango de fecha\n   {\n      $match:\n      {\n         \"date\": { $gte: new ISODate( \"2020-01-30\" ), $lt: new ISODate( \"2022-01-30\" ) }\n      }\n   },\n\n   // Etapa 2: Agrupamos los documentos recibidos por $match por fecha y calculamos los resultados\n   {\n      $group:\n      {\n         _id: { $dateToString: { format: \"%Y-%m-%d\", date: \"$date\" } },\n         totalOrderValue: { $sum: { $multiply: [ \"$price\", \"$quantity\" ] } },\n         averageOrderQuantity: { $avg: \"$quantity\" }\n      }\n   },\n\n   // Etapa 3: Ordenamos los documentos por por totalOrderValue en orden descendente\n   {\n      $sort: { totalOrderValue: -1 }\n   }\n\n ] )\n</code></pre> <p>La etapa de <code>$match</code>:</p> <ul> <li>Filtra los documentos de pedido de pizza en un rango de fechas especificado usando <code>$gte</code> y <code>$lt</code>.</li> <li>Pasa los documentos restantes a la fase de <code>$group</code>.</li> </ul> <p>La etapa <code>$group</code>:</p> <ul> <li>Agrupa los documentos por fecha usando <code>$dateToString</code>.</li> <li>Para cada grupo, calcula:</li> <li>Valor total del pedido usando <code>$sum</code> y <code>$multiply</code>.</li> <li>Cantidad promedio de pedido usando <code>$avg</code>.</li> <li>Pasa los documentos agrupados a la etapa <code>$sort</code>.</li> </ul> <p>La etapa de <code>$sort</code>:</p> <ul> <li>Ordena los documentos por el valor total del pedido para cada grupo en orden descendente (-1).</li> <li>Devuelve los documentos ordenados.</li> </ul> <p>La salida del ejemplo ser\u00eda:</p> <pre><code>[\n   { _id: '2022-01-12', totalOrderValue: 790, averageOrderQuantity: 30 },\n   { _id: '2021-03-13', totalOrderValue: 770, averageOrderQuantity: 15 },\n   { _id: '2021-03-17', totalOrderValue: 630, averageOrderQuantity: 30 },\n   { _id: '2021-01-13', totalOrderValue: 350, averageOrderQuantity: 10 }\n]\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/mongoDB.html#6-ejemplo","title":"6. Ejemplo","text":"<p>Usando la collection de restaurantes cargada anteriormente, vamos a resolver como ejemplo, algunas operaciones:</p> <ol> <li> <p>Comprobamos la carga</p> <pre><code>show dbs\ndb.restaurantes.find()\n</code></pre> </li> <li> <p>Crear una consultar para encontrar qu\u00e9 restaurantes no tienen direcci\u00f3n (Todas tienen direcci\u00f3n)</p> <pre><code>db.restaurantes.find({address:{$exists:false}})\n</code></pre> </li> <li> <p>Contar el n\u00famero de restaurantes que si tienen direcci\u00f3n</p> <pre><code>db.restaurantes.find({address:{$exists:true}}).count()\n</code></pre> </li> <li> <p>Crear una consulta para encontrar aquellos restaurantes de cocina italiana que se encuentren en la zona geogr\u00e1fica con c\u00f3digo postal 10075</p> <p><pre><code>db.restaurantes.find({$and:[{\"cuisine\":\"Italian\"},{\"address.zipcode\":\"10075\"}]})\n</code></pre> 5. Encontrar aquellos restaurantes que tengan grado A, puntuaci\u00f3n 11 y fecha 2014-10-01T00:00:00Z</p> <pre><code>db.restaurantes.find({grades:{\"date\":ISODate(\"2014-10-01T00:00:00Z\"),\"grade\":\"A\",\"score\":11}})\n</code></pre> </li> <li> <p>Contabiliza cu\u00e1ntos restaurantes tienen una puntuaci\u00f3n menor o igual a 5</p> <pre><code>db.restaurantes.find({\"grades.score\":{$lte:5}}).count()\n</code></pre> </li> <li> <p>Obtener los nombres del segundo y el tercer restaurante de cocina italiana ordenados por nombre</p> <pre><code>db.restaurantes.find({\"cuisine\":\"Italian\"}).sort({name:1}).limit(2).skip(1)\n</code></pre> </li> <li> <p>A\u00f1adir una valoraci\u00f3n al restaurante 41156888</p> <pre><code>db.restaurantes.find({\"restaurant_id\":\"41156888\"}) // Listamos primero para ver las que tiene actualmente\ndb.restaurantes.updateOne({\"restaurant_id\":\"41156888\"},{$push:{grades: {\"date\":ISODate(\"2016-01-02T00:00:00.000Z\"),\"grade\":\"A\",\"score\":14}}})\ndb.restaurantes.find({\"restaurant_id\":\"41156888\"}) // Comprobamos que se ha a\u00f1adido correctamente\n</code></pre> </li> </ol>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html","title":"UD 2 - Procesado y Presentaci\u00f3n Datos Almacenados - Neo4j","text":"<p>En esta secci\u00f3n se profundiza en una tecnolog\u00eda concreta de bases de datos orientadas a grafos. Aunque existe un amplio abanico de tecnolog\u00edas que dan soporte a este tipo de bases de datos, como lo son OrientDB1<sup>1</sup> , FlockDB<sup>2</sup> o InfiniteGraph<sup>3</sup> , en esta secci\u00f3n se profundiza en el trabajo con bases de datos orientadas a grafos utilizando Neo4j<sup>4</sup> .</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#1-introduccion","title":"1. Introducci\u00f3n","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#11-que-es-una-graph-database","title":"1.1 \u00bfQu\u00e9 es una Graph Database?","text":"<p>Una base de datos orientada a grafos almacena nodos y relaciones en lugar de tablas o documentos. Los datos se almacenan del mismo modo que se podr\u00edan esbozar ideas en una pizarra. Sus datos se almacenan sin restringirlos a un modelo predefinido, lo que permite una forma muy flexible de pensar y utilizarlos.</p> <p>Para obtener m\u00e1s informaci\u00f3n sobre los fundamentos de los grafos, consulta los conceptos de bases de datos orientados a grafos.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#12-propiedades-del-modelo-de-grafo","title":"1.2 Propiedades del modelo de grafo","text":"<p>En Neo4j, la informaci\u00f3n se organiza como nodos, relaciones y propiedades.</p> Figura Neo4j Introducci\u00f3n 1: Bloques de construcci\u00f3n de un modelo de Base de datos de grafos. (Fuente: Neo4J) <p>Los nodos son las entidades del grafo:</p> <ul> <li>Los nodos se pueden etiquetar con labels que representen sus diferentes roles en su dominio (por ejemplo, <code>Persona</code>).</li> <li>Los nodos pueden contener cualquier n\u00famero de pares clave-valor o propiedades (por ejemplo, <code>nombre</code>).</li> <li>Las etiquetas de los nodos tambi\u00e9n pueden adjuntar metadatos (como \u00edndices o informaci\u00f3n de restricciones) a ciertos nodos.</li> </ul> <p>Las relaciones proporcionan conexiones dirigidas y con nombre entre dos entidades de nodo (por ejemplo, Persona <code>LOVES</code> a Persona).</p> <ul> <li>Las relaciones siempre tienen una direcci\u00f3n, un tipo, un nodo inicial y un nodo final, y pueden tener propiedades, al igual que los nodos.</li> <li>Los nodos pueden tener cualquier n\u00famero o tipo de relaciones sin sacrificar el rendimiento.</li> <li>Aunque las relaciones siempre son dirigidas, se pueden navegar eficientemente en cualquier direcci\u00f3n.</li> </ul> <p>Si deseas obtener m\u00e1s informaci\u00f3n sobre cualquiera de estos, puede leer m\u00e1s en Modelado de datos de grafos. \u200b</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#13-que-es-neo4j","title":"1.3 \u00bfQu\u00e9 es Neo4j?","text":"<p>Neo4j es una base de datos de grafos nativa, lo que significa que implementa un modelo de grafos real hasta el nivel de almacenamiento. Los datos se almacenan a medida que los escribe en la pizarra, en lugar de como una \"abstracci\u00f3n de grafo\" encima de otra tecnolog\u00eda. M\u00e1s all\u00e1 del grafo central, Neo4j tambi\u00e9n proporciona: transacciones ACID (ojo, igual que las RDBMS), soporte de cl\u00faster y conmutaci\u00f3n por error en tiempo de ejecuci\u00f3n.</p> Figura Neo4j Introducci\u00f3n 2: Ejemplo de esquema de grafo. (Fuente: Neo4J) <p>\u00bfQu\u00e9 hace que Neo4j sea el grafo con el que es m\u00e1s f\u00e1cil trabajar?</p> <ul> <li>Cypher\u00ae, un lenguaje de consulta declarativo similar a SQL, pero optimizado para grafos. Ahora lo utilizan otras bases de datos como SAP HANA Graph y Redis Graph a trav\u00e9s del proyecto openCypher.</li> <li>Recorridos de tiempo constante en grafos grandes tanto en profundidad como en anchura debido a la representaci\u00f3n eficiente de nodos y relaciones. Permite ampliar a miles de millones de nodos en hardware moderado.</li> <li>Esquema de grafo de propiedades flexibles que puede adaptarse con el tiempo, lo que permite materializar y agregar nuevas relaciones m\u00e1s adelante para atajar y acelerar los datos del dominio cuando el negocio necesita cambios.</li> <li>Controladores para lenguajes de programaci\u00f3n populares, incluidos Java, JavaScript, .NET, Python y muchos m\u00e1s.</li> </ul>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#14-por-que-graph-database","title":"1.4 \u00bfPor qu\u00e9 Graph Database?","text":"<p>Vivimos en un mundo conectado y comprender la mayor\u00eda de los dominios requiere procesar conjuntos ricos de conexiones para comprender lo que realmente est\u00e1 sucediendo. A menudo encontramos que las conexiones entre elementos son tan importantes como los elementos mismos.</p> <p>\u00bfDe qu\u00e9 otra manera la gente hace esto hoy? Si bien las bases de datos relacionales existentes pueden almacenar estas relaciones, las consultan con costosas operaciones <code>JOIN</code>o b\u00fasquedas cruzadas, a menudo ligadas a un esquema r\u00edgido. Resulta que las bases de datos \"relacionales\" no manejan de manera eficiente las relaciones. En una base de datos de grafos, no hay <code>JOIN</code> ni b\u00fasquedas. Las relaciones se almacenan de forma nativa junto con los elementos de datos (los nodos) en un formato mucho m\u00e1s flexible. Todo lo relacionado con el sistema est\u00e1 optimizado para recorrer los datos r\u00e1pidamente; Millones de conexiones por segundo, por n\u00facleo.</p> <p>Las bases de datos de grafos abordan grandes desaf\u00edos que muchos de nosotros enfrentamos a diario. Los problemas de datos modernos a menudo implican relaciones de muchos a muchos con datos heterog\u00e9neos que establecen necesidades para:</p> <ul> <li>Navegar por jerarqu\u00edas profundas.</li> <li>Encontrar conexiones ocultas entre elementos distantes.</li> <li>Descubrir las interrelaciones entre elementos.</li> </ul> <p>Ya sea una red social, una red de pago o una red de carreteras, encontrar\u00e1s que todo es un grafo interconectado de relaciones. Y cuando se quieren hacer preguntas sobre el mundo real, muchas preguntas se refieren a las relaciones m\u00e1s que a los elementos de datos individuales.</p> <p>Vivimos en un mundo conectado y comprender la mayor\u00eda de los dominios requiere procesar conjuntos ricos de conexiones para comprender lo que realmente est\u00e1 sucediendo. A menudo encontramos que las conexiones entre elementos son tan importantes como los elementos mismos.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#15-donde-y-como-se-utiliza-neo4j","title":"1.5. \u00bfD\u00f3nde y c\u00f3mo se utiliza Neo4j?","text":"<p>Neo4j es utilizado hoy por miles de nuevas empresas, instituciones educativas y grandes empresas en todos los sectores, incluidos servicios financieros, gobiernos, energ\u00eda, tecnolog\u00eda, comercio minorista y manufactura. Desde nuevas tecnolog\u00edas innovadoras hasta impulsar negocios, los usuarios generan informaci\u00f3n con grafos, generan nuevos ingresos y mejoran su eficiencia general.</p> <p>Puede encontrar m\u00e1s informaci\u00f3n sobre numerosos casos de uso en los que los datos conectados son m\u00e1s importantes en la web de Neo4j - Casos de uso.</p> Figura Neo4j Introducci\u00f3n 3: Casos de uso. (Fuente: Neo4J)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#2-cypher","title":"2. Cypher","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#21-que-es-cypher","title":"2.1 \u00bfQu\u00e9 es Cypher?","text":"<p>Cypher es el lenguaje de consulta de grafos declarativos de Neo4j. Fue creado en 2011 por ingenieros de Neo4j como un lenguaje equivalente a SQL para bases de datos de grafos. Al igual que SQL, Cypher permite a los usuarios centrarse en qu\u00e9 recuperar del grafo, en lugar de c\u00f3mo recuperarlo. Como tal, Cypher permite a los usuarios aprovechar todo el potencial de sus bases de datos de grafos al permitir consultas eficientes y expresivas que revelan conexiones y grupos de datos previamente desconocidos.</p> <p>Cypher proporciona una forma visual de hacer coincidir patrones y relaciones. Se basa en el siguiente tipo de sintaxis ascii-art: <code>(nodos)-[:CONNECT_TO]\u2192(otherNodes)</code>. Los par\u00e9ntesis se utilizan para nodos circulares y <code>-[:ARROWS]\u2192</code> para relaciones. Escribir una consulta es efectivamente como dibujar un patr\u00f3n a trav\u00e9s de los datos del grafo. En otras palabras, entidades como los nodos y sus relaciones se integran visualmente en las consultas. Esto convierte a Cypher en un lenguaje muy intuitivo tanto para leer como para escribir.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#22-diferencias-en-cypher-y-sql","title":"2.2 Diferencias en Cypher y SQL","text":"<p>Cypher y SQL son similares en muchos aspectos. Por ejemplo, comparten muchas de las mismas palabras clave, como <code>WHERE</code> y <code>ORDER BY</code>. Sin embargo, existen algunas diferencias importantes entre los dos:</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#cypher-es-flexible-en-cuanto-a-esquemas","title":"Cypher es flexible en cuanto a esquemas","text":"<p>Si bien es posible y recomendable aplicar esquemas parciales utilizando \u00edndices y restricciones, Cypher y Neo4j ofrecen un mayor grado de flexibilidad de esquemas que SQL y una base de datos relacional. M\u00e1s espec\u00edficamente, los nodos y las relaciones en una base de datos Neo4j no necesitan tener una propiedad espec\u00edfica establecida porque otros nodos o relaciones en el mismo grafo tienen esa propiedad (a menos que haya una restricci\u00f3n de existencia creada en esa propiedad espec\u00edfica). Esto significa que no se requiere que los usuarios utilicen un esquema fijo para representar datos y que pueden agregar nuevos atributos y relaciones a medida que evolucionan sus grafos.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#orden-de-consulta","title":"Orden de consulta","text":"<p>Las consultas SQL comienzan con lo que un usuario quiere devolver, mientras que las consultas Cypher terminan con la cl\u00e1usula de devoluci\u00f3n. Por ejemplo, considere las dos consultas siguientes (ambas buscan en una base de datos t\u00edtulos de pel\u00edculas con una calificaci\u00f3n superior a 7):</p> SQLCypher <pre><code>SELECT movie.name\nFROM movie\nWHERE movie.rating &gt; 7\n</code></pre> <pre><code>MATCH (movie:Movie)\nWHERE movie.rating &gt; 7\nRETURN movie.title\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#las-consultas-cifradas-son-mas-concisas","title":"Las consultas cifradas son m\u00e1s concisas","text":"<p>Debido a su m\u00e9todo intuitivo, similar a una pizarra, para construir cl\u00e1usulas, las consultas Cypher suelen ser m\u00e1s concisas que sus consultas SQL equivalentes. Por ejemplo, considere las siguientes dos consultas (ambas buscando en una base de datos los nombres de los actores de la pel\u00edcula The Matrix):</p> SQLCypher <pre><code>SELECT actors.name\nFROM actors\n    LEFT JOIN acted_in ON acted_in.actor_id = actors.id\n    LEFT JOIN movies ON movies.id = acted_in.movie_id\nWHERE movies.title = \"The Matrix\"\n</code></pre> <pre><code>MATCH (actor:Actor)-[:ACTED_IN]-&gt;(movie:Movie {title: 'The Matrix'})\nRETURN actor.name\n</code></pre> <p>Toda la informaci\u00f3n de Cypher con Neo4j puedes consultarla en la documentaci\u00f3n oficial. Tambi\u00e9n puedes consultar la Cypher Cheat Sheet</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#3-instalacion-y-uso","title":"3. Instalaci\u00f3n y uso","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#31-docker","title":"3.1 Docker","text":"<p>Vamos a explicar como levantar un contenedor Neo4j con Docker</p> <ol> <li>Instalar Neo4j en Docker.</li> <li>Vamos a la imagen oficial de Ne4j en docker</li> <li>Observa con detenimiento la informaci\u00f3n que nos facilita la imagen oficial para el despliegue y configuraci\u00f3n de los contenedores</li> <li> <p>Seguimos los pasos, que tienen este formato</p> <pre><code>docker run \\\n--publish=7474:7474 --publish=7687:7687 \\\n--volume=$HOME/neo4j/data:/data \\\nneo4j\n</code></pre> </li> </ol> <p>Danger</p> <p>Dadas las caracter\u00edsticas de la imagen facilitada por Neo4j, hay que indicar la versi\u00f3n a usar en el contenedor en lugar de la \u00faltima <code>latest</code></p> <ol> <li>Necesitaremos a\u00f1adir el m\u00f3dulo Graph Data Science para poder realizar operaciones de grafos sobre nuestra Base de Datos y APOC Cypher. Por tanto consultamos la documentaci\u00f3n que nos facilitan para ello</li> </ol> <p>Warning</p> <p>Para los Sistemas Operativos Windows, se necesita usar el car\u00e1cter de escape <code>\\</code> que permita interpretar al sistema operativo la doble comilla dentro de una comilla simple.</p> LinuxWindows <pre><code>docker run -it \\\n--name neo4j-bda1 \\\n--publish=7474:7474 --publish=7687:7687 \\\n--env NEO4J_AUTH=none \\\n--env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\nneo4j:5.13.0\n</code></pre> <pre><code>docker run -it --name neo4j-bda1 --publish=7474:7474 --publish=7687:7687 --env NEO4J_AUTH=none --env NEO4J_PLUGINS='[\\\"apoc\\\", \\\"graph-data-science\\\"]' neo4j:5.13.0\n</code></pre> <ol> <li>Entramos en neo4j =&gt; http://localhost:7474/</li> <li>Conectamos sin usuario ni contrase\u00f1a</li> </ol> Figura Docker Neo4j 1: Conexi\u00f3n al Neo4j. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#32-neo4j-aura","title":"3.2 Neo4j Aura","text":"<p>Neo4j Aura es una plataforma de grafos r\u00e1pida, escalable, siempre activa y totalmente automatizada que se ofrece como un servicio en la nube.</p> <p>Aura incluye AuraDB, la base de datos orientada a grafos como servicio para desarrolladores que crean aplicaciones inteligentes, y AuraDS, la ciencia de datos orientada a grafos como servicio para cient\u00edficos de datos que crean modelos predictivos y flujos de trabajo anal\u00edticos.</p> <p>Para m\u00e1s informaci\u00f3n consulta la documentaci\u00f3n oficial</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#33-auradb","title":"3.3 AuraDB","text":"<p>Neo4j AuraDB es un servicio de base de datos orientados a grafos en la nube (Database as a service) totalmente administrado.</p> <p>Neo4j AuraDB es la base de datos orientada a grafos totalmente administrada como servicio (Database as a service) que ayuda a crear aplicaciones inteligentes basadas en el contexto m\u00e1s r\u00e1pido con consultas ultrarr\u00e1pidas, informaci\u00f3n en tiempo real, herramientas de desarrollo integradas, visualizaci\u00f3n de datos e integraciones respaldadas por la comunidad de desarrolladores de grafos m\u00e1s grande.</p> <p>AuraDB es confiable, seguro y totalmente automatizado, lo que le permite concentrarse en crear aplicaciones de grafos sin preocuparse por la administraci\u00f3n de la base de datos. Tiene versiones gratuitas (AuraDB Free) y de pago (AuraDB Professional and AuraDB Enterprise). Pero no podemos usar Graph Data Science (que nos interesa como analistas y cient\u00edficos de datos)</p> <p>Para m\u00e1s informaci\u00f3n consulta la documentaci\u00f3n oficial</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#34-aurads","title":"3.4 AuraDS","text":"<p>Neo4j AuraDS es la soluci\u00f3n de ciencia de datos como servicio totalmente administrada para cient\u00edficos de datos que unifica la superficie de aprendizaje autom\u00e1tico (ML) y la base de datos orientada a grafos en un \u00fanico espacio de trabajo, lo que facilita descubrir las conexiones en big data y responder preguntas cr\u00edticas para el negocio.</p> <p>No tiene versi\u00f3n gratuita. Para m\u00e1s informaci\u00f3n consulta la documentaci\u00f3n oficial</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#35-sandbox-neo4j","title":"3.5 Sandbox Neo4j","text":"<p>Para poder hacer uso de Neo4j de forma gratuita, Neo4j proporciona Neo4j Sandbox, una instancia gratuita de Neo4j basada en la nube, donde podemos aprender sobre Neo4j, probar sus ideas o jugar con los ejemplos de datos predise\u00f1ados.</p> <ol> <li>Accedemos al servicio de Neo4j sandbox</li> <li>Nos registramos/logueamos en Neo4j</li> <li>Nos muestra una serie de proyectos para crear una instancia sobre ellos.</li> </ol> Figura Neo4j Sandbox 1: Proyectos. (Fuente: Propia)  <ol> <li>Elige uno.</li> <li>Esperamos mientras crea la instancia</li> </ol> Figura Neo4j Sandbox 2: Creando una instancia. (Fuente: Propia)  <ol> <li>Desplegamos las opciones</li> </ol> Figura Neo4j Sandbox 3: Opciones de una instancia. (Fuente: Propia)  <ol> <li>Vamos a connection details y copiamos la contrase\u00f1a</li> </ol> Figura Neo4j Sandbox 4: Details project. (Fuente: Propia)  <ol> <li>Abrimos la instancia en el navegador (tenemos varios opciones a elegir)</li> </ol> Figura Neo4j Sandbox 5: Abriendo una instancia. (Fuente: Propia)  <ol> <li>Se abre una nueva ventana con la instancia levantada para acceder a ella</li> <li>Tenemos varias opciones de acceso (Elige la que quieras)<ol> <li>Single sing on: Nos abrir\u00e1 una nueva pesta\u00f1a y nos pedir\u00e1 nuestras credenciales de Neo4j</li> <li>Username / Password: Neo4j (por defecto) / la copiada en el paso anterior de connection details</li> </ol> </li> </ol> Figura Neo4j Sandbox 6: Accediendo a una instancia. (Fuente: Propia)  <ol> <li>Se conecta a la base de datos. A la izquierda nos aparece una gu\u00eda de uso, que depender\u00e1 de la instancia elegida.</li> <li>Ejecutamos la siguiente instrucci\u00f3n para ver es esquema del grafo <code>CALL db.schema.visualization()</code> y ejecutamos.</li> </ol> Figura Neo4j Sandbox 7: Esquema de la base de datos. (Fuente: Propia)  <ol> <li>Ejecutamos esta otra instrucci\u00f3n para ver toda la base de datos <code>match(n) return n</code></li> </ol> Figura Neo4j Sandbox 8: Datos de la base de datos. (Fuente: Propia)  <ol> <li>Una consulta de ejemplo, donde buscamos todas las pel\u00edculas que se estrenaron despu\u00e9s del a\u00f1o 2000 limitando el resultado a 5 elementos </li> </ol> <pre><code>MATCH (m:Movie)\nWHERE m.released &gt; 2000\nRETURN m LIMIT 5\n</code></pre> <ol> <li>Podemos observar el resultado del grafo, la tabla resultante, en texto o el c\u00f3digo</li> </ol> Figura Neo4j Sandbox 9: Consulta a la base de datos. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#4-get-started-neo4j","title":"4. Get started Neo4j","text":"<p>Para comenzar a trabajar con Neo4j, es necesario crear un grafo o bien partir de un grafo ya existente que se importa dentro del \u00e1rea de trabajo de Neo4j. A continuaci\u00f3n, se muestra c\u00f3mo implementar ambos procedimientos.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#41-importacion-de-datos","title":"4.1 Importaci\u00f3n de datos","text":"<p>Imaginemos un caso en el que se pretende importar un grafo a partir de un fichero .csv. En el ejemplo que se muestra a continuaci\u00f3n, se dispone de dos ficheros .csv que contienen, respectivamente, la definici\u00f3n de los nodos y de las aristas del grafo que se pretende importar.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#definicion-y-importacion-de-los-nodos","title":"Definici\u00f3n y importaci\u00f3n de los nodos","text":"<p>As\u00ed pues, el archivo csv siguiente muestra los nodos del grafo. Para cada nodo se almacena su identificador, latitud, longitud y poblaci\u00f3n</p> <p><pre><code>id,latitude,longitude,population\n\"Amsterdam\",52.379189,4.899431,821752\n\"Utrecht\",52.092876,5.104480,334176\n\"Den Haag\",52.078663,4.288788,514861\n\"Immingham\",53.61239,-0.22219,9642\n\"Doncaster\",53.52285,-1.13116,302400\n\"Hoek van Holland\",51.9775,4.13333,9382\n\"Felixstowe\",51.96375,1.3511,23689\n\"Ipswich\",52.05917,1.15545,133384\n\"Colchester\",51.88921,0.90421,104390\n\"London\",51.509865,-0.118092,8787892\n\"Rotterdam\",51.9225,4.47917,623652\n\"Gouda\",52.01667,4.70833,70939\n</code></pre> Definici\u00f3n de nodos de un grafo</p> <p>Dados este fichero, es posible importar los nodos del grafo:</p> <p><pre><code> // Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (place:Place {ciudad:row.id})\nSET place.latitude = toFloat(row.latitude),\nplace.longitude = toFloat(row.latitude),\nplace.population = toInteger(row.population)\n</code></pre> Importaci\u00f3n de nodos</p> <p>As\u00ed, en el listado anterior se define como identificador de recurso la direcci\u00f3n donde se encuentra el archivo que se pretende cargar, el cual es cargado a trav\u00e9s de la instrucci\u00f3n LOAD CSV. La instrucci\u00f3n MERGE permite definir cada fila como nodo de un grafo de tipo lugar (place) y a\u00f1adir a cada nodo la propiedad id cuyo valor ser\u00e1 el identificador de la fila en la que se encuentra cada nodo dentro del fichero csv. Finalmente, los valores de latitud y longitud se convierten en valores de tipo real y la poblaci\u00f3n en un valor de tipo entero.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#definicion-y-importacion-de-las-relaciones-de-los-nodos","title":"Definici\u00f3n y importaci\u00f3n de las relaciones de los nodos","text":"<p>Por su parte, el siguiente archivo csv especifica cada uno de los enlaces entre los nodos del grafo. Para cada uno de los enlaces se identifica el origen y destino del enlace, el tipo de relaci\u00f3n que se define entre ellos y el coste. </p> <p><pre><code>src,dst,relationship,cost\n\"Amsterdam\",\"Utrecht\",\"EROAD\",46\n\"Amsterdam\",\"Den Haag\",\"EROAD\",59\n\"Den Haag\",\"Rotterdam\",\"EROAD\",26\n\"Amsterdam\",\"Immingham\",\"EROAD\",369\n\"Immingham\",\"Doncaster\",\"EROAD\",74\n\"Doncaster\",\"London\",\"EROAD\",277\n\"Hoek van Holland\",\"Den Haag\",\"EROAD\",27\n\"Felixstowe\",\"Hoek van Holland\",\"EROAD\",207\n\"Ipswich\",\"Felixstowe\",\"EROAD\",22\n\"Colchester\",\"Ipswich\",\"EROAD\",32\n\"London\",\"Colchester\",\"EROAD\",106\n\"Gouda\",\"Rotterdam\",\"EROAD\",25\n\"Gouda\",\"Utrecht\",\"EROAD\",35\n\"Den Haag\",\"Gouda\",\"EROAD\",32\n\"Hoek van Holland\",\"Rotterdam\",\"EROAD\",33\n</code></pre> Definici\u00f3n de aristas de un grafo</p> <p>Dado este fichero, es posible importar las relaciones de los nodos del grafo. An\u00e1logamente a la importaci\u00f3n de nodos, a continuaci\u00f3n mostramos el c\u00f3digo necesario para la importaci\u00f3n del fichero que contiene los enlaces entre los nodos del grafo.</p> <p><pre><code>//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/transport-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (origin:Place {ciudad: row.src})\nMATCH (destination:Place {ciudad: row.dst})\nMERGE (origin)-[:EROAD {distance: toInteger(row.cost)}]-&gt;(destination)\n</code></pre> Importaci\u00f3n de aristas</p> <p>La diferencia principal de este listado con respecto al anterior, es que en este se necesita identificar mediante la instrucci\u00f3n MATCH los nodos de origen y destino de cada uno de los enlaces, lo cual es posible mediante los atributos \u201csrc\u201d y \u201cdst\u201d incluidos en el archivo .csv. Finalmente, se utiliza la sentencia MERGE para crear una relaci\u00f3n entre \u201corigin\u201d y \u201cdestination\u201d de tipo EROAD que contiene la propiedad \u201cdistance\u201d cuyo valor es el coste que aparec\u00eda en el fichero .csv.</p> <p>Podemos comprobarlo listando todo el grafo</p> <pre><code>match(n) return n\n</code></pre> Figura Neo4j Importando Datos 1: Importando datos <p>El grafo resultado es el siguiente</p> Figura Neo4j Importando Datos 2: Grafo de ejemplo resultante. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#42-creacion-de-grafos","title":"4.2 Creaci\u00f3n de grafos","text":"<p>Otra alternativa es la creaci\u00f3n de un grafo desde cero, especificando para ello sus nodos y enlaces. La siguiente imagen muestra un ejemplo de un grafo de coocurrencia de hashtags en el que cada uno de los nodos es un hashtag y los enlaces entre ellos se producen cuando dos hashtags aparecieron en un mismo tweet. Sobre cada enlace, se puede apreciar un valor que indica en cu\u00e1ntos tweets concurrieron los hashtags conectados por medio de dicha arista.</p> Figura Neo4j Creaci\u00f3n de un grafo 1: Grafo de ejemplo. (Fuente: Propia)  <p>Para representar este grafo en Neo4j, se puede escribir el siguiente fragmento de c\u00f3digo:</p> <p><pre><code>CREATE\n(JS:Hashtag {name: 'JoaquinSabina'}),\n(RS:Hashtag {name: 'Rusia2018'}),\n(AG:Hashtag {name: 'Argentina'}),\n(FD:Hashtag {name: 'Feliz Domingo'}),\n(MS:Hashtag {name: 'Messi'}),\n\n(JS)-[:COOC {ntweet: 52}]-&gt;(FD),\n(FD)-[:COOC {ntweet: 52}]-&gt;(JS),\n(RS)-[:COOC {ntweet: 183}]-&gt;(FD),\n(FD)-[:COOC {ntweet: 183}]-&gt;(RS),\n(RS)-[:COOC {ntweet: 73}]-&gt;(AG),\n(AG)-[:COOC {ntweet: 73}]-&gt;(RS),\n(AG)-[:COOC {ntweet: 112}]-&gt;(MS),\n(MS)-[:COOC {ntweet: 112}]-&gt;(AG),\n(FD)-[:COOC {ntweet: 81}]-&gt;(MS),\n(MS)-[:COOC {ntweet: 81}]-&gt;(FD)\n</code></pre> Creaci\u00f3n grafo de co-ocurrencia de hashtags</p> <p>Tal y como se puede ver, en primer lugar se definen los nodos y sus propiedades y a continuaci\u00f3n los enlaces y sus propiedades. Todo grafo en Neo4j tiene aristas dirigidas, si bien es cierto que cuando se ejecutan las consultas puede no tenerse en cuenta la direcci\u00f3n de las aristas. Por este motivo, se han especificado las aristas en ambas direcciones. En caso de querer crear una arista que sea interpretada como no dirigida, se debe crear sin el operador flecha que determina el sentido de la arista. Esto es posible hacerlo solo fuera de la instrucci\u00f3n CREATE. El listado 4.6 muestra c\u00f3mo crear la primera relaci\u00f3n del grafo solo que de forma no dirigida.</p> <p><pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'})\nMATCH(FD:Hashtag{name:'Feliz Domingo'})\nMERGE (JS)-[:COOC{ntweet:52}]-(FD)\n</code></pre> Creaci\u00f3n de una arista no dirigida</p> <p>Para mostrar el grafo, es posible utilizar el siguiente comando. La figura 4.3 muestra c\u00f3mo quedar\u00eda el grafo creado en el listado 4.5.</p> <pre><code>MATCH(n) return (n)\n</code></pre> Figura Neo4j Creaci\u00f3n de un grafo 2: Visualizaci\u00f3n del grafo de co-ocurrencia de hashtags. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#43-eliminar-nodos-y-grafos","title":"4.3 Eliminar nodos y grafos","text":"<p>Siguiendo con el ejemplo anterior, para eliminar un nodo creado, es posible utilizar el comando:</p> <pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'}) delete (JS)\n</code></pre> <p>Sin embargo, un nodo solo puede ser eliminado si se eliminan las relaciones que contiene. Para ello, es posible utilizar el comando:</p> <pre><code>MATCH(JS:Hashtag{name:'JoaquinSabina'}) detach delete (JS)\n</code></pre> <p>Mientras que, para eliminar todos los nodos y aristas, es posible escribir:</p> <pre><code>MATCH(n) detach delete(n)\n</code></pre>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#5-neo4j-graph-data-science","title":"5. Neo4j Graph Data Science","text":"<p>La biblioteca Neo4j Graph Data Science (GDS) proporciona versiones paralelas implementadas de manera eficiente de algoritmos de grafos comunes, expuestos como procedimientos Cypher. Adem\u00e1s, GDS incluye canales de aprendizaje autom\u00e1tico para entrenar modelos predictivos supervisados para resolver problemas de grafos, como la predicci\u00f3n de relaciones faltantes.</p> <p>Los algoritmos de grafos se utilizan para calcular m\u00e9tricas de grafos, nodos o relaciones. Pueden proporcionar informaci\u00f3n sobre entidades relevantes en el grafo (centralidades, clasificaci\u00f3n) o estructuras inherentes como comunidades (detecci\u00f3n de comunidades, partici\u00f3n de grafos, agrupaci\u00f3n), etc.</p> <p>Para verificar la instalaci\u00f3n ejecuta:</p> <pre><code>RETURN gds.version();\n</code></pre> <p>Para listar todos los procedimientos disponibles ejecuta:</p> <pre><code>CALL gds.list();\n</code></pre> <p>La biblioteca Neo4j Graph Data Science contiene una gran cantidad de algoritmos, los cuales detallaremos a continuaci\u00f3n.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#6-recorridos-sobre-grafos","title":"6. Recorridos sobre grafos","text":"<p>Los recorridos sobre grafos son una utilidad imprescindible en el trabajo con esta estructura de datos. Del mismo modo, los recorridos tambi\u00e9n son una herramienta fundamental cuando se trabaja con bases de datos orientadas a grafos.</p> <p>Un recorrido sobre grafos es un procedimiento sistem\u00e1tico que permite explorar un grafo examinando todos sus v\u00e9rtices y aristas, comenzando desde un v\u00e9rtice inicial. A la hora de recorrer un grafo, existen dos tipos de recorridos: el recorrido en anchura (denotado BFS, por sus siglas en ingl\u00e9s) y el recorrido en profundidad (denotado DFS, tambi\u00e9n por sus siglas en ingl\u00e9s).</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#61-recorrido-en-anchura-bfs","title":"6.1 Recorrido en anchura (BFS)","text":"<p>El recorrido en anchura, tambi\u00e9n conocido por sus siglas en ingl\u00e9s Breadth First Search (BFS) es un procedimiento que permite recorrer un grafo, explorando todos sus nodos y aristas.</p> <p>El recorrido en anchura parte de un nodo inicial. A continuaci\u00f3n, el m\u00e9todo comienza a explorar todos los nodos hijo del nodo inicial seleccionado que se encuentran a un salto. Una vez visitados, comienzan a explorarse los nodos hijo del siguiente nivel y as\u00ed sucesivamente hasta haber recorrido el grafo completo. Este recorrido es muy utilizado cuando se busca un camino con el n\u00famero m\u00ednimo de aristas entre dos v\u00e9rtices dados o cuando se busca un ciclo simple, es decir, una secuencia de nodos y aristas que se pueden describir circularmente en el que todos los nodos y aristas son distintos.</p> <p>Por ejemplo, usando el grafo anterior del punto importaci\u00f3n de datos. Es posible recorrerlo utilizando BFS escribiendo el siguiente c\u00f3digo Neo4j que aparece en el listado 4.7. </p> Figura Neo4j Importando Datos 2: Grafo de ejemplo resultante. (Fuente: Propia)  <p>Info</p> <p>Solamente hay que crear un <code>project graph</code> del algoritmo si es la misma configuraci\u00f3n. Es decir, es reutilizable</p> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_BFR',\n    'Place',\n    'EROAD'\n)\n</code></pre> <p><pre><code>//Realizamos la b\u00fasqueda en anchura\nMATCH (a:Place{id:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.bfs.stream('myGraph_BFR', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.id ] AS tags\nRETURN tags\n</code></pre> Recorrido en anchura (BFS)</p> <p>En el c\u00f3digo anterior, se define un grafo llamado myGraph_BFR el cual est\u00e1 formado por nodos de tipo Place y cuyas relaciones son del tipo EROAD y tienen una propiedad, denominada distance. A continuaci\u00f3n, se define el nodo inicial con las sentencias MATCH y WITH. Despu\u00e9s, se llama al m\u00e9todo que realiza el recorrido BFS sobre el grafo anterior partiendo desde el nodo inicial para producir un camino (path). Finalmente, se devuelven todos los nodos que forman parte del camino, mostrando los identificadores de cada uno de ellos. El resultado del recorrido es: Doncaster, London, Colchester, Ipswich, Felixstowe, Hoek van Holand, Den Haag, Rotterdam, Gouda, Utrecht.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#62-recorrido-en-profundidad-dfs","title":"6.2 Recorrido en profundidad (DFS)","text":"<p>El recorrido en profundidad, tambi\u00e9n conocido por sus siglas en ingl\u00e9s Depth First Search (DFS) es un procedimiento alternativo al recorrido en anchura para explorar un grafo, recorriendo todos sus nodos y aristas.</p> <p>El procedimiento para recorrer un grafo mediante el m\u00e9todo DFS es el siguiente: en primer lugar, se parte de un nodo inicial. A partir de \u00e9l, se empiezan a recorrer todos sus hijos hasta el \u00faltimo nivel. Una vez llegados al \u00faltimo nivel del grafo, se vuelve hacia atr\u00e1s recorriendo todos los hijos de los nodos de niveles anteriores. </p> <p>En Neo4j, el procedimiento para recorrer un grafo utilizando el recorrido DFS se muestra a continuaci\u00f3n. El procedimiento, como se puede comprobar, es pr\u00e1cticamente id\u00e9ntico al c\u00f3digo BFS, solo que cambiando el m\u00e9todo de recorrido. El resultado obtenido es: Doncaster, London, Colchester, Ipswich, Felixstowe, Hoek van Holland, Rotterdam, Den Haag, Gouda, Utrecht.</p> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_DFS',\n    'Place',\n    'EROAD'\n)\n</code></pre> <p><pre><code>//Realizamos la b\u00fasqueda en Profundidad\nMATCH (a:Place{ciudad:'Doncaster'})\nWITH id(a) AS startNode\nCALL gds.dfs.stream('myGraph_DFS', {sourceNode: startNode})\nYIELD path\nUNWIND [ n in nodes(path) | n.ciudad ] AS tags\nRETURN tags\n</code></pre> Recorrido en profundidad (DFS)</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#7-caminos-minimos","title":"7. Caminos m\u00ednimos","text":"<p>El c\u00e1lculo y la obtenci\u00f3n de caminos m\u00ednimos dentro de un grafo es uno de los problemas cl\u00e1sicos en teor\u00eda de grafos con multitud de aplicaciones en el mundo real. Un camino dentro de un grafo es un conjunto de nodos y aristas que conectan un par de nodos del grafo. En el caso de los grafos no pesados, es decir, aquellos cuyas aristas no tienen coste, el c\u00e1lculo del camino m\u00ednimo se obtiene minimizando el n\u00famero de saltos entre el nodo origen y el nodo destino. En el caso de los grafos pesados, aquellos cuyas aristas tienen coste, el c\u00e1lculo del camino m\u00ednimo se obtiene a trav\u00e9s del camino en el que la suma de los costes de sus aristas es m\u00ednima.</p> <p>Junto con los recorridos de grafos, los m\u00e9todos de obtenci\u00f3n de caminos m\u00ednimos son muy \u00fatiles cuando se trabaja en entornos din\u00e1micos, donde aparecen y desaparecen continuamente aristas del grafo o sus costes cambian din\u00e1micamente. El c\u00e1lculo de caminos m\u00ednimos tambi\u00e9n es muy \u00fatil en aplicaciones que deben dar respuestas en tiempo real. Algunos ejemplos claros pueden ser encontrar rutas entre dos localidades, como ocurre en Google Maps u obtener los grados de separaci\u00f3n entre una empresa y un empleado potencial al que se quiere contratar, como puede ocurrir en LinkedIn.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#71-algoritmo-de-dijkstra","title":"7.1 Algoritmo de Dijkstra","text":"<p>El algoritmo de Dijkstra es uno de los algoritmos m\u00e1s utilizados en teor\u00eda de grafos para la obtenci\u00f3n de caminos m\u00ednimos. Este algoritmo tiene 2 versiones: uno que indica un nodo origen y el camino m\u00ednimo a todos los dem\u00e1s, y otro donde muestra el camino m\u00ednimo indicando un nodo origen y destino.</p> <ul> <li>Algoritmo de Dijkstra: Single-Source Shortest Path (de un nodo a todos los dem\u00e1s)</li> </ul> <p>Dado el grafo de ciudades con el que se est\u00e1 trabajando a lo largo de este cap\u00edtulo, el siguiente c\u00f3digo permite obtener los caminos m\u00ednimos entre un nodo del grafo y todos los dem\u00e1s nodos alcanzables(Single-Source Shortest Path).</p> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_Dijkstra',\n    'Place',\n    'EROAD',\n    {\n        relationshipProperties: 'distance'\n    }\n)\n</code></pre> <p><pre><code>MATCH (source:Place {ciudad: 'Amsterdam'})\nCALL gds.allShortestPaths.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).ciudad AS sourceNodeName,\n    gds.util.asNode(targetNode).ciudad AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).ciudad] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Ejecuci\u00f3n del algoritmo de Dijkstra sobre el nodo Amsterdam</p> <p>En el listado anterior, se utiliza la sentencia MATCH para definir el nodo de origen y, posteriormente, se guarda el grafo incluyendo el nodo de origen y la propiedad de distancia que es la utilizada para obtener el camino m\u00ednimo.</p> <ul> <li>Algoritmo de Dijkstra: Single Source-Target Shortest Path (de un nodo origen a otro destino)</li> </ul> <p>A continuaci\u00f3n, se producir\u00e1 un conjunto de caminos en el que se mostrar\u00e1 informaci\u00f3n completa del nodo de origen, destino, coste total y camino recorrido as\u00ed como los distintos costes parciales. En caso de querer obtener el camino m\u00ednimo entre un par de nodos concretos(Single Source-Target Shortest Path), es posible utilizar el siguiente c\u00f3digo an\u00e1logo al anterior.</p> <p><pre><code>MATCH (source:Place {ciudad: 'Amsterdam'}), (target:Place {ciudad: 'London'})\nCALL gds.shortestPath.dijkstra.stream('myGraph_Dijkstra', {\n    sourceNode: source,\n    targetNode: target,\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).ciudad AS sourceNodeName,\n    gds.util.asNode(targetNode).ciudad AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).ciudad] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Ejecuci\u00f3n del algoritmo de Dijkstra sobre el par de nodos Amsterdam-London</p> <p>El camino m\u00ednimo entre Amsterdam y Londres es el formado por: Amsterdam Immingham, Doncaster, London. Este camino tiene un coste total de 720.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#72-algoritmo-a","title":"7.2 Algoritmo A*","text":"<p>El algoritmo A* es una t\u00e9cnica de b\u00fasqueda informada que permite, al igual que el algoritmo de Dijkstra, calcular el camino m\u00ednimo entre dos nodos dados cualesquiera. Al contrario que el algoritmo de Dijkstra, cuando se va a seleccionar el siguiente nodo del camino, la t\u00e9cnica A no solo se tiene en cuanta la distancia ya calculada, sino que este algoritmo combina la distancia calculada con el resultado de una funci\u00f3n heur\u00edstica. Esta funci\u00f3n toma un nodo como entrada y devuelve un valor que corresponde con el coste de llegar al nodo objetivo desde ese nodo. Esta t\u00e9cnica, que se conoce tambi\u00e9n como de b\u00fasqueda informada, contin\u00faa el recorrido del grafo en cada iteraci\u00f3n desde el nodo con el menor coste combinado entre la distancia ya calculada y la funci\u00f3n heur\u00edstica. El siguiente c\u00f3digo muestra c\u00f3mo obtener el camino m\u00ednimo mediante el m\u00e9todo A entre Amsterdam y Londres.</p> <pre><code>// Creamos el grafo. \nCALL gds.graph.project(\n    'myGraph_A',\n    'Place',\n    'EROAD',\n    {\n        nodeProperties: ['latitude', 'longitude', 'population'],\n        relationshipProperties: 'distance'\n    }\n)\n</code></pre> <p><pre><code>MATCH (source:Place {ciudad: 'Amsterdam'}), (target:Place {ciudad: 'London'})\nCALL gds.shortestPath.astar.stream('myGraph_A', {\n    sourceNode: source,\n    targetNode: target,\n    latitudeProperty: 'latitude',\n    longitudeProperty: 'longitude',\n    relationshipWeightProperty: 'distance'\n})\nYIELD index, sourceNode, targetNode, totalCost, nodeIds, costs, path\nRETURN\n    index,\n    gds.util.asNode(sourceNode).ciudad AS sourceNodeName,\n    gds.util.asNode(targetNode).ciudad AS targetNodeName,\n    totalCost,\n    [nodeId IN nodeIds | gds.util.asNode(nodeId).ciudad] AS nodeNames,\n    costs,\n    nodes(path) as path\nORDER BY index\n</code></pre> Ejecuci\u00f3n del algoritmo A* sobre el par de nodos Amsterdam-London</p> <p>Como se puede ver, este fragmento de c\u00f3digo es similar a los vistos anteriormente, solo que a la hora de guardar el grafo, se almacenan los propiedades de latitud y longitud, necesarias en Neo4j para ejecutar el algoritmo A*.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#8-medidas-de-centralidad","title":"8. Medidas de centralidad","text":"<p>La centralidad se define como la relevancia de un nodo dentro de un grafo. Por tanto, el estudio de medidas de centralidad permite identificar nodos relevantes dentro de un grafo. Esta identificaci\u00f3n permitir\u00e1 entender el comportamiento de la red, qu\u00e9 nodos permiten viralizar con mayor rapidez el contenido de la red, desde qu\u00e9 nodos la informaci\u00f3n est\u00e1 m\u00e1s accesible, etc. El estudio de medidas de centralidad tiene multitud de aplicaciones, aunque son especialmente notables las relacionadas con los \u00e1mbitos de la publicidad y el marketing, donde el estudio de estas m\u00e9tricas permite identificar actores relevantes dentro de una red a los que se puede contratar para anunciar un producto o identificar sobre qu\u00e9 actores enviar informaci\u00f3n para alcanzar a m\u00e1s usuarios.</p> <p>Respecto a las medidas de centralidad, no existe una \u00fanica medida sino que, en funci\u00f3n de los datos y del prop\u00f3sito que se pretende alcanzar, se utilizan unas m\u00e9tricas u otras. a continuaci\u00f3n, se van a definir tres medidas de centralidad con las que se realizar\u00e1n ejemplos en Neo4j: centralidad de grado, cercan\u00eda e intermediaci\u00f3n. </p> <p>Para trabajar con medidas de centralidad, se va a importar un grafo social que servir\u00e1 de ejemplo para el c\u00e1lculo de estas m\u00e9tricas. El siguiente c\u00f3digo muestra lo necesario para su creaci\u00f3n. Por otra parte, mostramos la imagen del grafo importado.</p> <p><pre><code>// Importamos los nodos\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-nodes.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMERGE (:User {id: row.id})\n\n//Importamos las relaciones\nWITH \"https://github.com/neo4j-graph-analytics/book/raw/master/data/\" AS base\nWITH base + \"social-relationships.csv\" AS uri\nLOAD CSV WITH HEADERS FROM uri AS row\nMATCH (source:User {id: row.src})\nMATCH (destination:User {id: row.dst})\nMERGE (source)-[:FOLLOWS]-&gt;(destination)\n</code></pre> Listado 4.12: Importaci\u00f3n de un grafo social</p> Figura Neo4j. Medida de centralidad 1: Visualizaci\u00f3n de un grafo social. (Fuente: Propia)"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#81-centralidad-de-grado-degree-centrality","title":"8.1 Centralidad de grado (Degree Centrality)","text":"<p>El grado de un nodo en un grafo se define como el n\u00famero de aristas o conexiones que entran o salen de un nodo. As\u00ed pues, en un nodo de un grafo se distinguen dos grados: el grado de entrada, correspondiente al conjunto de conexiones que entran a un nodo y el grado de salida, que se corresponde con el conjunto de aristas que salen de un nodo. El primero indica la prominencia de un nodo dentro de la red, mientras que el segundo la influencia del nodo dentro del grafo. Esta medida de centralidad es muy utilizada para identificar actores prominentes o influyentes o para identificar conductas fraudulentas, caracterizadas en ocasiones por una actividad anormal. Para calcular la centralidad de grado en este grafo, se almacenar\u00e1 en primer lugar el grafo creado y, a continuaci\u00f3n, se ejecutar\u00e1 el m\u00e9todo que calcula la centralidad de grado seg\u00fan el siguiente c\u00f3digo. El nodo con mayor grado es Doug, que tiene un grado de 5, seguido de Alice que tiene un grado de 3.</p> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n{\n    FOLLOWS: {\n    orientation: 'REVERSE'\n    }\n}\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuaci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisi\u00e9ramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n'myGraph_centralidad_grado',\n'User',\n{\n    FOLLOWS: {\n    orientation: 'REVERSE',\n    properties: ['score']\n    }\n}\n)\n</code></pre> <p>La centralidad de grado de este grafo ser\u00eda</p> <p><pre><code>CALL gds.degree.stream('myGraph_centralidad_grado')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score AS followers\nORDER BY followers DESC, id DESC\n</code></pre> C\u00e1lculo de la centralidad de grado</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#82-cercania-closeness-centrality","title":"8.2 Cercan\u00eda (Closeness Centrality)","text":"<p>La cercan\u00eda es una medida de centralidad que determina qu\u00e9 nodos del grafo expanden r\u00e1pida y eficientemente la informaci\u00f3n a trav\u00e9s del grafo. Para calcular esta medida, se obtiene la suma del inverso de las distancias de un nodo al resto. De esta forma, dado un nodo u la cercan\u00eda del mismo se calcula por medio de la siguiente ecuaci\u00f3n:</p> Figura Neo4j. Ecuaci\u00f3n de cercan\u00eda. (Fuente: Neo4J) <p>Donde n es el n\u00famero de nodos del grafo y d(u, v) es la distancia del camino m\u00ednimo entre u y v. La cercan\u00eda es una m\u00e9trica muy utilizada para estimar tiempos de llegada en redes log\u00edsticas, para descubrir actores en posiciones privilegiadas en redes sociales o para estudiar la prominencia de palabras en un documento en el campo de la miner\u00eda de textos. Para obtener la cercan\u00eda en el grafo social de ejemplo, se puede utilizar el fragmento de c\u00f3digo mostrado en el listado 4.14. En el grafo de la imagen, Doug y David tienen una cercan\u00eda de 1.</p> <pre><code>CALL gds.graph.project(\n    'myGraph_cercania',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p><pre><code>CALL gds.closeness.stream('myGraph_cercania')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> C\u00e1lculo de la cercan\u00eda</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#83-intermediacion-betweenness-centrality","title":"8.3 Intermediaci\u00f3n (Betweenness Centrality)","text":"<p>La intermediaci\u00f3n es una medida de centralidad que permite detectar la influencia que tiene un nodo o actor del grafo en el flujo de informaci\u00f3n o de recursos de la red. El c\u00e1lculo de la intermediaci\u00f3n permite identificar a nodos que hacen de puentes entre distintas porciones del grafo. Esta medida de centralidad es muy utilizada para la identificaci\u00f3n de influencers y el estudio de la viralizaci\u00f3n de mensajes en redes sociales. Intuitivamente, la intermediaci\u00f3n de un nodo ser\u00e1 mayor en tanto en cuanto dicho nodo aparezca en los caminos m\u00ednimos de cualquier otro par de nodos. M\u00e1s formalmente, la intermediaci\u00f3n puede calcularse seg\u00fan la siguiente ecuaci\u00f3n.</p> Figura Neo4j. Ecuaci\u00f3n de intermediaci\u00f3n. (Fuente: Neo4J) <p>Donde u es el nodo del cual se calcula la intermediaci\u00f3n (B),s y t son nodos del grafo, p(u) es el n\u00famero de caminos m\u00ednimos entre s y t que pasan por u y p es el n\u00famero de caminos m\u00ednimos entre s y t. Para el c\u00e1lculo de la intermediaci\u00f3n, se puede utilizar el siguiente c\u00f3digo. Al ejecutar este listado, se obtiene que Alice tiene la mayor intermediaci\u00f3n, teniendo un valor de 10.</p> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p>Como no tenemos ninguna propiedad/coste/puntuaci\u00f3n en la relaci\u00f3n, no hay que indicarla. Si la quisi\u00e9ramos tener en cuenta habr\u00eda que a\u00f1adirla dentro de follow de la siguiente forma:</p> <pre><code>CALL gds.graph.project(\n    'myGraph_intermediacion',\n    'User',\n    {\n        FOLLOWS:\n        {\n            properties: 'weight'\n        }\n    }\n)\n</code></pre> <p>El c\u00e1lculo de la indeterminaci\u00f3n ser\u00eda</p> <p><pre><code>CALL gds.betweenness.stream('myGraph_intermediacion')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).id AS id, score\nORDER BY score DESC\n</code></pre> C\u00e1lculo de la intermediaci\u00f3n</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#9-deteccion-de-comunidades","title":"9. Detecci\u00f3n de comunidades","text":"<p>A la hora de trabajar con grafos reales, que presentan una gran cantidad de nodos y enlaces, muchas veces se pretende identificar comunidades dentro del grafo para aplicar algoritmos sobre ellas. Una comunidad es, por tanto, un conjunto de nodos que presentan m\u00e1s relaciones entre s\u00ed que con el resto de nodos fuera de la comunidad. La detecci\u00f3n e identificaci\u00f3n de comunidades permite identificar comportamientos emergentes y de reba\u00f1o dentro de una red. De esta forma, es posible detectar y predecir h\u00e1bitos de los usuarios. A continuaci\u00f3n, se van a aplicar tres m\u00e9todos de detecci\u00f3n de comunidades sobre el grafo social de ejemplo que se ha utilizado en secciones anteriores.</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#91-conteo-de-triangulostriangle-count","title":"9.1 Conteo de tri\u00e1ngulos(Triangle Count)","text":"<p>Un tri\u00e1ngulo es un conjunto de tres nodos que tienen relaciones entre s\u00ed. El conteo de tri\u00e1ngulos para un nodo dado, permite estudiar o inspeccionar de forma global un grafo y, aplicado sobre componentes conexas, permite inspeccionar regiones de un grafo. Para aplicar este m\u00e9todo, es necesario almacenar el grafo teniendo en cuenta que las aristas deben almacenarse como UNDIRECTED. El siguiente c\u00f3digo muestra el fragmento de c\u00f3digo necesario para aplicar este m\u00e9todo sobre el grafo social de ejemplo. La ejecuci\u00f3n de este m\u00e9todo da como resultado que Alice y Doug son aquellos que pertenecen a m\u00e1s tri\u00e1ngulos, un total de 5.</p> <pre><code>CALL gds.graph.project(\n    'myGraph_conteo_triangulo',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n</code></pre> <p><pre><code>CALL gds.triangleCount.stream('myGraph_conteo_triangulo')\nYIELD nodeId, triangleCount\nRETURN gds.util.asNode(nodeId).id AS id, triangleCount\nORDER BY triangleCount DESC\n</code></pre> Ejecuci\u00f3n del conteo de tri\u00e1ngulos</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#92-coeficiente-local-de-clustering-local-clustering-coefficient","title":"9.2 Coeficiente local de clustering (Local Clustering Coefficient)","text":"<p>Este coeficiente proporciona una medida cuantitativa del grado de agrupaci\u00f3n de un nodo. Para el c\u00e1lculo del coeficiente local de clustering se utiliza el conteo de tri\u00e1ngulos, de forma que se compara el grado de agrupaci\u00f3n de un nodo con el grado m\u00e1ximo de agrupaci\u00f3n que podr\u00eda tener. As\u00ed pues, el coeficiente local de clustering se puede calcular mediante la siguiente ecuaci\u00f3n.</p> Figura Neo4j Ecuaci\u00f3n de coeficiente local. (Fuente: Neo4J) <p>Donde u es un nodo, R(u) es el n\u00famero de relaciones que a trav\u00e9s de los vecinos de u (lo cual puede medirse con el n\u00famero de tri\u00e1ngulos que pasan por u y k(u) es el grado de u. El c\u00e1lculo del coeficiente local de clustering puede realizarse por medio del siguiente c\u00f3digo. Los actores Bridget, Charles, Mark y Michael tienen el mayor coeficiente local de clustering, que es 1. </p> <pre><code>CALL gds.graph.project(\n    'myGraph_LCC',\n    'User',\n    {\n        FOLLOWS: {\n        orientation: 'UNDIRECTED'\n        }\n    }\n)\n</code></pre> <p><pre><code>CALL gds.localClusteringCoefficient.stream('myGraph_LCC')\nYIELD nodeId, localClusteringCoefficient\nRETURN gds.util.asNode(nodeId).id AS id, localClusteringCoefficient\nORDER BY localClusteringCoefficient DESC\n</code></pre> C\u00e1lculo del coeficiente local de clustering</p> <p>Por \u00faltimo, el coeficiente global de clustering se calcula como la suma normalizada de los coeficientes de clustering locales. As\u00ed, estos coeficientes nos permiten encontrar medidas cuantitativas para detectar comunidades, pudiendo especificar incluso un umbral para establecer la comunidad (por ejemplo, especificar que los nodos han de estar conectados en un 40 %).</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#93-componentes-fuertemente-conexas-strongly-connected-components","title":"9.3 Componentes fuertemente conexas (Strongly Connected Components)","text":"<p>alpha a 04-11-2023</p> <p>En un grafo dirigido, una componente fuertemente conexa es aquel grupo de nodos en el que cualquier nodo puede ser alcanzado por cualquier otro en ambas direcciones. El estudio de las componentes fuertemente conexas en un grafo permite estudiar la conectividad de la red. Para realizar el c\u00e1lculo de las componentes conexas, el cual se realiza en un tiempo de procesamiento proporcional al n\u00famero de nodos, podemos utilizar el siguiente c\u00f3digo. El resultado permite comprobar que hay un total de cuatro componentes fuertemente conexas en el grafo social.</p> <pre><code>CALL gds.graph.project(\n    'myGraph_SCC',\n    'User',\n    'FOLLOWS'\n)\n</code></pre> <p><pre><code>CALL gds.alpha.scc.stream('myGraph_SCC', {})\nYIELD nodeId, componentId\nRETURN gds.util.asNode(nodeId).id AS Name, componentId AS Component\nORDER BY Component DESC\n</code></pre> Obtenci\u00f3n de componentes fuertemente conexas</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#10-prediccion-de-enlaces","title":"10. Predicci\u00f3n de enlaces","text":"<p>Los grafos son estructuras de datos que representan sistemas din\u00e1micos, que evolucionan a lo largo del tiempo. Por este motivo, es muy com\u00fan que en un grafo cualquiera aparezcan y desaparezcan nuevos nodos y conexiones entre dichos nodos. Los m\u00e9todos de predicci\u00f3n de enlaces permiten predecir qu\u00e9 enlaces se formar\u00e1n pr\u00f3ximamente entre los nodos del grafo, permitiendo adelantarse a los acontecimientos y prevenir eventualidades. Como norma general, los m\u00e9todos de predicci\u00f3n de enlaces se basan en medidas de cercan\u00eda y de centralidad asumiendo que los nuevos enlaces se producir\u00e1n, mayoritariamente, sobre/desde los nodos m\u00e1s relevantes. A continuaci\u00f3n, se detalla el funcionamiento de tres m\u00e9todos com\u00fanmente utilizados en predicci\u00f3n de enlaces. </p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#101-vecinos-comunes-common-neighbors","title":"10.1 Vecinos comunes (Common Neighbors)","text":"<p>alpha a 04-11-2023</p> <p>El m\u00e9todo de vecinos comunes se basa en la idea gen\u00e9rica de que dos actores de la red que tienen una relaci\u00f3n con un usuario com\u00fan tendr\u00e1n m\u00e1s posibilidad de conectarse entre s\u00ed que quienes no. Formalmente, dados dos nodos, la posibilidad de que se produzca un enlace entre los nodos x e y viene dada por la siguiente ecuaci\u00f3n.</p> Figura Neo4j Ecuaci\u00f3n de vecinos comunes. (Fuente: Neo4J) <p>Donde N(x) es el conjunto de nodos adyacentes a x y N(y) es el conjunto de nodos adyacentes a y. Cuanto mayor es el valor de CN calculado, existe mayor posibilidad de que se produzca un nuevo enlace entre x e y. En el grafo social de ejemplo, el c\u00e1lculo de vecinos comunes para Charles y Bridget se puede realizar a trav\u00e9s del siguiente c\u00f3digo. El resultado de este c\u00e1lculo es 2.</p> <p><pre><code>MATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.commonNeighbors(x,y) AS score\n</code></pre> Obtenci\u00f3n del valor de vecinos comunes para Charles y Bridget</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#102-adhesion-preferencial-preferential-attachment","title":"10.2 Adhesi\u00f3n preferencial (Preferential Attachment)","text":"<p>alpha a 04-11-2023</p> <p>Este m\u00e9todo se basa en la idea general de que cuanto m\u00e1s conectado est\u00e1 un nodo, es m\u00e1s probable que reciba nuevos enlaces. Formalmente, esta idea se puede expresar por medio de la siguiente ecuaci\u00f3n.</p> Figura Neo4j Ecuaci\u00f3n de adhesi\u00f3n preferencial. (Fuente: Neo4J) <p>En el grafo social de ejemplo, el c\u00e1lculo de adhesi\u00f3n preferencial para Charles y Bridget se puede realizar a trav\u00e9s del siguiente c\u00f3digo. El resultado de este c\u00e1lculo es 10.</p> <p><pre><code>MATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.preferentialAttachment(x, y) AS score\n</code></pre> Obtenci\u00f3n del valor de adhesi\u00f3n preferencial para Charles y Bridget</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#103-asignacion-de-recursos-resource-allocation","title":"10.3 Asignaci\u00f3n de recursos (Resource Allocation)","text":"<p>alpha a 04-11-2023</p> <p>Se trata de una m\u00e9trica compleja que eval\u00faa la cercan\u00eda de un par de nodos para determinar la posibilidad de que, entre ellos, se produzca un nuevo enlace. Para ello, se utiliza la expresi\u00f3n dada en la siguiente ecuaci\u00f3n.</p> Figura Neo4j Ecuaci\u00f3n de asignaci\u00f3n de recursos. (Fuente: Neo4J) <p>En el grafo social de ejemplo, el c\u00e1lculo de la asignaci\u00f3n de recursos para Charles y Bridget se puede realizar a trav\u00e9s del siguiente c\u00f3digo. El resultado de este c\u00e1lculo es 0.309.</p> <p><pre><code>//No funciona ni en la documentaci\u00f3n oficial (09-11-2022)\nMATCH (x:User{id:'Charles'})\nMATCH (y:User{id:'Bridget'})\nRETURN gds.alpha.linkprediction.resourceAllocation(x, y) AS score\n</code></pre> Obtenci\u00f3n del valor de asignaci\u00f3n de recursos para Charles y Bridget</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#11-machine-learning","title":"11. Machine Learning","text":"<p>Existe tambi\u00e9n la posibilidad de trabajar con Machine learning en los sistema de Big Data orientados a grafos como Neo4j</p> <p>Toda la documentaci\u00f3n de los mismos se encuentra tambi\u00e9n en la documentaci\u00f3n oficial. Esta funcionalidad no forma parte de los Resultados de Aprendizaje de este m\u00f3dulo, ya que existe un m\u00f3dulo especifico para este tema. A\u00fan as\u00ed, podr\u00eda ser interesante investigarlo e intentar integrarlo como una pr\u00e1ctica opcional \ud83d\ude09</p>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/neo4j.html#12-ejemplo","title":"12. Ejemplo","text":"<ol> <li> <p>Para esta pr\u00e1ctica vamos a usar la herramienta SandBox Neo4j: https://sandbox.neo4j.com/</p> <ul> <li>Crearemos un nuevo proyecto y elegiremos el proyecto de twitter.</li> <li>Vamos obtener conocimiento a trav\u00e9s de las relaciones la cuenta oficial de Twitter Neo4j (anteriormente se pod\u00eda con la nuestra, actualmente no tienen habilitada esta opci\u00f3n)</li> <li>Seguimos los pases para dar acceso a Neo4j a nuestra cuenta. No olvides derogar el permiso al terminar el ejemplo</li> </ul> </li> <li> <p>El esquema del grafo que nos va a crear de nuestra cuenta de Twitter es la siguiente</p> </li> </ol> Figura Neo4j Ejemplo. Data model Twitter. (Fuente: Propia) <ul> <li>Qui\u00e9n te est\u00e1 mencionando en Twitter</li> <li>\u00bfQui\u00e9nes son tus seguidores m\u00e1s influyentes?</li> <li>Qu\u00e9 etiquetas usas con frecuencia</li> <li>La gente que tuitea sobre ti, pero no los sigues</li> <li>Enlaces de retweets interesantes</li> <li>Otras personas tuiteando con algunos de tus mejores hashtags</li> </ul> <ol> <li>Menciones</li> </ol> <p>La siguiente consulta de Cypher determina qui\u00e9n te est\u00e1 mencionando en Twitter (en este caso a la cuenta de Neo4j).</p> <p><pre><code>// Graph of some of your mentions\nMATCH (u:User)-[p:POSTS]-&gt;(t:Tweet)-[:MENTIONS]-&gt;(m:Me)\nWITH u,p,t,m, COUNT(m.screen_name) AS count\nORDER BY count DESC\nRETURN u,p,t,m\nLIMIT 10\n</code></pre> Detalles de algunas de tus menciones</p> <pre><code>// Number of your mentions\nMATCH (u:User)-[:POSTS]-&gt;(t:Tweet)-[:MENTIONS]-&gt;(m:Me)\nRETURN m.screen_name AS screen_name, COUNT(m.screen_name) AS count \n</code></pre> <ol> <li>Seguidores m\u00e1s influyentes</li> </ol> <pre><code>// Most influential followers\nMATCH (follower:User)-[:FOLLOWS]-&gt;(u:User:Me)\nRETURN follower.screen_name AS user, follower.followers AS followers\nORDER BY followers DESC\nLIMIT 10\n</code></pre> <ol> <li>Hashtags mas usados</li> </ol> <pre><code>// The hashtags you have used most often\nMATCH (h:Hashtag)&lt;-[:TAGS]-(t:Tweet)&lt;-[:POSTS]-(u:User:Me)\nWITH h, COUNT(h) AS Hashtags\nORDER BY Hashtags DESC\nRETURN h.name, Hashtags\nLIMIT 10\n</code></pre> <ol> <li>Recomendaciones de seguidores. \u00bfQui\u00e9n tuitea sobre ti, pero no lo sigues?</li> </ol> <pre><code>// Follower Recommendations - tweeting about you, but you don't follow\nMATCH (ou:User)-[:POSTS]-&gt;(t:Tweet)-[mt:MENTIONS]-&gt;(me:User:Me)\nWITH DISTINCT ou, me\nWHERE (ou)-[:FOLLOWS]-&gt;(me) AND NOT (me)-[:FOLLOWS]-&gt;(ou)\nRETURN ou.screen_name\n</code></pre> <ol> <li>Enlaces de retweets interesantes. \u00bfQu\u00e9 enlaces retuiteas y con qu\u00e9 frecuencia se marcan como favoritos?</li> </ol> <pre><code>// Links from interesting retweets\nMATCH\n  (:Me)-[:POSTS]-&gt;\n  (t:Tweet)-[:RETWEETS]-&gt;(rt)-[:CONTAINS]-&gt;(link:Link)\nRETURN\n  t.id_str AS tweet, link.url AS url, rt.favorites AS favorites\nORDER BY\n  favorites DESC\nLIMIT 10\n</code></pre> <p>Mismo que el anterior, pero mostrando el nombre del usuario del tweet original</p> <pre><code>// Links from interesting retweets\nMATCH\n  (u:Me)-[:POSTS]-&gt;(t:Tweet)-[:RETWEETS]-&gt;(rt)-[:CONTAINS]-&gt;(link:Link)\nMATCH\n  (u:Me)-[:POSTS]-&gt;(t_link:Tweet)-[:RETWEETS]-&gt;(rt)&lt;-[:POSTS]-(m:User)\nRETURN\n  t.id_str AS tweet, link.url AS url, rt.favorites AS favorites, m.screen_name\nORDER BY\n  favorites DESC\nLIMIT 10\n</code></pre> <ol> <li>Hashtags comunes. \u00bfQu\u00e9 usuarios tuitean con algunos de tus mejores hashtags?</li> </ol> <pre><code>// Users tweeting with your top hashtags\nMATCH\n  (me:Me)-[:POSTS]-&gt;(tweet:Tweet)-[:TAGS]-&gt;(ht)\nMATCH\n  (ht)&lt;-[:TAGS]-(tweet2:Tweet)&lt;-[:POSTS]-(sugg:User)\nWHERE\n  sugg &lt;&gt; me\n  AND NOT\n  (tweet2)-[:RETWEETS]-&gt;(tweet)\nWITH\n  sugg, collect(distinct(ht)) as tags\nRETURN\n  sugg.screen_name as friend, size(tags) as common\nORDER BY\n  common DESC\nLIMIT 20\n</code></pre> <ol> <li> <p>https://orientdb.org \u21a9</p> </li> <li> <p>https://github.com/twitter-archive/flockdb \u21a9</p> </li> <li> <p>https://objectivity.com/infinitegraph/ \u21a9</p> </li> <li> <p>https://neo4j.com \u21a9</p> </li> </ol>"},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/Ejercicios_clase/MongoDB_Ejemplo1.html","title":"Big Data APlicado","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/Ejercicios_clase/MongoDB_Ejemplo1.html#ud-3-gestion-de-soluciones-iii","title":"UD 3 - Gesti\u00f3n de Soluciones III","text":""},{"location":"UD2%20-%20Procesado%20y%20Presentaci%C3%B3n%20Datos%20Almacenados/Ejercicios_clase/MongoDB_Ejemplo1.html#ejemplo-1-mongodb","title":"Ejemplo 1 MongoDB","text":"<ol> <li> <p>Instalar MongoDB. Para ello puedes optar por diferentes opciones</p> <ul> <li>Instalarla en tu m\u00e1quina local</li> <li>Usar Atlas MongoDB: https://www.mongodb.com/docs/atlas/getting-started/</li> <li>Crear un contendor docker. Yo voy a usar este caso</li> </ul> </li> <li> <p>Instalar MondoDB en Docker</p> </li> <li>Vamos a la imagen oficial de mondodb en docker</li> <li>Seguimos los pasos, que tienen este formato</li> </ol> <p><code>docker run -d --network some-network --name some-mongo \\     -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\     -e MONGO_INITDB_ROOT_PASSWORD=secret \\     mongo</code></p> <pre><code>En mi caso voy a a\u00f1adr un bind mount para poder luego importar un fichero para el ejemplo. Por tanto ser\u00eda:\n\n```\ndocker run -d --name mongo-bda1 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=admin --mount type=bind,src=/home/jaime/BigData/BDA/,dst=/home/bda mongo\n```\n</code></pre> <ol> <li> <p>Abre una terminal en el contenedor</p> <pre><code>docker exec -it mongodb-bda1 bash\n</code></pre> </li> <li> <p>Entrar a la consola de mongo y autorizarnos</p> <pre><code>mongosh\nuse admin\ndb.auth(\"admin\",\"admin\")\n</code></pre> </li> <li> <p>Ver las bases de datos que hay</p> <pre><code>show collections\n</code></pre> </li> <li> <p>Ver las colecciones de una base de datos</p> <pre><code>show dbs\n</code></pre> </li> <li> <p>Crear y/o usar una Base de datos (use nombre_bd)</p> <p><pre><code>use db_ejemplo1\n</code></pre> 8. Probamos los ejemplos que aparecen en el temario</p> </li> <li> <p>Probamos otro ejemplo. Vamos a importar un archivo fuente. Para ello nos abrimos otra terminal en la m\u00e1quina y ejecutamos el siguiente comando</p> <pre><code>mongoimport --db=db_ej1_restaurantes --collection=restaurantes --file=home/bda/UD3/Ejemplo1/restaurantes1.csv --authenticationDatabase=admin --username=admin --password=admin\n</code></pre> </li> <li> <p>Comprobamos la carga</p> <pre><code>show dbs\ndb.restaurantes.find()\n</code></pre> </li> <li> <p>Crear una consultar para encontrar qu\u00e9 restaurantes no tienen direcci\u00f3n (Todas tienen direcci\u00f3n)</p> <pre><code>db.restaurantes.find({address:{$exists:false}})\n</code></pre> </li> <li> <p>Crear una consulta para encontrar aquellos restaurantes de cocina italiana que se encuentren en la zona geogr\u00e1fica con c\u00f3digo postal 10075</p> <p><pre><code>db.restaurantes.find({$and:[{\"cuisine\":\"Italian\"},{\"address.zipcode\":\"10075\"}]})\n</code></pre> 13. Encontrar aquellos restaurantes que tengan grado A, puntuaci\u00f3n 11 y fecha 2014-10-01T00:00:00Z</p> <pre><code>db.restaurantes.find({grades:{\"date\":ISODate(\"2014-10-01T00:00:00Z\"),\"grade\":\"A\",\"score\":11}})\n</code></pre> </li> <li> <p>Contabiliza cu\u00e1ntos restaurantes tienen una puntuaci\u00f3n menor o igual a 5</p> <pre><code>db.restaurantes.find({\"grades.score\":{$lt:5}}).count()\n</code></pre> </li> <li> <p>Obtener los nombres del segundo y el tercer restaurante de cocina italiana ordenados por nombre</p> <pre><code>db.restaurantes.find({\"cuisine\":\"Italian\"}).sort({name:1}).limit(2).skip(1)\n</code></pre> </li> <li> <p>A\u00f1adir una valoraci\u00f3n al restaurante 41156888</p> <pre><code>db.restaurantes.updateOne({\"restaurant_id\":\"41156888\"},{$push:{grades: {\"date\":ISODate(\"2016-01-02T00:00:00.000Z\"),\"grade\":\"A\",\"score\":14}}})\n</code></pre> </li> </ol>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html","title":"UD 3 - Introducci\u00f3n a la Computaci\u00f3n Paralela y Distribuida","text":""},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#resultados-de-aprendizaje","title":"Resultados de Aprendizaje","text":"<ul> <li>RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.</li> </ul>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/index.html#contenido-y-criterios-de-evaluacion","title":"Contenido y Criterios de Evaluaci\u00f3n","text":"Contenido Criterios de Evaluaci\u00f3n Introducci\u00f3n a la Computaci\u00f3n Paralela y Distribuida RA5075.3a - Se ha valorado la importancia de la calidad de los datos en los sistemas de ficheros distribuidos."},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html","title":"UD 3 - Introducci\u00f3n a la Computaci\u00f3n Paralela y Distribuida","text":""},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#31-computacion-paralela","title":"3.1 Computaci\u00f3n Paralela","text":"<p>Evoluci\u00f3n de computaci\u00f3n desde Von Neuman a los sistemas actuales de computaci\u00f3n</p> <p>Tanto el volumen de datos como la velocidad de procesamiento han seguido una trayectoria de aumento exponencial desde el inicio de la era digital, pero el primero ha aumentado a un ritmo mucho mayor que el segundo. De esta forma se hace imprescindible contar con nuevas herramientas como la computaci\u00f3n o el procesamiento paralelo para conseguir salvar la brecha generada. Adem\u00e1s de proporcionar una mayor capacidad de procesamiento para hacer frente a los requisitos de los grandes conjuntos de datos, el procesamiento paralelo tiene el potencial de aliviar el cuello de botella de von Neumann cuando los datos necesarios no pueden ser suministrados al procesador a la velocidad requerida.</p> <p>Los algoritmos y arquitecturas de procesamiento paralelo se estudian desde desde la d\u00e9cada de 1950 como una forma de mejorar el rendimiento de los sistemas de informaci\u00f3n y, m\u00e1s recientemente, como una forma de mejorar su rendimiento manteniendo el consumo de energ\u00eda bajo control.</p> <p>Los primeros supercomputadores segu\u00edan una arquitectura de flujo de instrucciones \u00fanico, flujo de datos m\u00faltiple (SIMD) que utiliza una \u00fanica unidad de ejecuci\u00f3n de instrucciones, con cada instrucci\u00f3n aplicada a m\u00faltiples elementos de datos simult\u00e1neamente. La otra clase principal de arquitecturas paralelas se conoce como MIMD, en la que hay m\u00faltiples flujos de instrucciones adem\u00e1s de m\u00faltiples flujos de datos. Los superordenadores modernos, incluida la mayor\u00eda de las entradas de la lista Top500 Supercomputers, tienden a ser MIMD. Los superordenadores de gama alta suelen utilizarse para realizar c\u00e1lculos num\u00e9ricos intensivos con n\u00fameros en coma flotante. Por ello, su rendimiento se mide en operaciones de coma flotante por segundo, o FLOPS.</p> <p>Por otro lado, la ley de Moore muestra el crecimiento exponencial en el n\u00famero de transistores, los cu\u00e1les eran doblados aproximadamente cada 2 a\u00f1os. Hasta principios del siglo XXI, el aumento de la densidad de los chips iba acompa\u00f1ado de una mejora exponencial del rendimiento, debido a las correspondientes frecuencias de reloj m\u00e1s altas. Despu\u00e9s, las l\u00edneas de tendencia de la frecuencia de reloj y el rendimiento comenzaron a aplanarse, sobre todo por el efecto de la disipaci\u00f3n del calor, lo que dificult\u00f3 la refrigeraci\u00f3n de los circuitos superdensos. En ese momento, la atenci\u00f3n a la mejora del rendimiento se traslad\u00f3 al uso de m\u00faltiples procesadores independientes en el mismo chip, dando lugar a los procesadores multin\u00facleo. Esto provoca que el rendimiento sea m\u00e1s eficiente desde el punto de vista energ\u00e9tico, dado que el consumo de energ\u00eda es una funci\u00f3n superlineal de el rendimiento de un solo n\u00facleo.</p> <p>Sin embargo, la aparici\u00f3n del Big Data requiere una reevaluaci\u00f3n de la forma en que medir el rendimiento de los superordenadores. Mientras que la clasificaci\u00f3n de FLOPS la entrada/salida y el ancho de banda de almacenamiento asumen un papel m\u00e1s importante en la determinaci\u00f3n del rendimiento (un superordenador que realiza c\u00e1lculos cient\u00edficos puede recibir escasos par\u00e1metros de entrada y un conjunto de ecuaciones que definen un un modelo y luego realizar c\u00e1lculos durante d\u00edas o semanas, antes de arrojar las respuestas), muchas aplicaciones Big Data requieren un flujo constante de nuevos datos que se introducen, se almacenan, se procesan y se ofrecen como resultados, con lo que posiblemente se ponga a prueba la memoria y el ancho de banda de E/S, capacidades que suelen ser m\u00e1s limitadas que la velocidad de c\u00e1lculo.</p> <p>Por tanto, en los \u00faltimos a\u00f1os, se han introducido muchos modelos abstractos de procesamiento paralelo en un esfuerzo por para representar con precisi\u00f3n los recursos de procesamiento, almacenamiento y comunicaci\u00f3n, junto con sus interacciones, permitiendo a los desarrolladores de aplicaciones paralelas visualizar y aprovechar las ventajas disponibles, sin tener que ocuparse en detalles espec\u00edficos de la m\u00e1quina. Con un alto nivel de abstracci\u00f3n podemos distinguir los enfoques de procesamiento paralelo de datos y de control.</p> <p>El paralelismo de datos implica la partici\u00f3n de un gran conjunto de datos entre m\u00faltiples nodos de procesamiento, cada uno de ellos operando sobre una parte asignada de los datos, antes de que los resultados parciales se terminen combinando. A menudo, en las aplicaciones Big Data el mismo conjunto de operaciones debe ejecutarse en cada subconjunto de datos, por lo que el procesamiento SIMD es la alternativa m\u00e1s eficiente. En la pr\u00e1ctica, sin embargo, la sincronizaci\u00f3n de un gran n\u00famero de nodos de procesamiento introduce sobrecargas e ineficiencias que reducen las ganancias de velocidad. Por eso puede ser beneficioso dirigir los nodos de procesamiento para que ejecuten un \u00fanico programa en m\u00faltiples de datos de forma as\u00edncrona, con una coordinaci\u00f3n dispersa.</p> <p>En los esquemas basados en el paralelismo de control varios nodos operan independientemente resolviendo subproblemas, sincroniz\u00e1ndose entre s\u00ed mediante el env\u00edo de mensajes informativos y de sincronizaci\u00f3n. Dicha independencia permite emplear recursos heterog\u00e9neos y recursos espec\u00edficos de la aplicaci\u00f3n sin interferencias cruzadas ni recursos m\u00e1s lentos obstaculicen el progreso de los m\u00e1s r\u00e1pidos. La Ley de Amdahl (Gene Amdahl, 1867) dice que a no ser que un programa secuencial pudiera ser completamente paralelizado, el speedup o aceleraci\u00f3n que se obtendr\u00e1 ser\u00e1 muy limitado independientemente del n\u00famero de n\u00facleos disponibles.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#32-computacion-distribuida","title":"3.2 Computaci\u00f3n Distribuida","text":"<p>Un sistema distribuido es una colecci\u00f3n de ordenadores aut\u00f3nomos enlazados por una red de ordenadores y soportados por un software que hace que la colecci\u00f3n act\u00fae como un servicio integrado.</p> <p>Algunos de los aspectos m\u00e1s importantes a tener en cuenta dentro de un sistema distribuido actual son los siguientes:</p> <ul> <li> <p>Apertura: El sistema debe ser ampliable, es decir, debe existir la posibilidad de a\u00f1adir nuevos recursos y servicios compartidos y que \u00e9stos se puedan poner a disposici\u00f3n de los diferentes componentes que forman el sistema.</p> </li> <li> <p>Concurrencia: Los distintos componentes de un sistema distribuido pueden demandar acceder a un recurso simult\u00e1neamente. Es necesario que el sistema est\u00e9 dise\u00f1ado para permitirlo.</p> </li> <li> <p>Escalabilidad: Un sistema es escalable si mantiene su eficiencia cuando hay un incremento significativo en el n\u00famero de recursos y el n\u00famero de usuarios.</p> </li> <li> <p>Heterogeneidad: El sistema distribuido puede estar formado por una variedad de diferentes redes, sistemas operativos, hardware, etc. A pesar de estas diferencias, los componentes deben poder interactuar entre s\u00ed.</p> </li> <li> <p>Tolerancia a fallos: Cualquier sistema puede fallar. En el caso de un sistema distribuido a pesar del fallo de un componente tiene que ser posible que el sistema siga funcionando.</p> </li> <li> <p>Transparencia: La transparencia permite que ciertos aspectos del sistema sean invisibles a las aplicaciones, por ejemplo, la ubicaci\u00f3n, el acceso, la concurrencia, la gesti\u00f3n de los fallos, la replica de la informaci\u00f3n, etc.</p> </li> </ul> <p>Los sistemas computacionales distribuidos son sistemas de computaci\u00f3n de alto rendimiento que est\u00e1n formados por conjuntos de ordenadores interconectados mediante una red que ofrecen funcionalidades diversas algunas de ellas propias de la supercomputaci\u00f3n, tales como la computaci\u00f3n paralela. Entre estos sistemas cabe destacar los basados en Grid o en arquitecturas Cloud (en la nube) y de forma m\u00e1s b\u00e1sica, los denominados Clusters.</p> <p>Los Clusters est\u00e1n formados por colecciones de computadores de similares caracter\u00edsticas interconectados mediante una red. Los computadores hacen uso de un mismo sistema operativo y un middleware que se encarga de abstraer y virtualizar los diferentes computadores del sistema dando la visi\u00f3n al usuario de un sistema operativo \u00fanico. Los clusters son sistemas dedicados a la supercomputaci\u00f3n. El sistema operativo de un cluster es est\u00e1ndar y, por lo tanto, es el middleware quien provee de librer\u00edas que permiten la computaci\u00f3n paralela.</p> Figura 3.1: Esquema de organizaci\u00f3n de un servicio de supercomputaci\u00f3n en Cluster. (Fuente: UCLM)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#33-grid-computing","title":"3.3 Grid Computing","text":"<p>Se define Grid Computing como una infraestructura persistente que soporta la computaci\u00f3n y actividades de datos intensivas a trav\u00e9s de m\u00faltiples organizaciones virtuales. Evidentemente, todas las implementaciones de Grid Computing se basan en la comunicaci\u00f3n a trav\u00e9s de Internet mediante distintos protocolos. Actualmente existen dos aproximaciones para la implementaci\u00f3n de arquitecturas en Grid: Arquitecturas SOA y Arquitecturas Peer-to-Peer. </p> <p>Las arquitecturas SOA (Service Oriented Architecture) est\u00e1n basadas en la agregaci\u00f3n de servicios a los que se accede remotamente de forma independiente de las plataformas en las que se ejecutan y del lenguaje en los que est\u00e1n implementados. Un servicio es un programa autocontenido con una funci\u00f3n y un interfaz bien definido. Este tipo de servicios se adapta muy bien a los sistemas Grid ya que ofrece, entre otras ventajas, una alta portabilidad al permitir interactuar servicios de sistemas heterog\u00e9neos a trav\u00e9s de interfaces que son independientes de las plataformas donde est\u00e1n alojados; interoperabilidad a trav\u00e9s de protocolos est\u00e1ndar de redes, y a su vez, permiten el uso de clientes ligeros que a\u00edslan a los consumidores de la complejidad de los servicios.</p> Figura 3.2: Arquitecturas SOA. (Fuente: appseconnect) <p>Las arquitecturas Peer-To-Peer son agregaciones de programas equivalentes que se ejecutan sobre plataformas heterog\u00e9neas y que comparten parte de su memoria y capacidad de c\u00f3mputo. Estas plataformas presentan una alta tolerancia a errores, dado que cada nodo tiene la misma funci\u00f3n que el resto. Este tipo de arquitecturas son m\u00e1s habituales en un tipo de computaci\u00f3n conocido como Volunteer Computing.</p> <p>Escalabilidad</p> <p>La tecnolog\u00eda Grid permite de una forma muy f\u00e1cil la escalabilidad, puesto que los nodos que act\u00faan en ella son independientes a los dem\u00e1s. Esto permite que, si alguno de los nodos falla, se pueden delegar las tareas que estuviera realizando al resto de nodos, lo que permite seguir con la ejecuci\u00f3n del programa y evitar paradas que puedan afectar a la efectividad de la obtenci\u00f3n de los resultados deseados.</p> <p>Adem\u00e1s, este tipo de estructuraci\u00f3n permite que se puedan escalar los recursos para cada computador o nodo de forma independiente en caso de que fuera necesario, sin tener que afectar al resto de computadoras. En el caso de actualizaciones que deban sufrir los equipos para obtener nuevos fragmentos de programas o mejoras acerca de ciertos aspectos, esta tecnolog\u00eda permite realizarlas de forma sencilla. Esto se consigue mediante la obtenci\u00f3n de los recursos que vayan a verse afectados por estas actualizaciones y son puestas offline y retiradas de la red. Mientras tanto, las tareas que suelan llevar son delegadas al resto de computadoras que se encuentren en las otras localizaciones. Esto permite que las actualizaciones se procedan en forma de cascada, haciendo que el trabajo en los proyectos que se est\u00e9n llevando a cabo no se vea afectado.</p> <p>Ventajas e Inconvenientes</p> <p>Algunas de las ventajas que ofrece el Grid Computing son las siguientes:</p> <ul> <li>Es robusta en cuanto a sucesos que pudieran afectar a parte de su infraestructura, por ejemplo, cat\u00e1strofes naturales. Los entornos Grid son bastante modulares e independientes, lo que reduce sus puntos vulnerables a fallo.</li> <li>Es una manera muy eficaz de utilizar los recursos de una manera \u00f3ptima y eficiente en una organizaci\u00f3n.</li> <li>Puede utilizarse para balanceos de carga y conexiones de red que sean redundantes ofreciendo muchas facilidades en cuanto a la escalabilidad y actualizaci\u00f3n y siendo muy conveniente para ejecutar programas o tareas de forma paralela.</li> </ul> <p>Algunas de las desventajas que tiene esta tecnolog\u00eda son: los riesgos de seguridad que ofrece la participaci\u00f3n de diferentes entidades heterog\u00e9neas y la necesidad de una interconexi\u00f3n de altas prestaciones.</p> Figura 3.3: Grid Computing vs Cloud Computing. (Fuente: Medium)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#34-cloud-computing","title":"3.4 Cloud Computing","text":"<p>La computaci\u00f3n en nube o Cloud Computing es una de las tecnolog\u00edas actuales de mayor implantaci\u00f3n. El concepto de computaci\u00f3n en la nube supone el siguiente paso evolutivo de la computaci\u00f3n distribuida superando los sistemas legacy y evolucionando hacia los sistemas distribuidos a gran escala. El objetivo de este modelo inform\u00e1tico es hacer un mejor uso de los recursos distribuidos, ponerlos en com\u00fan para lograr un mayor rendimiento y poder abordar problemas de computaci\u00f3n a gran escala.</p> <p>En la computaci\u00f3n en la nube se deben abordar diferentes retos como: la virtualizaci\u00f3n, la escalabilidad, la interoperabilidad, la calidad del servicio, la tolerancia a errores y los modelos de nube.</p> <p>B\u00e1sicamente, la nube puede definirse mediante tres modelos distintos:</p> <ul> <li> <p>Nube privada: Los datos y procesos se gestionan dentro de la organizaci\u00f3n sin las restricciones de ancho de banda de la red, los requerimientos de seguridad y los requisitos legales que supone el uso de los servicios de la nube p\u00fablica a trav\u00e9s de redes p\u00fablicas y abiertas. Algunos ejemplos son Amazon VPC.</p> </li> <li> <p>Nube p\u00fablica: Describe la computaci\u00f3n en nube en el sentido tradicional, los recursos son facilitados de forma din\u00e1mica sobre una base de auto-servicio a trav\u00e9s de aplicaciones web/servicios web, desde un proveedor externo que comparte recursos. Algunos ejemplos son Azure o Amazon EC2.</p> </li> <li> <p>Nube h\u00edbrida: El entorno est\u00e1 formado por m\u00faltiples proveedores internos y/o externos. Algunos ejemplos son RightScale, Asigra Hybrid Cloud Backup, Carpathia, Skytap y Elastra.</p> </li> </ul> <p>La arquitectura en la nube o arquitectura cloud subyacen en una infraestructura que se se utiliza s\u00f3lo cuando se necesita para extraer los recursos necesarios bajo demanda y realizar un trabajo espec\u00edfico, para luego ceder los recursos innecesarios. Los servicios son accesibles en cualquier parte del mundo, y la nube aparece como un \u00fanico punto de acceso para todas las necesidades inform\u00e1ticas de los usuarios. Las arquitecturas en la nube abordan las principales dificultades que rodean el procesamiento de datos a gran escala. Existen diferentes categor\u00edas de servicios en la nube, como infraestructura, plataforma y aplicaciones. Estos servicios se prestan y consumen en tiempo real a trav\u00e9s de Internet.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#341-software-como-servicio-software-as-a-service-saas","title":"3.4.1 Software como servicio - Software as a Service (SaaS)","text":"<p>El software como servicio es una plataforma multiusuario. Utiliza recursos comunes y una instancia \u00fanica tanto del c\u00f3digo objeto de una aplicaci\u00f3n como de la base de datos subyacente para dar soporte a varios clientes simult\u00e1neamente. Este servicio es el nuevo m\u00e9todo en la distribuci\u00f3n de software de aplicaciones. Algunos ejemplos de los principales proveedores son SalesForce.com, NetSuite, Oracle, IBM y Microsoft, etc.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#342-plataforma-como-servicio-paas-platform-as-a-service-paas","title":"3.4.2 Plataforma como servicio PaaS - Platform as a Service (PaaS)","text":"<p>La plataforma como servicio proporciona a los desarrolladores una plataforma que incluye todos los sistemas y entornos, que comprende el ciclo de vida completo de desarrollo, prueba despliegue y alojamiento de aplicaciones web como un servicio prestado con una base en la nube. Proporciona una forma m\u00e1s f\u00e1cil de desarrollar aplicaciones empresariales y diversos servicios a trav\u00e9s de Internet. El PaaS se plantea para facilitar el mantenimiento de la infraestructura de trabajo de los diferentes sistemas. Su aplicaci\u00f3n debe reducir el tiempo de desarrollo al ofrecer un amplio abanico de herramientas y servicios f\u00e1cilmente disponibles, y con una r\u00e1pida capacidad de escalado.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#343-infraestructura-como-servicio-infrastructure-as-a-service-iaas","title":"3.4.3 Infraestructura como servicio - Infrastructure as a Service (IaaS)","text":"<p>Infraestructura como servicio ofrece acceso basado en la web a almacenamiento y potencia de c\u00e1lculo. Por ello, no se necesita gestionar o controlar la infraestructura subyacente de la nube, pero si que se tiene control sobre los sistemas operativos, el almacenamiento y las aplicaciones desplegadas. Adem\u00e1s de una mayor flexibilidad, una de las principales ventajas de IaaS es el esquema de pago basado en el uso. Esto permite a los clientes pagar a medida que crecen. Otra ventaja importante es la de utilizar siempre la \u00faltima tecnolog\u00eda.</p> <p>El camino hacia la computaci\u00f3n en nube est\u00e1 impulsado por muchos factores, como la ubicuidad del acceso (todo lo que se necesita es un navegador), la facilidad de gesti\u00f3n (no hay necesidad de mejorar la experiencia del usuario, ya que no se necesita configuraci\u00f3n o copia de seguridad), y una menor inversi\u00f3n (soluci\u00f3n empresarial asequible desplegada sobre la base del pago por uso del hardware, con un software de sistemas proporcionado por los proveedores de la nube). Adem\u00e1s, la computaci\u00f3n en nube ofrece muchas ventajas a los proveedores, como una infraestructura f\u00e1cil de gestionar dado que el centro de datos tiene un hardware y un software de sistema homog\u00e9neos.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#344-soluciones-cloud-compiting","title":"3.4.4 Soluciones Cloud Compiting","text":"<ul> <li>Amazon Elastic Map Reduce (EMR)<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Amazon EMR?</li> </ul> </li> <li>Microsoft Azure HDInsight<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Azure HDInsight?</li> </ul> </li> <li>Google Dataproc<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Dataproc?</li> </ul> </li> </ul>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#345-principales-servicios-de-datos-en-cloud","title":"3.4.5 Principales Servicios de Datos en Cloud","text":"<p>Para comprender mejor los servicios que ofrecen los proveedores cloud, podemos clasificarlos en varios grupos en funci\u00f3n de sus caracter\u00edsticas:</p> <ul> <li>Ingesta</li> <li>Procesamiento</li> <li>Almacenamiento</li> <li>An\u00e1lisis</li> </ul> <p>En la siguiente tabla se muestran estos grupos de servicios, con la categor\u00eda a la que pertenecen, las alternativas en los principales proveedores de servicios cloud (HDInsight, AWS y GCP) y su opci\u00f3n Open Source.</p> GRUPO CATEGOR\u00cdA AZURE AWS GCP OPEN SOURCE Ingesta ETL Data Factory Glue Dataprep Apache NiFi Ingesta Message Queue Streaming EventHub Kinesis Pub/Sub Ingesta Scheduling Logic Apps, Batch Stepfunctions, Cloudwatch, EventBridge Cloud Scheduler, Cloud Batch, Cloud Workflows, Cloud Composer Apache Airflow Procesamiento Procesamiento Functions, Data Factory, Databricks Lambda, EMR, Glue, Databricks Cloud Functions, DataProc, DataFlow Apache Flink, Apache Spark, Apache Hadoop Almacenamiento Relacional Azure SQL DB RDS CloudSQL, BigTable MySQL, MariaDB, PostgreSQL Almacenamiento NoSQL CosmosDB DynamoDB, DocumentDB Datastore, Firestore Elasticsearch, Apache Cassandra, MongoDB Almacenamiento OLAP, Data Warehouse Synapse Analytics, Snowflake Redshift, Athena, Snowflake BigQuery, Snowflake Apache Druid, Apache Hive, Presto Almacenamiento Objetos Blob Storage S3 Cloud Storage Minio, Ceph Almacenamiento Cache Cache for Redis ElastiCache Memorystore Memcached, Redis Almacenamiento Grafo CosmosDB Neptune Vertex AI Neo4j An\u00e1lisis Machine Learning Azure ML SageMaker Cloud datalab, Vertex AI Tensorflow, Keras, PyTorch An\u00e1lisis Business Intelligence PowerBI Quicksight Data Studio, Looker Grafana, Apache Superset"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#35-big-data-pipeline","title":"3.5 Big Data Pipeline","text":"<p>Una pipeline de datos es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Las pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. Por tanto, el proceso de extremo a extremo de recopilar datos, convertirlos en conocimiento, entrenar un modelo, difundir conocimiento y aplicar el modelo cuando y donde sea necesario la acci\u00f3n para lograr el objetivo empresarial est\u00e1 unido por un data pipeline.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#351-etapas","title":"3.5.1 Etapas","text":"<p>Un proceso de big data tiene cinco etapas:</p> <ul> <li>Captura de datos de fuentes internas y externas. Las fuentes de datos (aplicaciones m\u00f3viles, sitios web, aplicaciones web, microservicios, dispositivos IoT, etc.) est\u00e1n instrumentadas para capturar datos relevantes.</li> <li>Ingerir datos a trav\u00e9s de trabajos por lotes o flujos. Las fuentes instrumentadas bombean los datos a varios puntos de entrada (HTTP, MQTT, cola de mensajes, etc.). Tambi\u00e9n puede haber trabajos para importar datos de servicios como Google Analytics. Los datos pueden tener dos formas: blobs por lotes y secuencias.</li> <li> <p>Almacenar en Data Lake o Data Warehouse. A menudo, los datos/eventos sin procesar se almacenan en Data Lakes y se limpian, se eliminan duplicados y anomal\u00edas y se transforman para ajustarse al esquema. Finalmente, estos datos listos para consumir se almacenan en un Data Warehouse.</p> </li> <li> <p>Procesamiento de an\u00e1lisis inform\u00e1ticos y/o funciones de aprendizaje autom\u00e1tico. Aqu\u00ed es donde ocurren el an\u00e1lisis, la ciencia de datos y el aprendizaje autom\u00e1tico. La computaci\u00f3n puede ser una combinaci\u00f3n de procesamiento por lotes (batch) y/o en flujo (streaming). Los modelos y los conocimientos se almacenan en el almac\u00e9n de datos.</p> </li> <li>Uso en dashboards, ciencia de datos y aprendizaje autom\u00e1tico. La informaci\u00f3n se entrega a trav\u00e9s de dashboards, correos electr\u00f3nicos, SMS, notificaciones, microservicios, etc. </li> </ul> Figura 3.4: Estados de un BigData Pipeline. (Fuente: ml4devs)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#352-arquitectura","title":"3.5.2. Arquitectura","text":"<p>Hay varias opciones de arquitecturas que ofrecen diferentes compensaciones de rendimiento y costes. Es decir, habr\u00e1 distintas soluciones arquitect\u00f3nicas dependiendo del tipo de soporte, an\u00e1lisis y BI que necesitemos y recursos de los que dispongamos en cada caso.Para decidir que arquitectura, deber\u00edas hacerte algunas preguntas para identificar requisitos, como por ejemplo:</p> <ul> <li>\u00bfNecesita informaci\u00f3n en tiempo real o por lotes?</li> <li>\u00bfCu\u00e1l es la tolerancia al estancamiento de su aplicaci\u00f3n?</li> <li>\u00bfCu\u00e1les son las restricciones de costes?</li> <li>...</li> </ul> <p>Como ejemplo general que abarca un gran numero de escenarios de arquitecturas, tomamos Lambda Architecture, cuya arquitectura de procesamiento de datos est\u00e1 dise\u00f1ada para manejar cantidades masivas de datos en batch y streaming.</p> Figura 3.5: Arquitectura de un BigData Pipeline. (Fuente: ml4devs)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#353-data-pipeline-en-la-nube","title":"3.5.3 Data Pipeline en la nube","text":"<p>En la imagen se muestra Los Data Pipeline en los diferentes servicios de Cloud Computing. Cada uno de ellos se corresponde estrechamente con la arquitectura general de big data pipeline analizada anteriormente. Podemos utilizarlos como referencia para seleccionar tecnolog\u00edas adecuadas a nuestras necesidades.</p> Figura 3.6: BigData Pipeline on AWS, Azure and GCP. (Fuente: ml4devs)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#354-bases-de-datos-en-la-nube","title":"3.5.4 Bases de datos en la nube","text":"<p>Como informaci\u00f3n adicional, y para completar el gran ecosistema de cloud computing, a\u00f1adimos la siguiente imagen donde se muestran las diferentes Bases de Datos en los servicios de Cloud Computing.</p> Figura 3.7: Cloud DataBase on AWS, Azure and GCP. (Fuente: ml4devs)"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#36-sistemas-de-archivos-distribuidos","title":"3.6 Sistemas de Archivos Distribuidos","text":"<p>El sistema de archivos es un subsistema de un sistema operativo cuyo objetivo es organizar, recuperar y almacenar datos archivos. Un sistema de archivos distribuido (DFS) es un sistema de archivos con archivos compartidos en recursos de almacenamiento dispersos en una red. El DFS hace f\u00e1cil compartir archivos entre aplicaciones cliente de forma controlada y autorizada. Asimismo, los usuarios pueden beneficiarse de los servicios de un DFS ya que pueden localizar todos los archivos compartidos dentro de un \u00fanico servidor o nombre de dominio.</p> <p>Una gran variedad de aplicaciones de an\u00e1lisis Big Data dependen de entornos distribuidos para analizar grandes cantidades de datos. A medida que aumenta la cantidad de datos, la necesidad de proporcionar soluciones de almacenamiento fiables y eficientes se ha convertido en una de las principales preocupaciones de los administradores de infraestructuras de Big Data. Los sistemas y m\u00e9todos tradicionales de almacenamiento no son \u00f3ptimos debido a sus restricciones de precio o rendimiento, mientras que el DFS se ha desarrollado con el fin de facilitar  compartir archivos tanto en redes locales (LAN) como en redes de \u00e1rea amplia (WAN).</p> <p>Una de las principales caracter\u00edsticas de los DFS es la transparencia que hace que los archivos se lean, se almacenen y se gestionen en las m\u00e1quinas cliente, mientras que el procesamiento real se produce en los servidores, es decir, el DFS implementa sus pol\u00edticas de control de acceso y almacenamiento a sus clientes de forma centralizada proporcionando su servicios a trav\u00e9s de la red.</p> <p>El objetivo principal de la transparencia en el DFS es ocultar a los clientes el hecho de que los procesos y los recursos est\u00e1n distribuidos f\u00edsicamente en la red y proporcionar una visi\u00f3n com\u00fan de un sistema de archivos centralizado. Es decir, el DFS pretende ser invisible/transparente para los clientes, que consideran que el DFS es similar a un sistema de archivos local.</p> <p>Adem\u00e1s de la transparencia, otro de los factores fundamentales en los DFS es la fiabilidad. Dado que el lugar de uso de los archivos puede ser diferente de su lugar de almacenamiento, los modos de fallo son sustancialmente m\u00e1s complejos en los DFS en comparaci\u00f3n con los sistemas de archivos locales. La replica y la codificaci\u00f3n de borrado son dos t\u00e9cnicas t\u00edpicas para lograr una alta fiabilidad.</p> <p>Los DFS utilizan la replica haciendo varias copias de un archivo de datos en diferentes servidores. Cuando un cliente solicita el archivo, accede de forma transparente a una de las copias. Dentro de la estrategia de r\u00e9plica la ubicaci\u00f3n de las r\u00e9plicas juega un papel cr\u00edtico en tanto en el rendimiento como en la fiabilidad de los DFS. Las r\u00e9plicas se almacenan en diferentes servidores seg\u00fan de acuerdo con un esquema de colocaci\u00f3n. Muchos DFS (por ejemplo HDFS) utiliza por defecto la replica aleatoria en la que las r\u00e9plicas se almacenan aleatoriamente en diferentes nodos, en diferentes racks, en diferentes ubicaci\u00f3n geogr\u00e1fica, de modo que si se produce un fallo en cualquier parte del sistema, los datos siguen estando disponibles.</p> <p>Sin embargo, utilizando esta estrategia, no se puede separar eficazmente el cl\u00faster en niveles mientras se mantiene la consistencia del mismo y, por lo tanto, es bastante susceptible a la p\u00e9rdida frecuente de datos debido a fallos. La implementaci\u00f3n de HDFS  limita la colocaci\u00f3n de r\u00e9plicas de bloques a grupos m\u00e1s peque\u00f1os de nodos, lo que reduce la probabilidad de p\u00e9rdida de bloques con m\u00faltiples fallos de nodos. </p> <p>El procedimiento de replica dispersa m\u00faltiples copias de un archivo, y los cambios tienen que propagarse a todas las r\u00e9plicas. Esto hace que la consistencia de los datos sea un aspecto clave en el funcionamiento de los DFS.</p> <p>HDFS (Hadoop File System) es uno de los sistemas de archivos distribuidos m\u00e1s populares en la actualidad. Caracter\u00edsticas</p> <p>Arquitectura: Un cl\u00faster HDFS consiste en un \u00fanico nodo de nombres (namenode), que es el nodo maestro que gestiona el espacio de nombres del sistema de archivos y regula el acceso a los archivos por parte de los clientes. El flujo de datos se dividido en bloques distribuidos entre los datanodes, que gestionan el almacenamiento en los nodos en los que se ejecutan.</p> <p>Acceso: HDFS utiliza una librer\u00eda de c\u00f3digo que permite a los clientes leer, escribir y eliminar archivos y crear y eliminar directorios.</p> <p>Replica: HDFS divide los datos en bloques que se replican a trav\u00e9s de los datanodes de acuerdo con una pol\u00edtica de colocaci\u00f3n, por la cual cada datanode tiene como m\u00e1ximo una copia de un bloque y cada rack tiene como m\u00e1ximo dos copias del bloque.</p> <p>Tolerancia a fallos: Cada datanode compara la versi\u00f3n del software yel ID del espacio de nombres con los del namenode. Si no coinciden, el datanode se apaga para preservar la integridad del sistema.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#37-tolerancia-a-fallos-en-big-data","title":"3.7 Tolerancia a fallos en Big Data","text":"<p>Todos los sistemas de Big Data necesitan manejar los fallos de software y hardware que se producen en el sistema despu\u00e9s de su desarrollo, lo que beneficiar\u00e1 a los sistemas de diferentes maneras, incluyendo: recuperaci\u00f3n de fallos, menor coste, mayor rendimiento, etc. La tolerancia a fallos es una configuraci\u00f3n que evita que un ordenador o dispositivo de red falle en caso de un problema o error inesperado. Para hacer que un computador o una red tolerante a fallos requiere que los usuarios o las empresas piensen en c\u00f3mo puede fallar un ordenador o dispositivo de red y tomen medidas que ayuden a prevenir ese tipo de fallos.</p> <p>El comportamiento de los sistemas Big Data se puede dividir en:</p> <ul> <li> <p>Batch Processing: Procesamiento por lotes de datos en reposo. En este escenario, los datos de origen se cargan en los dispositivos de almacenamiento de datos, ya sea por la propia aplicaci\u00f3n de origen o por un flujo de trabajo de orquestaci\u00f3n (orchestation workflow). A continuaci\u00f3n, los datos se procesan en el lugar por un trabajo paralelizado, que tambi\u00e9n puede ser iniciado por el flujo de trabajo de orquestaci\u00f3n.</p> </li> <li> <p>Stream Processing: Procesamiento continuo de datos en tiempo real, es decir, en cuando existe disponibilidad de datos estos se procesan de manera secuencial. Se establecen unos flujos de datos infinitos y sin l\u00edmites de tiempo.</p> </li> </ul> <p>Dentro de estos dos escenarios se pueden definir diferentes estrategias de tolerancia a fallos como se explica a continuaci\u00f3n.</p>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#371-tolerancia-a-fallos-en-batch","title":"3.7.1 Tolerancia a fallos en batch","text":"<p>En el modelo de c\u00e1lculo por lotes de Hadoop, las aplicaciones de dos principales mecanismos para hacer frente a los fallos son la replica de datos y el mecanismo de rollback (reversi\u00f3n).</p> <ul> <li> <p>Replica de datos (Data Replication): En el mecanismo de replica los datos estar\u00e1n en varios nodos de datos diferentes. Cuando se necesita la replica de datos, cualquier nodo de datos, cuya comunicaci\u00f3n no est\u00e9 ocupada puede copiar los datos. La principal ventaja de esta tecnolog\u00eda es que puede recuperarse instant\u00e1neamente de un fallo. El inconveniente es que se produce un consumo de una alta cantidad de recursos y la posibilidad de que los datos sean inconsistentes.</p> </li> <li> <p>Mecanismo de reversi\u00f3n (Rollback Mechanism): El informe de la copia (copy report) se guardar\u00e1 peri\u00f3dicamente. Si se produce un fallo el sistema se limita a volver a un punto de control (checkpoint) y, a continuaci\u00f3n, vuelve a iniciar la operaci\u00f3n desde ese punto. El m\u00e9todo adopta el concepto de rollback, es decir, el sistema volver\u00e1 al trabajo anterior. Este m\u00e9todo aumenta el tiempo de ejecuci\u00f3n de todo el sistema, porque el rollback necesita hacer una copia de seguridad y comprobar que se guarda un estado consistente. En comparaci\u00f3n con la replica supone m\u00e1s tiempo pero menos recursos.</p> </li> </ul>"},{"location":"UD3%20-%20Introducci%C3%B3n%20a%20la%20computaci%C3%B3n%20paralela%20y%20distribuida/comput_paralela_distribuida.html#372-tolerancia-a-fallos-en-streaming","title":"3.7.2 Tolerancia a fallos en Streaming","text":"<p>En el sistema de streaming, hay tres tipos de estrategia para realizar la gesti\u00f3n de los fallos: la espera pasiva, la espera activa y la copia de seguridad ascendente.</p> <ul> <li>Espera pasiva (Passive Standby): El sistema har\u00e1 regularmente una copia de seguridad del \u00faltimo estado en el nodo maestro a una copia del nodo r\u00e9plica. Cuando se produzca un fallo, el estado del sistema se restaurar\u00e1 a partir de los datos de la copia de seguridad. La estrategia de replicaci\u00f3n pasiva admite el caso de que la carga de datos sea mayor, pero el tiempo de recuperaci\u00f3n se incrementa. Los datos de copia de seguridad pueden guardarse en un sistema de almacenamiento distribuido para reducir el tiempo de recuperaci\u00f3n.</li> </ul> Figura 3.8: Esquema de nodos en streaming. Upstream Nu y downstream Nd. (Fuente: UCLM)  <ul> <li> <p>Espera activa (Active Standby): Cuando el sistema transmite datos para el nodo maestro, tambi\u00e9n transmite una copia de los datos para una replica del nodo al mismo tiempo. Cuando el nodo maestro falla, una de las r\u00e9plicas del nodo asume completamente el trabajo, y los nodos suplentes necesitan la asignaci\u00f3n de los mismos recursos del sistema. De esta manera, el tiempo de recuperaci\u00f3n del fallo es m\u00e1s corto, pero el rendimiento de los datos es menor. Tambi\u00e9n desperdicia m\u00e1s recursos del sistema.</p> </li> <li> <p>Copia de seguridad ascendente (Upstream Backup). Cada nodo maestro almacena su propio estado y los datos de salida en un archivo de registro. Cuando un nodo maestro falla, el nodo maestro anterior en el flujo (upstream) reproducir\u00e1 una copia de los datos en un archivo de registro al nodo correspondiente con el fin de recalcular las operaciones. Esta estrategia necesita m\u00e1s tiempo para reconstruir el estado de la recuperaci\u00f3n, por lo que el tiempo de recuperaci\u00f3n de fallos tiende a ser largo. La estrategia de copia de seguridad ascendente es una mejor opci\u00f3n en unas circunstancias de escasez de recursos.</p> </li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/index.html","title":"UD 4 - Apache Hadoop","text":""},{"location":"UD4%20-%20Apache%20Hadoop/index.html#resultados-de-aprendizaje","title":"Resultados de Aprendizaje","text":"<ul> <li>RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.<ul> <li>RA5075.2b - Se ha comprobado el poder de procesamiento de su modelo de computaci\u00f3n distribuida.</li> <li>RA5075.2e - Se ha visualizado que el sistema puede crecer f\u00e1cilmente a\u00f1adiendo m\u00f3dulos.</li> </ul> </li> <li>RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.<ul> <li>RA5075.3a - Se ha valorado la importancia de la calidad de los datos en los sistemas de ficheros distribuidos.</li> <li>RA5075.3b - Se ha valorado que a mayor volumen de tratamiento de datos corresponde un mayor peligro relacionado con la integridad de los datos.</li> <li>RA5075.3c - Se ha reconocido que los sistemas de ficheros distribuidos implementan una suma de verificaci\u00f3n para la comprobaci\u00f3n de los contenidos de los archivos.</li> </ul> </li> <li>RA5075.4 Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen.<ul> <li>RA5075.4a - Se han aplicado herramientas de monitorizaci\u00f3n eficiente de los recursos.</li> <li>RA5075.4e - Se ha comprobado la fiabilidad de los datos seg\u00fan respuestas.</li> <li>RA5075.4f - Se ha analizado la estabilidad de servicios.</li> </ul> </li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/index.html#contenido-y-criterios-de-evaluacion","title":"Contenido y Criterios de Evaluaci\u00f3n","text":"Contenidos RA5075.2b RA5075.2e RA5075.3a RA5075.3b RA5075.3c RA5075.4a RA5075.4e RA5075.4f Plataforma y Ecosistema X HDFS X X X X X MapReduce X X X X Yarn X X X X Cluster Apache Hadoop X X X X X X X X"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html","title":"UD 4 - Apache Hadoop - Plataforma y Ecosistema","text":"<p>Apache Hadoop es un framework de software que soporta aplicaciones distribuidas bajo una licencia libre. Permite a las aplicaciones trabajar con miles de nodos y petabytes de datos.</p> <p>Fue inicialmente concebido para resolver un problema de escalabilidad en Nutch, un motor de b\u00fasqueda Open Source que pretend\u00eda indexar mil millones de p\u00e1ginas web.</p> Figura 1 Hadoop: Componentes b\u00e1sicos de Hadoop. (Fuente: Ministerio de Educaci\u00f3n) <p>Al mismo tiempo, Google hab\u00eda publicado los documentos que describ\u00edan su novedoso sistema de archivos distribuidos, el Google File System (GFS), y MapReduce, un framework de computaci\u00f3n para procesamiento paralelo. La exitosa implementaci\u00f3n de estos conceptos en Nutch result\u00f3 en dos proyectos separados, el segundo se convirti\u00f3 en Hadoop, un proyecto Apache de primera clase.</p> <p>Hadoop</p> <p>El nombre de Hadoop no es ning\u00fan acr\u00f3nimo, sino un nombre inventado. Su creador se lo puso por un elefante amarillo de peluche que ten\u00eda su hijo. Pens\u00f3 que un nombre corto, relativamente f\u00e1cil de deletrear y pronunciar ser\u00eda adecuado</p> <p>Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n.</p> <p>\u00bfQu\u00e9 es Hadoop?</p> <p>Apache Hadoop es una plataforma opensource que ofrece la capacidad de almacenar y procesar, a \"bajo\" coste, grandes vol\u00famenes de datos, sin importar su estructura, en un entorno distribuido, escalable y tolerante a fallos, basado en la utilizaci\u00f3n de hardware commodity y en un paradigma del procesamiento a los datos. </p> <p>Hadoop es una plataforma, lo que significa que es la base sobre la que construir aplicaciones. Se podr\u00eda hacer el s\u00edmil de Hadoop como una caja de herramientas que proporciona un conjunto de herramientas con las que construir una gran variedad de aplicaciones que requieran almacenar y procesar grandes vol\u00famenes de datos. La selecci\u00f3n de qu\u00e9 herramienta utilizar para cada aplicaci\u00f3n la realizaremos en funci\u00f3n de las necesidades de cada caso de uso.</p> <p>Otras soluciones, como MongoDB u otras bases de datos NoSQL no se consideran plataformas, ya que tienen un \u00fanico prop\u00f3sito y ofrecen un tipo de funcionalidad.</p> <p>Hadoop aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos.</p> <p>Sus caracter\u00edsticas son:</p> <ul> <li>Confiable: Crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento.</li> <li>Tolerante a fallos: Tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n.</li> <li>Heterogeneo: Los datos que pueden almacenarse y procesarse en Hadoop pueden ser de cualquier tipo: estructurados, semiestructurados o datos no estructurados.</li> </ul> Figura 2 Hadoop: Datos heterog\u00e9neos. (Fuente: Ministerio de Educaci\u00f3n) <ul> <li>Portable: Se puede instalar en todo tipos de hardware y sistemas operativos.</li> <li>Escalable: Los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local.</li> </ul> Figura 3 Hadoop: Escalable. (Fuente: Ministerio de Educaci\u00f3n) <ul> <li>Distribuido: Hadoop se basa en una infraestructura que tiene muchos servidores (tambi\u00e9n llamados nodos) que trabajan conjuntamente para almacenar y para procesar los datos, a diferencia de los sistemas centralizados, donde todo se realiza en un \u00fanico servidor.</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#1-componentes-y-ecosistema","title":"1. Componentes y Ecosistema","text":"Figura 4 Hadoop: Ecosistema Hadoop. (Fuente: Ministerio de Educaci\u00f3n) <p>El n\u00facleo de Hadoop se compone de:</p> <ul> <li>Hadoop Common: un conjunto de utilidades comunes</li> <li>HDFS (Hadoop Distributed File System): un sistema de ficheros distribuidos (capa de almacenamiento) que almacena los datos en una estructura basada en espacios de nombres (directorios, subdirectorios, etc)</li> <li>YARN: un gestor de recursos (capa de procesamiento) para el manejo del cl\u00faster y la planificaci\u00f3n de procesos, que permite ejecutar aplicaciones sobre los datos almacenados en HDFS</li> <li>MapReduce: un sistema para procesamiento paralelo de grandes conjuntos de datos, con aplicaciones que lo utilizan de forma transparente.</li> </ul> <p>Note</p> <p>Sin embargo, normalmente se identifica el nombre Hadoop con todo el ecosistema de componentes independientes que suelen incluirse para dotar a Hadoop de funcionalidades necesarias en proyectos Big Data empresariales, como puede ser la ingesta de informaci\u00f3n, el acceso a datos con lenguajes est\u00e1ndar, o las capacidades de administraci\u00f3n y monitorizaci\u00f3n. Estos componentes suelen ser proyectos opensource de Apache.. </p> <p>Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero local en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores.</p> <p>Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida.</p> <p>Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop.</p> <ul> <li>Accumulo: Base de datos NoSQL que ofrece funcionalidades de acceso aleatorio y at\u00f3mico.</li> <li>Ambari: Herramienta utilizada para instalar, configurar, mantener y monitorizar Hadoop.</li> <li>Atlas: Herramienta de gobierno de datos de Hadoop.</li> <li>Phoenix: Capa que permite acceder a los datos de HBase mediante interfaz SQL.</li> <li>Flume: Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n procedentes de sistemas real-time en Hadoop. Es \u00fatil para cargar y mover informaci\u00f3n como ficheros de logs, datos de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables</li> <li>HBase: Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop.<ul> <li>Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data.</li> <li>Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS.</li> <li>El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas y columnas, sobre un cl\u00faster Hadoop.</li> </ul> </li> <li>Hive: Permite acceder a ficheros de datos estructurados o semiestructurados que est\u00e1n en HDFS como si fueran una tabla de una base de datos relacional, utilizando un lenguaje similar a SQL (HiveSQL). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop.</li> <li>Impala: Herramienta con funcionalidad similar a Hive (tratamiento de los datos de HDFS mediante SQL) pero con un rendimiento elevado (tiempos de respuesta menores).</li> <li>Kafka: Sistema de mensajer\u00eda que permite recoger eventos en tiempo real as\u00ed como su procesamiento.</li> <li>Mahout: Conjunto de librer\u00edas para desarrollo y ejecuci\u00f3n de modelos de machine learning utilizando las capacidades de computaci\u00f3n de Hadoop.</li> <li>Oozie: Herramienta que permite definir flujos de trabajo en Hadoop as\u00ed como su orquestaci\u00f3n y planificaci\u00f3n.</li> <li>Pig: Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin.</li> <li>Spark: Aunque habitualmente no se asocia al ecosistema Hadoop, Apache Spark ha sido el mejor complemento de Hadoop en los \u00faltimos a\u00f1os. Apache Spark es un motor de procesamiento masivo de datos muy eficiente a gran escala que ofrece funcionalidades para ingenier\u00eda de datos, machine learning, grafos, etc. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop.</li> <li>Sqoop: Componente para importar o exportar datos estructurados desde bases de datos relacionales a Hadoop y viceversa.</li> <li>Storm: Sistema de procesamiento real-time de eventos con baja latencia.</li> <li>Zeppelin: Aplicaci\u00f3n web de notebooks que permite a los Data Scientists realizar an\u00e1lisis y evaluar c\u00f3digo de forma sencilla, as\u00ed como la colaboraci\u00f3n entre equipos.</li> <li>ZooKeeper: Herramienta t\u00e9cnica que permite sincronizar el estado de los diferentes servicios distribuidos de Hadoop.</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#2-distribuciones-hadoop","title":"2. Distribuciones Hadoop","text":"<p>No te preocupes si ves muchos componentes y piensas que es imposible dominar todos. En la realidad, los proyectos suelen utilizar s\u00f3lo una peque\u00f1a parte de los componentes dependiendo de las necesidades. En negrita encuentras los m\u00e1s utilizados, adem\u00e1s de los componentes core: HDFS y YARN.</p> <p>Cada componente es un proyecto Apache independiente, lo que impacta, entre otros a:</p> <ul> <li>Pol\u00edtica de versionado (periodicidad, identificaci\u00f3n, \u2026): cada componente sigue su propio camino en cuanto a cu\u00e1ndo se publican las nuevas versiones, qu\u00e9 mejoras o evoluciones incluyen, etc.</li> <li>Dependencias del proyecto con otras versiones de componentes del ecosistema y librer\u00edas externas: los componentes suelen tener dependencias entre ellos. Por ejemplo, Hive tiene dependencia de HDFS, o Phoenix de HBase. Las dependencias suelen ser dif\u00edciles de gestionar, por ejemplo, porque una versi\u00f3n de Phoenix requiere una versi\u00f3n espec\u00edfica de HBase.</li> <li>Roadmap y estrategia del proyecto: al tener grupos de trabajo diferentes, cada proyecto tiene su propia estrategia en cuanto a c\u00f3mo evolucionar la soluci\u00f3n, cu\u00e1ndo adaptarse a cambios externos, etc. y no siempre est\u00e1n alineados.</li> <li>Commiters / desarrolladores: los desarrolladores de cada proyecto son diferentes.</li> </ul> <p>Por este motivo, realizar una instalaci\u00f3n de toda una plataforma Hadoop con sus componentes asociados de forma independiente (lo que se denomina Hadoop Vanila) resulta muy complicado. Por ejemplo, al instalar la versi\u00f3n X de Phoenix necesitas la versi\u00f3n Y de HBase, pero otro componente (Hive, por ejemplo), requiere la versi\u00f3n Z de HBase.</p> <p>La misma dificultad ocurre para la resoluci\u00f3n de incidencias que puedan ocurrir en la plataforma cuando se ejecuta en producci\u00f3n.</p> <p>Para solventar las dificultades mencionadas, surgen las distribuciones comerciales de Hadoop, que contienen en un \u00fanico paquete la mayor parte de componentes del ecosistema, resolviendo dependencias, a\u00f1adiendo incluso utilidades, e incorporando la posibilidad de contratar soporte empresarial 24x7</p>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#21-distribuciones-cloudera","title":"2.1 Distribuciones: Cloudera","text":"<p>Note</p> <p>Cloudera es la principal distribuci\u00f3n que existe actualmente (hubo otras, como MAPR y HortonWorks, pero desaparece MAPR y Hortonworks se une a Cloudera)</p> <p>Utiliza la mayor parte de componentes de Apache, en alg\u00fan caso realizando algunas modificaciones, y a\u00f1ade alg\u00fan componente propietario (Cloudera Manager, Cloudera Navigator, etc.). CDN Cloudera y como descargarlo</p> Figura 5 Hadoop: Distribuci\u00f3n Cloudera. (Fuente: Cloudera) <p>CDH Cloudera</p> <p>CDH (Cloudera\u2019s Distribution including Apache Hadoop) es la distribuci\u00f3n de Cloudera con Apache Hadoop orientada a empresas. La \u00faltima versi\u00f3n es Cloudera 6 (CDH 6). Est\u00e1 disponible como paquetes RPM y paquetes para Debian, Ubuntu o Suse. Cloudera proporciona CDH en varias modalidades.</p> <p>La versi\u00f3n m\u00e1s completa y empresarial es Cloudera Enterprise, que incluye suscripciones por cada nodo del cl\u00faster, Cloudera Manager y el soporte t\u00e9cnico. Por otro lado, Cloudera Express es una versi\u00f3n m\u00e1s sencilla, sin actualizaciones o herramientas de disaster recovery. Por \u00faltimo, existe una versi\u00f3n gratuita de CDH: Cloudera Community. Permite desplegar un cl\u00faster con un n\u00famero de nodos limitado.</p> <p>Es posible ejecutar Cloudera desde un contenedor Docker. Proporciona una imagen Docker con CDH y Cloudera Manager que sirve como entorno para aprender Hadoop y su ecosistema de una forma sencilla y sin necesidad de Hardware potente. Tambi\u00e9n es \u00fatil para desarrollar aplicaciones o probar sus funcionalidades.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#22-soluciones-cloud-computing","title":"2.2 Soluciones Cloud Computing","text":"<ul> <li>Amazon Elastic Map Reduce (EMR)<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Amazon EMR?</li> </ul> </li> <li>Microsoft Azure HDInsight<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Azure HDInsight?</li> </ul> </li> <li>Google Dataproc<ul> <li>Sitio Oficial</li> <li>\u00bfQu\u00e9 es Dataproc?</li> </ul> </li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#3-arquitectura","title":"3. Arquitectura","text":"<p>Hadoop se basa en un modelo de despliegue distribuido y est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos (data local computing). </p> <p>Pese a que hay un conjunto de servidores trabajando en paralelo y de forma conjunta, para un usuario externo todos ellos act\u00faan como si fuera una sola m\u00e1quina, es decir, un usuario del sistema de ficheros (HDFS) ver\u00e1 la estructura de directorios, subdirectorios y ficheros, pero no tendr\u00e1 que conocer en qu\u00e9 servidores est\u00e1 cada fichero (lo mismo ocurre con cualquier otro componente que se ejecuta en toda la infraestructura)</p> <p>Su filosof\u00eda es almacenar todos los datos en un lugar y procesarlos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento.</p> <p>Note</p> <p>Cluster y Nodo: Al conjunto de servidores que trabajan para implementar las funcionalidades de Apache Hadoop se le denomina cl\u00faster, y a cada uno de los servidores que forman parte del cl\u00faster se le denomina nodo. A partir de ahora, cuando usemos la palabra \"cl\u00faster de Hadoop\" debes pensar en el conjunto de servidores que forman la plataforma que est\u00e1 en ejecuci\u00f3n, y cuando usemos la palabra \"nodo\" debes pensar en cada uno de los servidores que componen el cl\u00faster.</p> <p>Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre:</p> <ul> <li>Nodos worker: Realizan los trabajos. Tratan con los datos locales y los procesos de aplicaci\u00f3n. Por ejemplo, para el almacenamiento, cada worker se ocupar\u00e1 de almacenar una parte, mientras que para la ejecuci\u00f3n de trabajos, cada worker realiza una parte del trabajo. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato (commodity hardware) mediante servidores X86.</li> <li>Nodos master: Encargados de los procesos de gesti\u00f3n global, es decir, controlar la ejecuci\u00f3n o el almacenamiento de los trabajos y/o datos. Son los nodos que controlan el trabajo que realizan los nodos worker, por ejemplo, asignando a cada worker una parte del proceso o de los datos a almacenar, vigilando que est\u00e1n realizando el trabajo y no est\u00e1n ca\u00eddos, rebalanceando el trabajo a otros nodos en caso de que un worker tenga problemas, etc. Normalmente se necesitan 3.</li> <li>Nodos edge: Hacen de puente entre el cl\u00faster y la red exterior y proporcionan interfaces, ya que normalmente un cl\u00faster Hadoop no tiene conexi\u00f3n con el resto de servidores e infraestructura de la empresa, por lo que toda la comunicaci\u00f3n desde el exterior hacia el cl\u00faster se canaliza a trav\u00e9s de los nodos frontera, que adem\u00e1s, ofrecen las APIs para poder invocar a servicios del cl\u00faster.</li> </ul> Figura 6 Hadoop: Arquitectura Hadoop. (Fuente: Ministerio de Educaci\u00f3n) <p>El hardware t\u00edpico donde se ejecuta un cluster Hadoop ser\u00eda:</p> <p>Note</p> <p>Commodity Hardware: A veces el concepto hardware commodity suele confundirse con hardware dom\u00e9stico, cuando lo que hace referencia es a hardware no espec\u00edfico, que no tiene unos requerimientos en cuanto a disponibilidad o resiliencia exigentes</p> <ul> <li>Nodos master: 12 HDs x 2-3 TB JBOD (Just a bunch of disks = s\u00f3lo un mont\u00f3n de discos) - 2CPUs x 8 cores - 256 GB RAM</li> <li>Nodos worker: 2 HDs x 2-3 TB RAID - 2CPUs x 8 cores - 256 GB RAM</li> <li>Nodos edge: 2 HDs x 2-3 TB RAID - 2CPUs x 8 cores - 256 GB RAM</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#4-uso-de-hadoop","title":"4. Uso de Hadoop","text":"<p>Es importante analizar y tener en cuenta en que casos reales es aconsejable el uso de Hadoop y cuando no lo es.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#cuando-usar-hadoop","title":"\u00bfCu\u00e1ndo Usar Hadoop?","text":"<ul> <li>Cuando el volumen de datos es mayor que la capacidad de los sistemas tradicionales (no cabe en una m\u00e1quina).</li> <li>Cuando hay un problema de variedad de datos, porque son diversos o porque cambian frecuentemente.</li> <li>Cuando se requiere una escalabilidad que no pueden ofrecer los sistemas tradicionales, por volumen, por velocidad de proceso, por rendimiento global, y no se requiere un nivel de transaccionalidad elevado.</li> <li>Cuando se pretende tener una plataforma con la capacidad de almacenamiento y procesamiento de un gran volumen de datos para cubrir diferentes casos de uso (con la misma plataforma).</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#cuando-no-usar-hadoop","title":"\u00bfCu\u00e1ndo no usar Hadoop?","text":"<ul> <li>Cuando los sistemas tradicionales son capaces de dar soporte a los casos de uso y cuando los formatos/tipos de datos son fijos o no cambian apenas.</li> <li>Cuando se tiene requisitos de transaccionalidad muy estrictos, es decir, cuando se pretende cubrir la operativa de una empresa (por ejemplo, en un banco: las transferencias, movimientos, pagos, etc.).</li> <li>Cuando s\u00f3lo se requiere resolver un caso de uso \"Big Data\" muy espec\u00edfico.</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#5-instalacion-hadoop-core","title":"5. Instalaci\u00f3n Hadoop Core","text":"<p>Para trabajar en esta y las siguientes sesiones, vamos a utilizar una m\u00e1quina virtual. A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario hadoop y la contrase\u00f1a hadoop.</p> <p>Tambi\u00e9n puedes instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Ubuntu Mate 22.04 LTS y la versi\u00f3n 3.3.4 de Hadoop. Puedes seguir las instrucciones de esta secci\u00f3n.</p> <p>Ay\u00fadate tambi\u00e9n de la informaci\u00f3n de la p\u00e1gina oficial</p> <p>Note</p> <p>Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida, porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#51-instalacion","title":"5.1 Instalaci\u00f3n","text":"<ol> <li>Java\u2122 debe ser instalado. Las versiones de Java recomendadas se encuentran descritas en HadoopJavaVersions.</li> </ol> <pre><code>sudo apt-get install openjdk-11-jdk\n/usr/bin/java\n</code></pre> <ol> <li>ssh debe estar instalado y sshd debe estar ejecut\u00e1ndose para usar las secuencias de comandos de Hadoop que administran los demonios ssh remotos de Hadoop, ya que vamos a usar las secuencias de comandos de inicio y detecci\u00f3n opcionales.</li> </ol> <pre><code>sudo apt-get install ssh\n</code></pre> <ol> <li> <p>Abre la terminal en el directorio <code>$HOME</code></p> </li> <li> <p>Para obtener la distribuci\u00f3n de Apache Haddop, descarga la versi\u00f3n estable m\u00e1s reciente desde Apache Download Mirrors</p> </li> </ol> <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n</code></pre> <ol> <li>Una vez descargado, desempaquetamos el archivo descargado con el comando tar y entra dentro de la carpeta:</li> </ol> <pre><code>tar -xzf hadoop-3.3.4.tar.gz\ncd hadoop-3.3.4\n</code></pre> <ol> <li>Edita el siguiente archivo <code>etc/hadoop/hadoop-env.sh</code> para definir la variable de entorno de Java y a\u00f1adela.</li> </ol> <pre><code># Technically, the only required environment variable is JAVA_HOME.\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n</code></pre> <ol> <li>Para poder usar los comandos de HDFS en cualquier lugar del sistema, sin tener que hacerlo desde el directorio de Hadoop (por ejemplo /home/hadoop/hadoop-3.3.4/bin), creamos las variables de entorno y a\u00f1adimos al PATH. Para ello abrimos el archivo <code>~/.bashrc</code> y a\u00f1adimos al final el siguiente c\u00f3digo y ejecuta el comando <code>source ~/.bashrc</code></li> </ol> ~/.bashrc<pre><code>export HADOOP_HOME=$HOME/hadoop-3.3.4\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li>Ejecuta el siguiente comando. Si no da error, podemos continuar</li> </ol> <pre><code>bin/hadoop\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/1_Plataforma_Hadoop.html#52-configuracion-pseudo-distributed-operation","title":"5.2 Configuraci\u00f3n (Pseudo-Distributed Operation)","text":"<p>Hadoop se puede ejecutar en un solo nodo en un modo pseudo-distributed donde cada demonio de Hadoop se ejecuta en un proceso Java separado.</p> <p>Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> <ol> <li>El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo <code>core-site.xml</code>. En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos (namenode) de la arquitectura.</li> </ol> core-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://bda-iesgrancapitan:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html","title":"UD 4 - Apache Hadoop - HDFS","text":"<p>Hadoop Distributed File System (HDFS) Es el componente principal del ecosistema Hadoop. Hace posible almacenar conjuntos de datos masivos con tipos de datos estructurados, semi-estructurados y no estructurados como im\u00e1genes, v\u00eddeo, datos de sensores, etc.</p> <p>Es un sistema de almacenamiento distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google detallando su Google File System en 2003.</p> <p>Est\u00e1 optimizado para obtener un alto rendimiento y trabajar con m\u00e1xima eficiencia cuando se leen archivos grandes. Para obtener este rendimiento, utiliza tama\u00f1os de bloque inusualmente grandes y optimizaci\u00f3n de localizaci\u00f3n de los datos para reducir la E/S de red.</p> <p>Con el fin de ofrecer una visi\u00f3n de los recursos como una sola unidad crea una capa de abstracci\u00f3n como un sistema de ficheros \u00fanico. Est\u00e1 basado en la idea de que mover el procesamiento es mucho m\u00e1s r\u00e1pido, f\u00e1cil y eficiente que mover grandes cantidades de datos, que pueden producir altas latencias y congesti\u00f3n en la red.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#1-caracteristicas-hdfs","title":"1. Caracter\u00edsticas HDFS","text":"<ul> <li>Es un sistema de ficheros distribuido, es decir, se ejecuta sobre diferentes nodos que trabajan en conjunto ofreciendo a los usuarios y aplicaciones que utilizan el sistema, un interfaz como si s\u00f3lo hubiera un \u00fanico servidor por detr\u00e1s.</li> <li>Est\u00e1 dise\u00f1ado para ejecutarse sobre hardware commodity, es decir, no requiere unos servidores espec\u00edficos o costosos. Esto conlleva la necesidad de poder sobreponerse a los fallos que pudieran tener los servidores o algunas partes de los servidores.</li> <li>Est\u00e1 optimizado para almacenar ficheros de gran tama\u00f1o y para hacer operaciones de lectura o escritura masivas. Su objetivo es cubrir los casos de uso de anal\u00edtica masiva, no los casos de uso que dan soporte a las operaciones de las empresas.</li> <li>Tiene capacidad para escalar horizontalmente hasta vol\u00famenes de Petabytes y miles de nodos, y est\u00e1 dise\u00f1ado para poder dar soporte a m\u00faltiples clientes con acceso concurrente. La escalabilidad se consigue a\u00f1adiendo m\u00e1s servidores</li> <li>No establece ninguna restricci\u00f3n sobre los tipos de datos que se almacenan en el sistema, ya que \u00e9stos pueden ser estructurados, semiestructurados o no disponer de ninguna estructura, como el caso de im\u00e1genes o v\u00eddeos.</li> <li>HDFS tiene una orientaci\u00f3n \"write-once, read many\", que significa \"se escribe una vez, se lee muchas veces\", es decir, asume que un archivo una vez escrito en HDFS no se modificar\u00e1, aunque se puede acceder a \u00e9l muchas veces. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido (append-only). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos.</li> </ul> <p>Recuerda las caracter\u00edsticas con esta imagen</p> Figura 1 HDFS: Caracter\u00edsticas HDFS. (Fuente: Ministerio de Educaci\u00f3n)"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#2-bloques","title":"2. Bloques","text":"<p>Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. En HDFS, los ficheros se dividen en bloques, como en la mayor\u00eda de sistemas de ficheros. Sin embargo, el tama\u00f1o de un bloque en HDFS es muy grande, de 128 megabytes por defecto. En el sistema operativo de un PC (Windows, Linux, etc.), el tama\u00f1o suele ser de 512 bytes o 4 kilobytes, es decir, unas 50.000 veces m\u00e1s peque\u00f1o que en HDFS.</p> <p>El bloque es la unidad m\u00ednima de lectura, lo que significa que aunque tengamos un fichero que ocupa 1 kilobyte, tendremos que leer o escribir 128 megabytes cada vez que queramos operar con el fichero. Para ficheros grandes, por ejemplo, de 500 gigabytes, la ventaja que aporta es que hay que buscar y leer o escribir muchos menos bloques. Esta caracter\u00edstica explica por qu\u00e9 Hadoop est\u00e1 dise\u00f1ado para ficheros grandes y lecturas masivas, y por qu\u00e9 tiene un mal rendimiento para operaciones peque\u00f1as.</p> <p>Por lo tanto, cuando queremos escribir un fichero en HDFS, lo primero que se hace es dividir el fichero en bloques. A continuaci\u00f3n, los bloques son almacenados en diferentes nodos, no siendo necesario que los bloques de un mismo fichero est\u00e9n en un mismo nodo. Adem\u00e1s, un aspecto importante es que cada bloque se replica (se copia) en m\u00e1s de un nodo, lo que se conoce como el factor de replica. El factor de replica por defecto en HDFS es 3, lo que significa que cada bloque tiene 3 copias almacenadas en 3 nodos diferentes. La replicaci\u00f3n es el mecanismo con el que se consigue, entre otras cosas, la tolerancia a fallos.</p> <p>Al tener varias r\u00e9plicas de cada bloque en diferentes nodos, en caso de que un nodo se caiga, o que un disco de un nodo se corrompa, HDFS dispondr\u00e1 de otras copias, por lo que no se perder\u00e1n los datos. </p> Figura 2 HDFS: Bloques HDFS. (Fuente: Ministerio de Educaci\u00f3n) <p>En el ejemplo anterior, si se cayera el nodo 3, HDFS dispondr\u00eda de otras dos copias por cada bloque que almacena del fichero.</p> Figura 3 HDFS: Factor Replicaci\u00f3n HDFS. (Fuente: Ministerio de Educaci\u00f3n) <p>El factor de replica puede configurarse a nivel de fichero o directorio, es decir, podemos elegir un factor de replica diferente para los ficheros o directorios que consideremos. Cuanto mayor sea el factor de replica, m\u00e1s dif\u00edcil ser\u00e1 que perdamos los datos e incluso mejorar\u00e1 el rendimiento en las lecturas, porque para leer un bloque, HDFS podr\u00e1 utilizar cualquier nodo. Sin embargo, un factor de replica alto hace que las escrituras tengan peor rendimiento, al tener que hacer muchas copias en cada escritura, y adem\u00e1s, consumir\u00e1 m\u00e1s espacio real en disco.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#3-arquitectura-hdfs","title":"3. Arquitectura HDFS","text":"<p>La arquitectura de HDFS consta de distintos servicios y tipos de nodo, aunque fundamentalmente son tres tipos:</p> <ul> <li>NameNode(NN): Nodo de Nombres.</li> <li>Secondary NameNode(SNN): Nodo de Nombres Secundario.</li> <li>DataNode(DN): Nodos de Datos</li> </ul> <p>NameNode</p> <p>El nodo Namenode act\u00faa de maestro, manteniendo la metainformaci\u00f3n de todo el sistema de ficheros, esto es:</p> <ul> <li>Almacena el espacio de nombres HDFS</li> <li>La estructura de directorios, subdirectorios y los ficheros</li> <li>La informaci\u00f3n de los ficheros: tama\u00f1o, fecha de modificaci\u00f3n, propietario, permisos, etc.</li> <li>El factor de replica de cada fichero.</li> <li>Los bloques que componen cada fichero.</li> <li>La ubicaci\u00f3n de los distintos bloques (en qu\u00e9 nodo se encuentran).</li> </ul> <p>Note</p> <p>La informaci\u00f3n es almacenada tanto en disco, para garantizar la durabilidad en caso de una ca\u00edda del servidor, como en memoria, para poder acceder a la informaci\u00f3n lo m\u00e1s r\u00e1pido posible y optimizar el rendimiento.</p> <p>NameNode se compone principalmente de dos ficheros:</p> <ul> <li>FsImage: Contiene la estructura de directorio completa (espacio de nombres) de HDFS con detalles sobre la ubicaci\u00f3n de los datos en los bloques de datos y qu\u00e9 bloques est\u00e1n almacenados en qu\u00e9 nodo. NameNode utiliza este archivo cuando se inicia.</li> <li>EditLog: Es un registro de transacciones que registra los cambios en el sistema de archivos HDFS o cualquier acci\u00f3n realizada en el cl\u00faster HDFS, como la adici\u00f3n de un nuevo bloque, la replicaci\u00f3n, la eliminaci\u00f3n, etc. En resumen, registra los cambios desde que se cre\u00f3 la \u00faltima FsImage</li> </ul> <p>Cuando se inicia un NameNode, lee el estado HDFS de un archivo de imagen, fsimage, y luego aplica las ediciones del archivo de registro de ediciones. Seguidamente escribe un nuevo estado HDFS en el fsImage y comienza la operaci\u00f3n normal con un archivo de edici\u00f3n vac\u00edo.</p> <p>Example</p> <p>Por ejemplo, la creaci\u00f3n de un nuevo archivo en HDFS hace que NameNode inserte un registro en EditLog para indicarlo. De manera similar, cambiar el factor de replica de un archivo hace que se inserte un nuevo registro en EditLog. El NameNode utiliza un archivo en su sistema de archivos del sistema operativo anfitri\u00f3n local para almacenar el EditLog. Todo el espacio de nombres del sistema de archivos, incluida la asignaci\u00f3n de bloques a archivos y las propiedades del sistema de archivos, se almacena en un archivo denominado FsImage. La FsImage tambi\u00e9n se almacena como un archivo en el sistema de archivos local de NameNode.</p> <p>Adem\u00e1s de gestionar la metainformaci\u00f3n, coordina todas las lecturas y escrituras, y controla el funcionamiento de los Datanodes, es decir, detecta si hay alg\u00fan fallo en alg\u00fan nodo y toma las acciones necesarias en caso de que alguno est\u00e9 ca\u00eddo o con fallos.</p> <p>Es importante que el Namenode sea robusto y no tenga ca\u00eddas. Por este motivo, se utiliza hardware m\u00e1s resiliente que en el caso de los Datanodes.</p> <p>Secondary NameNode</p> <p>Para mejorar la tolerancia a fallos, suele existir un nodo secundario del maestro, denominado Secondary Namenode.</p> <p>El NameNode es el \u00fanico punto de fallo en HDFS ya que, si el Namenode falla, se pierde todo el sistema de archivos HDFS. Para reducir este riesgo esto, Hadoop implement\u00f3 el Secondary Namenode en la versi\u00f3n 3.</p> <p>Secondary Namenode no es un nodo de respaldo. Su principal funci\u00f3n es almacenar una copia de los ficheros fsimage y editlog. Comprueba los metadatos del sistema de archivos almacenados en NameNode. Esto es lo que se llama checkpointing. El proceso que sigue el NameNode secundario para fusionar peri\u00f3dicamente los archivos fsimage y edits log es el siguiente:</p> <ol> <li>El NameNode secundario obtiene los \u00faltimos archivos fsImage y editLog del NameNode primario.</li> <li>El NameNode secundario aplica cada transacci\u00f3n del archivo editLog a fsImage para crear un nuevo archivo FsImage fusionado.</li> <li>El archivo fsImage fusionado se transfiere de nuevo al NameNode primario.</li> </ol> Figura 4 HDFS: Relaci\u00f3n entre NameNode y DataNode. (Fuente: Telef\u00f3nica Tech) <p>DataNode</p> <p>Los Datanodes son los servicios que se encuentran en los nodos worker, y su labor principal es almacenar o leer los bloques que componen los ficheros que est\u00e1n almacenados en HDFS, con las siguientes particularidades:</p> <ul> <li>Habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes.</li> <li>Almacena y lee bloques de datos. El Datanode s\u00f3lo conoce los bloques que contiene, pero no sabe a qu\u00e9 fichero pertenecen o d\u00f3nde se encuentran el resto de bloques del fichero. Toda esta informaci\u00f3n s\u00f3lo est\u00e1 en el Namenode. Por eso es cr\u00edtico para HDFS.</li> <li>Env\u00edan al Namenode la lista de los bloques que almacenan, para que el Namenode pueda tener una lista actualizada de los bloques y su ubicaci\u00f3n.</li> <li>Almacenan un checksum por cada bloque para detectar si el bloque est\u00e1 corrupto y garantizar su integridad.</li> <li>Env\u00eda un latido (heartbeat) al Namenode, que es un mensaje corto indicando que est\u00e1 levantado</li> </ul> <p>Resumen</p> Figura 5 HDFS: Resumen Nodos HDFS. (Fuente: Ministerio de Educaci\u00f3n)"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#4-funcionamiento-lectura-y-escritura","title":"4. Funcionamiento (Lectura y Escritura)","text":"<p>Los datos que se escriben en HDFS son immutables, es decir, no pueden ser modificados.</p> <p>Esto significa que HDFS s\u00f3lo permite a\u00f1adir contenido a los ficheros, as\u00ed que por ejemplo, si en un fichero de 256 megabytes se pretende modificar un car\u00e1cter, HDFS crear\u00e1 un nuevo bloque con el cambio y lo escribir\u00e1 por completo, borrando el bloque anterior. </p> <p>Esto, junto con la caracter\u00edstica del tama\u00f1o de bloque de 128 megabytes, que es la unidad m\u00ednima de lectura, hace que el rendimiento de HDFS para operaciones sencillas sobre registros aleatorios sea muy pobre. Recuerda que HDFS est\u00e1 pensado para ficheros grandes y lecturas masivas. </p> <p>HDFS proporciona dos tipos de operaciones b\u00e1sicas con los ficheros: leer y escribir un fichero</p> <p>Lectura</p> <p>En el caso de las lecturas, un esquema simplificado de esta operaci\u00f3n ser\u00eda:</p> Figura 6 HDFS: Lectura HDFS. (Fuente: Ministerio de Educaci\u00f3n) <ol> <li>El cliente que desea leer un fichero de HDFS, mediante una librer\u00eda instalada en su equipo, realiza una llamada al Namenode para conocer qu\u00e9 bloques forman un fichero (llamemos X al fichero), as\u00ed como los Datanodes que contienen cada uno de los bloques.</li> <li>El Namenode retorna dicha informaci\u00f3n, y ordena para cada bloque los Datanodes que contienen dicho bloque en funci\u00f3n de la distancia al cliente (un algoritmo eval\u00faa la distancia entre el cliente y cada Datanode). El objetivo de esta lista ordenada es intentar reducir el tiempo de acceso a cada Datanode desde el cliente.</li> <li>Con la informaci\u00f3n recibida del Namenode, el cliente se comunica directamente con el Datanode 1 para solicitarle el primer bloque.</li> <li>El cliente se comunica con el Datanode 2 para obtener el bloque 2.</li> <li>El cliente se comunica con el Datanode 1 para obtener el bloque 3. </li> </ol> <p>Info</p> <p>Es preciso indicar que durante la operaci\u00f3n, la \u00fanica responsabilidad del Namenode es devolver al cliente la lista de bloques y la ubicaci\u00f3n de los mismos, pero no interviene en las lecturas. Es decir, para realizar las lecturas de cada bloque, el cliente se comunica directamente con los Datanodes, sin que los datos pasen por el Namenode. Esto hace que el Namenode no sea cuello de botella del proceso, y pueda atender m\u00faltiples peticiones en paralelo, ya que no le supone mucho esfuerzo de computaci\u00f3n atender las diferentes solicitudes de los clientes.</p> <p>Escritura</p> <p>En el caso de las escrituras, un esquema simplificado de esta operaci\u00f3n ser\u00eda:</p> Figura 7 HDFS: Escritura HDFS. (Fuente: Ministerio de Educaci\u00f3n) <ol> <li>El cliente, que desea escribir un fichero, invoca a un servicio del Namenode para solicitar la creaci\u00f3n del fichero, indic\u00e1ndole en la llamada el nombre y la ruta en la que desea guardarlo.</li> <li>El Namenode realiza una serie de verificaciones, como los permisos del usuario/cliente en el directorio, si el fichero ya existe, etc. En caso de que todas las verificaciones sean correctas, devuelve un OK, en caso contrario un KO.</li> <li>El cliente comienza a generar los bloques en los que se dividir\u00e1 el fichero utilizando una librer\u00eda de HDFS.</li> <li>Para cada bloque que desea escribir el cliente, se invoca al Namenode para obtener el Datanode en el que escribir el bloque.</li> <li>El Namenode devuelve la lista de Datanodes en los que escribir el bloque, y el cliente escribe dicho bloque en el primer Datanode obtenido, realizando una comunicaci\u00f3n directamente con dicho Datanode.</li> <li>Una vez escrito el bloque en el primer Datanode, \u00e9ste es responsable de comunicarse con el siguiente Datanode en la cadena para que escriba una copia del bloque. Una vez todos los Datanodes han escrito la r\u00e9plica, se devuelve un \"Ok\" al cliente para que escriba el siguiente bloque.</li> </ol> <p>Info</p> <p>Al igual que en el caso de la lectura, es importante se\u00f1alar que el Namenode no recibe en ning\u00fan momento los datos del fichero, sino que se limita a resolver las cuestiones relacionadas con la ubicaci\u00f3n de cada bloque. De esta manera, liberando al Namenode de la operativa de escritura, permite optimizar el funcionamiento y que el Namenode no se convierta en el cuello de botella de HDFS en las escrituras de fichero.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#5-factor-de-replica","title":"5. Factor de replica","text":"<p>Como sabemos, la replicaci\u00f3n es un concepto muy importante en HDFS, ya que nos permite tener una mayor tolerancia a fallos, pero tiene otras implicaciones en cuanto al rendimiento como acabamos de ver.</p> <p>Sin embargo, tiene una implicaci\u00f3n directa en la capacidad de almacenamiento. Ve\u00e1moslo.</p> <p>En un cl\u00faster, la capacidad de almacenamiento total viene dado por la suma de la capacidad de todos los discos que hay en los Datanodes. Por ejemplo, en un cl\u00faster de 20 nodos, con 12 discos de 3 terabytes por nodo, tendremos una capacidad de 36 terabytes por nodo, y 720 terabytes en total.</p> <p>Ahora bien, si todos los ficheros de HDFS van a tener un factor de replicaci\u00f3n 3 significar\u00e1 que cada fichero ocupar\u00e1 el triple, al haber 3 copias para cada datos. Esto hace que la capacidad total del cl\u00faster baje hasta 240 terabytes.</p> <p>Adem\u00e1s, cuando calculamos la capacidad real de un cl\u00faster, hay que dejar otro espacio para que las aplicaciones o los usuarios puedan guardar datos parciales de sus operaciones, logs, etc. Normalmente se reserva un 30 o 40% para este prop\u00f3sito, as\u00ed que nuestro cl\u00faster de 20 nodos y 36 terabytes por nodo, tendr\u00e1 una capacidad real de unos 150 terabytes. Sigue siendo una capacidad alta, pero est\u00e1 lejos de los 720 terabytes iniciales.</p> <p>Con esto, podemos afirmar por lo tanto que:</p> <ol> <li> <p>Un factor de replica alto:   * Mejora la tolerancia a fallos.   * Mejora la velocidad de lectura porque se pueden utilizar m\u00e1s Datanodes para recuperar un bloque.   * Reduce la velocidad de las escrituras porque cada bloque hay que almacenarlo en m\u00e1s Datanodes.   * Reduce la capacidad total de almacenamiento de un cl\u00faster.</p> </li> <li> <p>Un factor de replica bajo:   * Incrementa el riesgo de perder alg\u00fan dato si se corrompen los Datanodes que almacenan un bloque.   * Reduce la velocidad de lectura porque hay que leer cada bloque de uno o pocos Datanodes que lo contienen, y a lo mejor esos Datanodes est\u00e1n ocupados con otras operaciones.   * Incrementa la velocidad de escritura, al tener que escribir cada bloque en pocos Datanodes.   * Incrementa (o mejor dicho, reduce menos) la capacidad total de almacenamiento del cl\u00faster.</p> </li> </ol> <p>Con estos puntos enumerados, normalmente se aplican estas reglas para calcular el factor de replica \u00f3ptimo:</p> <ul> <li>Para datos temporales, que se van a escribir y quiz\u00e1s no se lean nunca, y que no son cr\u00edticos, el factor de replica suele ser bajo (1 \u00f3 2).</li> <li>Para datos cr\u00edticos, que es importante que no se puedan perder, y que suelen ser accedidos muchas veces, como por ejemplo una tabla maestra, el factor de replica suele ser alto (incluso teniendo una copia por cada Datanode si es accedida muchas veces y no ocupa mucho).</li> <li>Para el resto de ficheros, se suele dejar el factor de replica por defecto.</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#6-manejo-y-uso-de-hdfs","title":"6. Manejo y uso de HDFS","text":"<p>Example</p> <p>Para una primera aproximaci\u00f3n y para empezar a familiarizarnos con Apache Hadoop y HDFS usaremos la siguiente imagen de Cloudera (Necesitas pertenecer a IES Gran Capit\u00e1n)</p> <p>HDFS soporta operaciones similares a los sistemas Unix:</p> <ul> <li>Lectura, escritura o borrado de ficheros.</li> <li>Creaci\u00f3n, listado o borrado de directorios.</li> <li>Usuarios, grupos y permisos.</li> </ul> <p>En cuanto a los interfaces con los que poder usar el sistema de ficheros, ofrece diferentes interfaces, siendo los principales los mencionados a continuaci\u00f3n:</p> <ul> <li>Cliente de l\u00ednea de comandos: HDFS dispone de un amplio n\u00famero de comandos que pueden ser ejecutados en consola. </li> <li>Java API: HDFS est\u00e1 escrito en Java de forma nativa y ofrece un API que puede ser utilizado por aplicaciones con el mismo lenguaje.</li> <li>RestFul API(WebHDFS): para poder utilizar HDFS desde otros lenguajes, HDFS ofrece su funcionalidad mediante un servicio HTTP mediante el protocolo WebHDFS. Este interfaz, sin embargo, ofrece un rendimiento inferior al API de Java al utilizar HTTP como capa de transporte, por lo que no deber\u00eda utilizarse para operaciones masivas o con alto volumen de datos.</li> <li>NFS interface (HDFS NFS Gateway): es posible montar HDFS en el sistema de archivos de un cliente local utilizando la puerta de enlace NFSv3 de Hadoop.</li> <li>Librer\u00eda C: HDFS ofrece una librer\u00eda escrita en C, llamada libhdfs, que tiene un buen rendimiento, pero que no suele ofrecer toda la funcionalidad del API Java.</li> </ul> <p>Cliente de l\u00ednea de comandos</p> <p>Una vez dentro del sistema, el comando hadoop fs nos proporcionar\u00e1 todas las funcionalidades sobre HDFS. Si se introduce s\u00f3lo el comando, nos ofrecer\u00e1 la lista de opciones o comandos disponibles. Algunos de los comandos m\u00e1s utilizados son los siguientes:</p> <p>Info</p> <p> Figura 8 HDFS: HDFS DFS </p> <p><code>hadoop fs</code> es soportado por cualquier sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En cambio <code>hdfs dfs</code> es exclusivo de HDFS y es el usado en las versiones actuales. </p> <p>Note</p> <p>En la distribuci\u00f3n de cloudera, el sistema de archivos local por defecto est\u00e1 localizado en /home/cloudera y la localizaci\u00f3n por defecto de HDFS es /user/cloudera</p> <p>En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs, el cual requiere de otro argumento (empezando con un guion) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial.</p> <ul> <li> <p>Listar contenidos de un directorio: Para ver los contenidos del directorio HDFS el comando es el siguiente:</p> <p><code>hdfs dfs -ls /user/cloudera</code></p> </li> <li> <p>Crear un directorio: Para crear un nuevo directorio dentro del sistema de ficheros HDFS.</p> <p><code>hdfs dfs -mkdir /user/cloudera/prueba</code></p> </li> </ul> <p>Warning</p> <p>Atenci\u00f3n a los permisos necesarios para crear directorios en diferentes puntos del sistema de archivos</p> <ul> <li> <p>Copiar un fichero del sistema de archivos local al sistema de archivos HDFS. Se podr\u00e1 verificar esa copia mediante el comando     hdfs dfs -ls o mediante la operaci\u00f3n cat que se menciona a continuaci\u00f3n.</p> <p><code>hdfs dfs -copyFromLocal /home/cloudera/fichero /user/cloudera</code></p> <p>Por defecto, el destino de cualquier operaci\u00f3n HJDFS es /user/cloudera, de manera que es opcional especificar esa parte de la ruta salvo que sea diferente a la de por defecto.</p> </li> <li> <p>Visualizaci\u00f3n del contenido de un archivo: Para ver el contenido de un archivo ubicado en el sistema de archivo HDFS la operaci\u00f3n ser\u00e1 la siguiente:</p> <p><code>hdfs dfs -cat /user/cloudera/prueba/fichero</code></p> </li> <li> <p>Extraer un fichero del sistema de archivos HDFS: Con el fin de copiar a nuestros sistema de archivos local un archivo del sistema de archivos de HDFS se utilizar\u00e1 alguno de los siguientes comandos.     <code>hdfs dfs -copyToLocal /user/cloudera/prueba/fichero</code> <code>hdfs dfs -get /user/cloudera/prueba/fichero</code></p> </li> <li> <p>Mover ficheros dentro de HDFS: Para mover ficheros almacenados en HDFS a otros directorios de HDFS se podr\u00eda utilizar el siguiente comando:</p> <p><code>hdfs dfs -mv /user/cloudera/prueba/fichero /user/cloudera/</code></p> <p>Se permiten m\u00faltiples or\u00edgenes de ficheros, lo que obliga a que el destino sea un directorio. El movimiento de ficheros entre diferentes sistemas de archivos no est\u00e1 permitido.</p> </li> <li> <p>Copiar ficheros dentro de HDFS: Copia un fichero entre diferentes localizaciones dentro de HDFS. Tambi\u00e9n, como mv permite copiar desde diferentes or\u00edgenes, pero siempre con un directorio de destino final.</p> <p><code>hdfs dfs -cp -f /user/cloudera/prueba/fichero /user/cloudera/</code></p> <p>la opci\u00f3n -f permitir\u00e1 sobreescribir el destino si \u00e9ste existe previamente.</p> </li> <li> <p>Put: Permite copiar uno o varios or\u00edgenes desde el sistema de archivos local al sistema de archivos distribuido.</p> <p><code>hdfs dfs -put /user/cloudera/prueba/fichero /user/cloudera/</code></p> <p>Tambi\u00e9n permite leer desde la entrada est\u00e1ndar (stdin) y escribe en el sistema de archivos destino.</p> <p><code>hdfs dfs -put - /user/cloudera/entrada</code></p> </li> <li> <p>A\u00f1adir contenido al final del fichero: A veces es necesario hacer operaciones de concatenaci\u00f3n de ficheros, etc., para ello existe la operaci\u00f3n appendToFile que permite hacer esta operaci\u00f3n.     <code>hdfs dfs -appendToFile fichero_tail /user/cloudera/fichero</code></p> </li> <li> <p>Mezcla de ficheros: se utiliza para combinar varios archivos (o directorios) del sistema de archivos distribuido y luego ponerlo en un solo archivo de salida en nuestro sistema de archivos local. Dispone de una opci\u00f3n -nl con el fin de a\u00f1adir una nueva l\u00ednea al final de cada fichero.</p> <p><code>hdfs dfs -getmerge -nl file1.txt file2.txt /home/cloudera/output.txt</code></p> </li> <li> <p>Borrado de ficheros: La operaci\u00f3n rm permitir\u00e1 borrar los ficheros especificados como argumentos. Con la opci\u00f3n -R se borrar\u00e1n el directorio y los subdirectorios de forma recursiva.</p> <p><code>hdfs dfs -rm -r /user/cloudera/prueba</code></p> </li> <li> <p>Cambio de permisos a los ficheros: De la misma forma que en Linux la operaci\u00f3n chmod permitir\u00e1 realizar cambios en los permisos de uso de los ficheros. Con la opci\u00f3n -R hace que el cambio se propague recursivamente a trav\u00e9s de la estructura de directorios.</p> <p><code>hdfs dfs -chmod -R 777 /user/cloudera/prueba</code></p> </li> <li> <p>Comprobar uso de disco: Servir\u00e1 para comprobar cuando espacio de disco se est\u00e1 usando en HDFS. Si estamos interesados \u00fanicamente en el uso de disco de nuestro directorio de usuario el comando ser\u00e1:</p> <p><code>hdfs dfs -du</code></p> <p>Si por el contrario queremos conocer cuando espacio de disco est\u00e1 disponible en todo el cluster, el comando ser\u00e1:</p> <p><code>hdfs dfs -df</code></p> </li> <li> <p>Contar n\u00famero de directorios: El siguiente comando permite  obtener la informaci\u00f3n del n\u00famero de directorios , ficheros y tama\u00f1o de los mismos.</p> <p><code>hdfs dfs -count /user/cloudera</code></p> </li> <li> <p>Crea un fichero vac\u00edo:</p> <p><code>hdfs dfs -touchz /user/cloudera/emptyfile</code></p> </li> <li> <p>setrep: Modifica el factor de replica de un fichero o un directorio. Ya sabes que el factor de replica por defecto es 3. Con este comando se puede modificar para un fichero o directorio concreto.</p> <p><code>hdfs dfs -setrep 6 /user/cloudera/changerepfile</code></p> </li> </ul> <p>Recuerda</p> <p>Recuerda diferencia entre trabajar con HDFS o trabajar con el disco local de la m\u00e1quina en la que tienes abierto un terminal, que suele ser el nodo frontera. Este esquema te permitir\u00e1 ver la diferencia:</p> <p> Figura 9 HDFS: Comandos HDFS </p> <p>Cuando accedemos por terminal a una m\u00e1quina, que suele ser la m\u00e1quina frontera, y navegamos por su sistema de ficheros, lo estaremos haciendo sobre el disco o los discos que tiene esa m\u00e1quina. Cuando ejecutamos el comando <code>hdfs dfs</code> , \u00e9ste se ejecutar\u00e1 sobre el sistema de ficheros de HDFS, que es diferente al de la m\u00e1quina en la que estamos.</p> <p>Cuando queremos subir un fichero a HDFS, lo habitual es copiarlo primero en la m\u00e1quina frontera, y posteriormente subirlo a Hadoop con el comando put.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#7-instalando-hdfs","title":"7. Instalando HDFS","text":"<p>Note</p> <p>Recordamos de nuevo la instalaci\u00f3n de Hadoop vista en el punto anterior</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#71-instalacion","title":"7.1 Instalaci\u00f3n","text":"<ol> <li>Java\u2122 debe ser instalado. Las versiones de Java recomendadas se encuentran descritas en HadoopJavaVersions.</li> </ol> <pre><code>sudo apt-get install openjdk-8-jdk\n/usr/bin/java -version\n</code></pre> <ol> <li>ssh debe estar instalado y sshd debe estar ejecut\u00e1ndose para usar las secuencias de comandos de Hadoop que administran los demonios ssh remotos de Hadoop, ya que vamos a usar las secuencias de comandos de inicio y detecci\u00f3n opcionales.</li> </ol> <pre><code>sudo apt-get install ssh\n</code></pre> <ol> <li> <p>Abre una terminal</p> </li> <li> <p>Para obtener la distribuci\u00f3n de Apache Hadoop, descarga la versi\u00f3n estable m\u00e1s reciente desde Apache Download Mirrors</p> </li> </ol> <pre><code>wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n</code></pre> <ol> <li>Una vez descargado, desempaquetamos el archivo descargado con el comando tar y entra dentro de la carpeta:</li> </ol> <pre><code>sudo tar -xzf hadoop-3.3.6.tar.gz -C /opt\ncd /opt/hadoop-3.3.6\n</code></pre> <ol> <li>Edita el siguiente archivo <code>etc/hadoop/hadoop-env.sh</code> para definir la variable de entorno de Java y a\u00f1\u00e1dela.</li> </ol> <pre><code># Technically, the only required environment variable is JAVA_HOME.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n</code></pre> <ol> <li>Para poder usar los comandos de HDFS en cualquier lugar del sistema, sin tener que hacerlo desde el directorio de Hadoop (por ejemplo <code>/opt/hadoop-3.3.6/bin</code>), creamos las variables de entorno y a\u00f1adimos al PATH. Para ello abrimos el archivo <code>~/.bashrc</code> y a\u00f1adimos al final el siguiente c\u00f3digo y ejecuta el comando <code>source ~/.bashrc</code></li> </ol> ~/.bashrc<pre><code>export HADOOP_HOME=/opt/hadoop-3.3.6\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li>Ejecuta el siguiente comando. Si no da error, podemos continuar</li> </ol> <pre><code>hadoop version\n</code></pre> <p>Nos debe salir la versi\u00f3n de hadoop</p> <pre><code>Hadoop 3.3.6\nSource code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\nCompiled by ubuntu on 2023-06-18T08:22Z\nCompiled on platform linux-x86_64\nCompiled with protoc 3.7.1\nFrom source with checksum 5652179ad55f76cb287d9c633bb53bbd\nThis command was run using /opt/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#72-configuracion-pseudo-distributed-operation","title":"7.2 Configuraci\u00f3n (Pseudo-Distributed Operation)","text":"<p>Hadoop se puede ejecutar en un solo nodo en un modo pseudo-distributed donde cada demonio de Hadoop se ejecuta en un proceso Java separado.</p> <p>Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> <ol> <li>El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo <code>core-site.xml</code>. En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos (namenode) de la arquitectura. Pod\u00e9is sustituir el nombre del dominio <code>bda-iesgrancapitan</code> por el que quer\u00e1is</li> </ol> core-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://bda-iesgrancapitan:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>El siguiente paso es configurar el archivo <code>hdfs-site.xml</code> donde se indica tanto el factor de replica como la ruta donde se almacenan tanto los metadatos (namenode) como los datos en s\u00ed (datanode):</li> </ol> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Opcional: Si quieres especificar la ruta donde se almacenan los metadatos(namenode) y los datos(datanode) donde el propio hadoop los configura por defecto puedes hacerlo cambiando dichos par\u00e1metros correspondientes. Todos lo par\u00e1metros por defecto susceptibles de cambio se encuentran en este recurso</li> </ol> <p>Note</p> <p>Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode.</p> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Crea los directorios de <code>hadoop-data</code> configurados anteriormente en <code>hdfs-site.xml</code> para cuando ejecutemos hadoop y configura los permisos oportunos.</li> </ol> <pre><code>sudo mkdir -p /opt/hadoop\nsudo chown -R hadoop:hadoop /opt/hadoop\n</code></pre> <ol> <li>Comprobamos que podemos entrar por ssh al localhost sin un passphrase:</li> </ol> <pre><code>ssh localhost\nexit //Si hemos podido acceder\n</code></pre> <ol> <li>Si no puedes entrar por ssh al localhost sin un passphrase, ejecuta los siguientes comandos:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n</code></pre> <ol> <li>A\u00f1ade a <code>/etc/hosts</code>el nombre de tu dominio indicado en <code>core-site.xml</code> para que no te de error de resoluci\u00f3n de nombres. En mi caso a\u00f1ado la siguiente linea y reinicia el servicio:</li> </ol> <pre><code>127.0.0.1   bda-iesgrancapitan\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#73-ejecucion","title":"7.3 Ejecuci\u00f3n","text":"<ol> <li>Ejecuta el siguiente comando</li> </ol> <pre><code>hdfs namenode -format\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n2023-11-27 11:51:42,564 INFO namenode.NameNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-VirtualBox/127.0.1.1\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 3.3.6\n......\n......\n2023-11-27 11:51:44,678 INFO namenode.FSImage: Allocated new BlockPoolId: BP-693156123-127.0.1.1-1701082304668\n2023-11-27 11:51:45,014 INFO common.Storage: Storage directory /opt/hadoop/hadoop_data/hdfs/namenode has been successfully formatted.\n2023-11-27 11:51:45,164 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop/hadoop_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n2023-11-27 11:51:45,253 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop/hadoop_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n2023-11-27 11:51:45,386 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0\n2023-11-27 11:51:45,430 INFO namenode.FSNamesystem: Stopping services started for active state\n2023-11-27 11:51:45,431 INFO namenode.FSNamesystem: Stopping services started for standby state\n2023-11-27 11:51:45,452 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n2023-11-27 11:51:45,452 INFO namenode.NameNode: SHUTDOWN_MSG\n</code></pre> <ol> <li>Iniciando el demonio Namenode y Datanode</li> </ol> <pre><code>start-dfs.sh\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:/opt/hadoop-3.3.6$ start-dfs.sh \nStarting namenodes on [bda-iesgrancapitan]\nStarting datanodes\nStarting secondary namenodes [hadoop-VirtualBox]\nhadoop@hadoop-VirtualBox:/opt/hadoop-3.3.6$ jps\n16401 Jps\n15970 NameNode\n16085 DataNode\n16283 SecondaryNameNode\n</code></pre> <ol> <li>Accede desde el navegador a <code>http://bda-iesgrancapitan:9870/</code> para acceder al interfaz web de HDFS</li> </ol> Figura 1 Instalando HDFS: Interfaz Web. (Fuente: Propia)"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#74-usando-hdfs","title":"7.4 Usando HDFS","text":"<p>Vamos a investigar cu\u00e1l es el funcionamiento interno de HDFS estudiado en teor\u00eda.</p> <p>Para ello vamos a a\u00f1adir a HDFS un fichero de gran volumen. Accede al enlace y descarga el archivo genome_2021.zip</p> <ol> <li>Descargamos el archivo en el sistema de archivos local</li> </ol> <p><pre><code>wget https://files.grouplens.org/datasets/tag-genome-2021/genome_2021.zip\n</code></pre> 2. Vamos a observar la salida de logs en cada uno de los siguientes pasos, que nos va a servir para afianzar como como funciona HDFS. Observamos el log del namenode. En mi caso:</p> <pre><code>tail -f $HADOOP_HOME/logs/hadoop-hadoop-namenode-hadoop-VirtualBox.log\n</code></pre> <ol> <li>Lo a\u00f1adimos a HDFS</li> </ol> <pre><code>hdfs dfs -copyFromLocal genome_2021.zip /\n</code></pre> <p>La salida del log nos indica la divisi\u00f3n en bloques y la adici\u00f3n de la transacci\u00f3n en el EditLog ()</p> <pre><code>2023-11-27 11:58:19,590 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000001 -&gt; /opt/hadoop/had\noop_data/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002\n2023-11-27 11:58:19,608 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3\n2023-11-27 11:58:20,224 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /opt/hadoop/hadoop_data/hdfs/namenode/current/fsimage_0000000000000000000, fileSize: 401. Sent total: 401 by\ntes. Size of last segment intended to send: -1 bytes.\n2023-11-27 11:58:20,496 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /opt/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002, fileSize: 42. S\nent total: 42 bytes. Size of last segment intended to send: -1 bytes.\n2023-11-27 11:58:21,070 INFO org.apache.hadoop.hdfs.server.namenode.ImageServlet: Rejecting a fsimage due to small time delta and txnid delta. Time since previous checkpoint is 395 expecting at least 2700 txnid \ndelta since previous checkpoint is 2 expecting at least 1000000\n2023-11-27 12:37:53,007 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 83 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTime\ns(ms): 217 \n2023-11-27 12:37:53,265 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:37:55,015 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:37:58,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:00,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:03,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:07,189 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:09,171 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:13,229 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:16,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:18,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:22,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:26,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:28,264 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:31,502 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:33,935 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-11-27 12:38:36,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /genome_2021.zip._COPYING_ is closed by DFSClient_NONMAPREDUCE_-294249091_1\n2023-11-27 12:43:41,454 INFO org.apache.hadoop.http.HttpServer2: Process Thread Dump: jsp requested\n2023-11-27 12:58:22,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-11-27 12:58:22,096 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-11-27 12:58:22,096 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3, 51\n2023-11-27 12:58:22,096 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 50 Total time for transactions(ms): 83 Number of transactions batched in Syncs: 27 Number of syncs: 23 SyncTimes(ms): 18260 \n2023-11-27 12:58:22,164 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 50 Total time for transactions(ms): 83 Number of transactions batched in Syncs: 27 Number of syncs: 24 SyncTimes(ms): 18327 \n2023-11-27 12:58:22,164 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000003 -&gt; /opt/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000052\n2023-11-27 12:58:22,178 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 53\n2023-11-27 12:58:22,546 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /opt/hadoop/hadoop_data/hdfs/namenode/current/fsimage_0000000000000000000, fileSize: 401. Sent total: 401 bytes. Size of last segment intended to send: -1 bytes.\n2023-11-27 12:58:22,796 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /opt/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000052, fileSize: 2741. Sent total: 2741 bytes. Size of last segment intended to send: -1 bytes.\n2023-11-27 12:58:23,463 INFO org.apache.hadoop.hdfs.server.common.Util: Combined time for file download and fsync to all disks took 0,11s. The file download took 0,00s at 0,00 KB/s. Synchronous (fsync) write to disk of /opt/hadoop/hadoop_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000052 took 0,11s.\n2023-11-27 12:58:23,466 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000052 size 740 bytes.\n2023-11-27 12:58:23,580 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid &gt;= 0\n</code></pre> Figura 2 Instalando HDFS: SecondaryNamenode y Namenode. (Fuente: Ministerio de Educaci\u00f3n) <ol> <li>Como puedes observar en el log, se generan un conjunto de ficheros en la carpeta <code>current</code>, que contienen un conjunto de ficheros cuyos prefijos son:</li> </ol> <ul> <li>edits_000NNN: hist\u00f3rico de cambios que se van produciendo.</li> <li>edits_inprogress_NNN: cambios actuales en memoria que no se han persistido.</li> <li>fsimagen_000NNN: snapshot en el tiempo del sistema de ficheros.</li> </ul> <ol> <li>Si accedes a la carpeta HDFS <code>/opt/hadoop/hadoop_data/hdfs/namenode/current</code> desde nuestro sistema de archivos, puedes observarlos tambi\u00e9n</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:/opt/hadoop/hadoop_data/hdfs/namenode/current$ ls\nedits_0000000000000000001-0000000000000000002\nedits_0000000000000000003-0000000000000000052\nedits_inprogress_0000000000000000053\nfsimage_0000000000000000000\nfsimage_0000000000000000000.md5\nfsimage_0000000000000000052\nfsimage_0000000000000000052.md5\nseen_txid\nVERSION\n</code></pre> <ol> <li>Por otro lado, si accedemos a la carpeta HDFS <code>/opt/hadoop/hadoop_data/hdfs/datanode</code> desde nuestro sistema de archivos, y entramos dentro de su subdirectorio creado despu\u00e9s de la transacci\u00f3n, tambi\u00e9n podemos observar la generaci\u00f3n de los diferentes bloques</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:/opt/hadoop/hadoop_data/hdfs/datanode/current/BP-693156123-127.0.1.1-1701082304668/current/finalized/subdir0/subdir0$ ls -l \ntotal 1897632\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:37 blk_1073741825\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:37 blk_1073741825_1001.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:37 blk_1073741826\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:37 blk_1073741826_1002.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741827\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741827_1003.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741828\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741828_1004.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741829\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741829_1005.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741830\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741830_1006.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741831\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741831_1007.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741832\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741832_1008.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741833\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741833_1009.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741834\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741834_1010.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741835\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741835_1011.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741836\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741836_1012.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741837\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741837_1013.meta\n-rw-rw-r-- 1 hadoop hadoop 134217728 nov 27 12:38 blk_1073741838\n-rw-rw-r-- 1 hadoop hadoop   1048583 nov 27 12:38 blk_1073741838_1014.meta\n-rw-rw-r-- 1 hadoop hadoop  48980391 nov 27 12:38 blk_1073741839\n-rw-rw-r-- 1 hadoop hadoop    382667 nov 27 12:38 blk_1073741839_1015.meta\n</code></pre> <ol> <li>Comprobamos toda esta informaci\u00f3n y mucha m\u00e1s adicional a trav\u00e9s de la interfaz web de HDFS <code>http://bda-iesgrancapitan:9870/</code> (que es mayor que la que vimos con Cloudera, cuya versi\u00f3n de Hadoop es inferior)</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#75-administracion","title":"7.5 Administraci\u00f3n","text":"<p>HDFS tambi\u00e9n permite administraci\u00f3n desde linea de comandos. El m\u00e1s usado es la opci\u00f3n <code>hdfs dfsadmin</code></p> <p>Puedes ver todas las opciones en la documentaci\u00f3n oficial.</p> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs dfsadmin\nUsage: hdfs dfsadmin\nNote: Administrative commands can only be run as the HDFS superuser.\n    [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance] [-slownodes]]\n    [-safemode &lt;enter | leave | get | wait | forceExit&gt;]\n    [-saveNamespace [-beforeShutdown]]\n    [-rollEdits]\n    [-restoreFailedStorage true|false|check]\n    [-refreshNodes]\n    [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]\n    [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-finalizeUpgrade]\n    [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]\n    [-upgrade &lt;query | finalize&gt;]\n    [-refreshServiceAcl]\n    [-refreshUserToGroupsMappings]\n    [-refreshSuperUserGroupsConfiguration]\n    [-refreshCallQueue]\n    [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]\n    [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port|livenodes&gt; &lt;start|status|properties&gt;]\n    [-printTopology]\n    [-refreshNamenodes datanode_host:ipc_port]\n    [-getVolumeReport datanode_host:ipc_port]\n    [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]\n    [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]\n    [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]\n    [-fetchImage &lt;local directory&gt;]\n    [-allowSnapshot &lt;snapshotDir&gt;]\n    [-disallowSnapshot &lt;snapshotDir&gt;]\n    [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]\n    [-evictWriters &lt;datanode_host:ipc_port&gt;]\n    [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]\n    [-metasave filename]\n    [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt; [-namenode &lt;namenode_host:ipc_port&gt;]]\n    [-listOpenFiles [-blockingDecommission] [-path &lt;path&gt;]]\n    [-help [cmd]]\n\nGeneric options supported are:\n-conf &lt;configuration file&gt;        specify an application configuration file\n-D &lt;property=value&gt;               define a value for a given property\n-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager\n-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster\n-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath\n-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines\n\nThe general command line syntax is:\ncommand [genericOptions] [commandOptions]\n</code></pre> <p>Vamos a probar algunas de ellas:</p> <ul> <li><code>hdfs dfsadmin -report</code>: Realiza un resumen del sistema HDFS, donde podemos comprobar el estado de los diferentes nodos. Es similar al que aparece en el interfaz web.</li> <li><code>hdfs dfsadmin -listOpenFiles</code>: Comprueba si hay alg\u00fan fichero abierto.</li> <li><code>hdfs dfsadmin -printTopology</code>: Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo.</li> <li><code>hdfs dfsadmin -safemode enter</code>: Pone el sistema en modo seguro, el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.</li> <li><code>hdfs dfsadmin -safemode leave</code>: Sale del modo seguro.</li> </ul> <p>Otro ejemplo:</p> <ul> <li><code>hdfs fsck</code>: Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: <code>hdfs fsck /</code></li> </ul> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs fsck /\nonnecting to namenode via http://bda-iesgrancapitan:9870/fsck?ugi=hadoop&amp;path=%2F\nFSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path / at Mon Nov 27 13:14:05 CET 2023\n\n\nStatus: HEALTHY\n Number of data-nodes:  1\n Number of racks:       1\n Total dirs:            1\n Total symlinks:        0\n\nReplicated Blocks:\n Total size:    1928028583 B\n Total files:   1\n Total blocks (validated):  15 (avg. block size 128535238 B)\n Minimally replicated blocks:   15 (100.0 %)\n Over-replicated blocks:    0 (0.0 %)\n Under-replicated blocks:   0 (0.0 %)\n Mis-replicated blocks:     0 (0.0 %)\n Default replication factor:    1\n Average block replication: 1.0\n Missing blocks:        0\n Corrupt blocks:        0\n Missing replicas:      0 (0.0 %)\n Blocks queued for replication: 0\n\nErasure Coded Block Groups:\n Total size:    0 B\n Total files:   0\n Total block groups (validated):    0\n Minimally erasure-coded block groups:  0\n Over-erasure-coded block groups:   0\n Under-erasure-coded block groups:  0\n Unsatisfactory placement block groups: 0\n Average block group size:  0.0\n Missing block groups:      0\n Corrupt block groups:      0\n Missing internal blocks:   0\n Blocks queued for replication: 0\nFSCK ended at Mon Nov 27 13:14:06 CET 2023 in 13 milliseconds\n\n\nThe filesystem under path '/' is HEALTHY\n</code></pre> <p>Tambi\u00e9n existen otros comandos interesantes como: <code>balancer</code>, <code>cacheadmin</code>, <code>datanode</code>, <code>namenode</code>,... </p> <p>Puedes consultar la lista completa en la documentaci\u00f3n oficial</p>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#76-snapshots","title":"7.6 Snapshots","text":"<p>Mediante Snapshots podemos guardar la instant\u00e1nea de como se encuentra todo nuestros datos dentro del sistema de ficheros, que puede servir como copia de seguridad, para un futuro backup.</p> <p>Vamos a realizar un ejemplo. Creamos un directorio dentro de nuestro HDFS y copiamos nuestro fichero de genoma 2021 dentro de \u00e9l:</p> <p><pre><code>hdfs dfs -mkdir /bda\nhdfs dfs -cp /genome_2021.zip /bda\nhdfs dfs -ls /bda\n</code></pre> Activamos el uso de snapshot en el directorio que queramos obtener una instant\u00e1nea:</p> <pre><code>hdfs dfsadmin -allowSnapshot /bda\n</code></pre> <p>Procedemos a crear una instant\u00e1nea indicando la carpeta y el nombre que va a tener</p> <pre><code>hdfs dfs -createSnapshot /bda snapshot_bda_1\n</code></pre> <p>Se crea una carpeta oculta dentro de la carpeta que contendr\u00e1 la informaci\u00f3n <code>/bda/.snapshot/snapshot_bda_1</code></p> <p>Puedes verlo tambi\u00e9n desde la interfaz web de HDFS en su apartado de Snapshot</p> Figura 3 Instalando HDFS: Snapshot. (Fuente: Propia) <p>Vamos a borrar el archivo que hemos copiado y comprobamos</p> <p><pre><code>hdfs dfs -rm /bda/genome_2021.zip\n//Deleted /bda/genome_2021.zip\nhdfs dfs -ls /bda/\n</code></pre> Para recuperar el fichero usamos el snapshot creado anteriormente</p> <pre><code>hdfs dfs -cp /bda/.snapshot/snapshot_bda_1/genome_2021.zip /bda/genome_2021.zip\n</code></pre> <p>Para comprobar los directorios que actualmente soportan snapshot hacemos un ls de los mismos con su comando correspondiente:</p> <pre><code>hdfs lsSnapshottableDir\n</code></pre> <p>Por \u00faltimo, para borrar un snapshot:</p> <pre><code>hdfs dfs -deleteSnapshot /bda snapshot_bda_1\n</code></pre> <p>Y si queremos desabilitarlo los snapshot:</p> <pre><code>hdfs dfsadmin -disallowSnapshot /bda\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#77-navegacion-webui-hdfs","title":"7.7 Navegaci\u00f3n WebUI HDFS","text":"<p>Desde el apartado <code>Browser Directory</code> del Web IU <code>http://bda-iesgrancapitan:9870/explorer.html</code> podemos acceder al sistema de ficheros y su contenido de HDFS, incluidos los bloques.</p> Figura 4 Instalando HDFS: Navegaci\u00f3n WebUI. (Fuente: Propia)"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#78-permisos-webui-hdfs","title":"7.8 Permisos WebUI HDFS","text":"<p>Como hemos comentado en el punto anterior, podemos acceder al sistema de ficheros y su contenido de HDFS. Pero si intentamos borrar alg\u00fan contenido nos salta un error de permisos: <code>Permission denied: user=dr.who, access=WRITE, inode=\"/bda\":hadoop:supergroup:drwxr-xr-x</code>. Esto es debido a que, por defecto, los recursos v\u00eda web se realizan desde el usuario <code>dr.who</code></p> Figura 5 Instalando HDFS: Permisos WebUI. (Fuente: Propia) <p>Para poder tener permisos para ello podemos modificar los permisos:</p> <pre><code>hdfs dfs -mkdir /bda/prueba_permisos\nhdfs dfs -chmod 777 /bda/prueba_permisos\nhdfs dfs -cp /bda/genome_2021.zip /bda/prueba_permisos/\n//Ya podr\u00edamos borrar cualquier archivo dentro del directorio pruebas_permisos desde la WebUI\n</code></pre> <p>Otra posibilidad es modificar el archivo de configuraci\u00f3nb <code>core-site.xml</code> y a\u00f1adir la propiedad para modificar el usuario est\u00e1tico, en mi caso, el usuario <code>hadoop</code></p> core-site.xml<pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/2_Hadoop_HDFS.html#79-acceso-a-hdfs-a-traves-de-python","title":"7.9 Acceso a HDFS a trav\u00e9s de Python","text":"<p>Para ello, usaremos la libreria HdfsCLI. La instalamos mediante <code>pip</code></p> <pre><code>pip install hdfs\n</code></pre> <p>Para nuestro ejemplo, vamos a descargar un ejemplo con formato csv y a\u00f1adirlo a nuestro HDFS</p> <pre><code>wget https://www.ine.es/jaxi/files/tpx/csv_bdsc/53938.csv\nhdfs dfs -mkdir /bda/python\nhdfs dfs -copyFromLocal 53938.csv  /bda/python/\nhdfs dfs -ls /bda/python\n</code></pre> <p>Teniendo como referencia la documentaci\u00f3n, vamos a conectarnos a HDFS y copiar un archivo</p> <p>Creamos un fichero python con el siguiente c\u00f3digo:</p> <pre><code>from hdfs import InsecureClient\n\n# Datos de conexi\u00f3n\nHDFS_HOSTNAME = 'bda-iesgrancapitan'\nHDFS_PORT = 9870\nHDFS_CONNECTION = f'http://{HDFS_HOSTNAME}:{HDFS_PORT}'\n\n# En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura\nhdfs_client = InsecureClient(HDFS_CONNECTION)\n\n#Lectura\n# Leemos el fichero de '53938.csv' que tenemos en HDFS\nfichero = '/bda/python/53938.csv'\nwith hdfs_client.read(fichero) as reader:\n    texto = reader.read()\n\nprint(texto)\n\n#Escritura\n# Escribimos los elementos de la lista en formato csv\ndatos=\"dni,nombre,apellidos,direccion,cp\\n\"\nlista = [['123', 'Nombre1', 'Apellidos1', 'Mikasa1', '14000'],\n         ['456', 'Nombre4', 'Apellidos4', 'Mikasa4', '41000'],\n         ['789', 'Nombre7', 'Apellidos7', 'Mikasa7', '19000']]\nfor i in range(0, len(lista), 1):\n    for j in range(0, len(lista[i]), 1):\n        if(j&lt;len(lista[i])-1):\n            datos+=f'{lista[i][j]},'\n        else:\n            datos+=f'{lista[i][j]}\\n'\nhdfs_client.write(\"/bda/python/datos.csv\", datos)\n</code></pre> <p>Ejecutamos el fichero</p> <pre><code>python3 prueba_hdfs_Python.py\n</code></pre> <p>Comprobamos la craci\u00f3n del nuevo fichero</p> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs dfs -ls /bda/python/\nFound 2 items\n-rw-r--r--   1 hadoop supergroup      60116 2023-11-27 13:55 /bda/python/53938.csv\n-rw-r--r--   1 hadoop supergroup        145 2023-11-27 13:56 /bda/python/datos.csv\n</code></pre> <p>Adicionalmente, esta librer\u00eda te da funcionalidad opcional para <code>avro</code>, <code>dataframe</code> (con Pandas) y <code>Kerberos</code>. Estos casos son los m\u00e1s habituales en el mundo real.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html","title":"UD 4 - Apache Hadoop - MapReduce","text":""},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>MapReduce</p> <p>Hadoop MapReduce es un framework para escribir f\u00e1cilmente aplicaciones que procesan grandes cantidades de datos en paralelo en grandes cl\u00fasteres (miles de nodos) de hardware commodity de manera confiable y tolerante a fallos.</p> <p>MapReduce est\u00e1 dise\u00f1ado para poder procesar grandes cantidades de datos, ya que sigue una filosof\u00eda Divide y Vencer\u00e1s (DYV), que consiste en que para resolver un problema complejo, la mejor forma de hacerlo es dividirlo en fragmentos muy peque\u00f1os que pueden ser solucionados de forma independiente, resolverlo por separado e ir construyendo con las soluciones parciales la soluci\u00f3n final.</p> <p>Para el caso del procesamiento de datos de mucho volumen, la aproximaci\u00f3n Divide y Vencer\u00e1s que hace MapReduce consiste en dividir todo el conjunto de datos de entrada en peque\u00f1os fragmentos, procesarlos por separado, e ir agrupando los resultados parciales.</p> <p>Para ello, usa un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor.</p> <p>Note</p> <p>Hadoop Aunque MapReduce ha sido utilizado ampliamente por toda la comunidad de desarrolladores o proyectos Big Data, la realidad es que hoy en d\u00eda cada vez se usa menos, como puede verse en este gr\u00e1fico de tendencia de b\u00fasquedas en Google:</p> <p> Figura4.1_MapReduce: Historial de B\u00fasquedas en Google. (Fuente: Ministerio de Educaci\u00f3n) </p> <p>Sin embargo, es importante entender y comprender bien MapReduce por dos motivos:</p> <ul> <li>Utiliza un modelo de programaci\u00f3n que es com\u00fan en otras herramientas o frameworks Big Data.</li> <li>Muchas herramientas que se ejecutan sobre Hadoop, como puede ser el caso de Hive, pese a que ofrecen funcionalidad de alto nivel, como puede ser la capacidad de hacer consultas en formato SQL, por debajo ejecutan MapReduce. Entender MapReduce es fundamental para poder depurar o resolver problemas en la ejecuci\u00f3n de trabajos con este tipo de herramientas.</li> </ul> <p>Estos subprocesos asociados a la tarea se ejecutan de manera distribuida, en diferentes nodos de procesamiento o esclavos. Para controlar y gestionar su ejecuci\u00f3n, existe un proceso Master o Job Tracker. Tambi\u00e9n es el encargado de aceptar los nuevos trabajos enviados al sistema por los clientes.</p> Figura4.2_MapReduce WorkFlow. (Fuente: Apache Hadoop) <p>Este sistema de procesamiento se apoya en tecnolog\u00edas de almacenamiento de datos distribuidas, en cuyos nodos se ejecutan estas operaciones de tipo map y reduce. El sistema de ficheros distribuido de Hadoop es HDFS (Hadoop Distributed File System), encargado de almacenar los ficheros divididos en bloques de datos. HDFS proporciona la divisi\u00f3n previa de los datos en bloques que necesita MapReduce para ejecutar. Los resultados del procesamiento se pueden almacenar en el mismo sistema de almacenamiento o bien en una base de datos o sistema externo.</p> Figura4.3_MapReduce_Esquema de fases de MapReduce. (Fuente: AprenderBigData)"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#2-fases-y-funcionamiento","title":"2. Fases y Funcionamiento","text":"<p>Un trabajo de MapReduce se compone de cinco etapas distintas, ejecutadas en orden:</p> <ol> <li>Env\u00edo del trabajo, aceptaci\u00f3n y distribuci\u00f3n en el cl\u00faster.</li> <li>Ejecuci\u00f3n de la fase map:Se ejecuta en subtareas llamadas mappers. Estos componentes son los responsables de generar pares clave-valor filtrando, agrupando, ordenando o transformando los datos originales. Los pares de datos intermedios, no se almacenan en HDFS.</li> <li>Ejecuci\u00f3n de la fase shuffle: Puede no ser necesaria. Es el paso intermedio entre Map y reduce que ayuda a recoger los datos y ordenarlos de manera conveniente para el procesamiento. Con esta fase, se pretende agregar las ocurrencias repetidas en cada uno de los mappers.</li> <li>Ejecuci\u00f3n de la fase order. Ordena y/o baraja los datos a partir de la clave.</li> <li>Ejecuci\u00f3n de la fase reduce: Gestiona la agregaci\u00f3n de los valores producidos por todos los mappers del sistema (o por la fase shuffle) de tipo clave-valor en funci\u00f3n de su clave. Por \u00faltimo, cada reducer genera su fichero de salida de forma independiente, generalmente escrito en HDFS.</li> </ol> <p>De todas estas fases debes saber que el programador s\u00f3lo suele programar la fase map y reduce, siendo el resto de fases ejecutadas de forma autom\u00e1tica por MapReduce en base a los par\u00e1metros de configuraci\u00f3n.</p> Figura4.4_MapReduce_Framework MapReduce. (Fuente: Ministerio de Educaci\u00f3n) <p>En un trabajo Hadoop MapReduce, se dividen los datos de entrada en fragmentos independientes que son procesados por los mappers en paralelo. A continuaci\u00f3n, se ordenan los resultados del map, que son la entrada para los reducers. Generalmente, las entradas y salidas de los trabajos se almacenan en un sistema de ficheros, siendo los nodos de almacenamiento y de c\u00f3mputo los mismos. Tambi\u00e9n es muy com\u00fan que la l\u00f3gica de la aplicaci\u00f3n no se pueda descomponer en una \u00fanica ejecuci\u00f3n de MapReduce, por lo que se encadenan varias de estas fases, tratando los resultados de una como entrada para los mappers de la siguiente fase.</p> <p>Esta caracter\u00edstica, permite ejecutar las tareas de cada fragmento en el nodo donde se almacena, reduciendo el tiempo de acceso a los datos y los movimientos entre nodos del cl\u00faster.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#3-webui","title":"3. WebUI","text":"<p>Podemos observar los trabajos realizados desde la Interfaz WebUI en el puerto 8088 <code>http://bda-iesgrancapitan:8088</code></p> Figura4.5_MapReduce_WebUI. (Fuente: Propia)"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#4-ejemplos","title":"4. Ejemplos","text":"<p>Para explicar el funcionamiento de MapReduce, se van a utilizar los siguientes ejemplos:</p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#41-ejemplo-1","title":"4.1 Ejemplo 1","text":"<p>El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores:</p> Figura4.6_MapReduce_Ejemplo1_Map y Reduce. (Fuente: Ministerio de Educaci\u00f3n) <p>Siguiendo MapReduce:</p> <ul> <li>Fase de mapeo (Map): Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados.</li> <li>Fase de reducci\u00f3n (Reduce): Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n.</li> </ul> <p>Es un proceso de procesamiento costoso. Los pasos ser\u00edan los siguientes:</p> Figura4.7_MapReduce_Ejemplo1 Detallado. (Fuente: Ministerio de Educaci\u00f3n) <ol> <li>Lectura desde HDFS de los ficheros de entrada.</li> <li>Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos que tengamos.</li> <li>El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias).</li> <li>Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave.</li> <li>La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color.</li> <li>Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS.</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#42-ejemplo-2","title":"4.2 Ejemplo 2","text":"<p>Imagina que tenemos un fichero de muchos terabytes de datos con todas las cotizaciones de todas las empresas de todas las bolsas del mundo desde hace 30 a\u00f1os, con una cotizaci\u00f3n cada minuto. Cada l\u00ednea del fichero tiene el siguiente formato:</p> <p><code>Fecha y hora (d\u00eda/mes/a\u00f1o hora:minutos:segundos);nombre de la empresa;valor de cotizaci\u00f3n actual;valor de cotizaci\u00f3n anterior</code></p> <p>Por ejemplo, algunas de las l\u00edneas del fichero podr\u00edan ser las siguientes:</p> <pre><code>20/01/2021 11:54:34;SANTANDER;4,54;4,49\n14/05/1995 09:54;TELEFONICA;11,90;12,01\n01/01/1997 08:03:21;SANTANDER;11,24;11,49\n19/06/2022 11:54:22;APPLE;111,25;114,89\n23/04/2003 16:32:11;ALPHABET;34,49;36,44;\n21/12/2020 10:10:56;TELEFONICA;14,31;14;29\n26/02/1995 14:09:40;MICROSOFT;132,29;133,95\n04/05/1999 11:05:34;WALLMART;34,98;35,05\n</code></pre> <p>Tomando una media de 35.000 empresas cotizadas en el mundo, es decir, 35.000 cotizaciones por minuto ser\u00edan 25.200.000 cotizaciones al d\u00eda, y un total de 275.940.000.000 l\u00edneas en el fichero por los 30 a\u00f1os a 365 d\u00edas por cada a\u00f1o, que son unos 25 terabytes de datos en un \u00fanico fichero.</p> <p>Nuestro objetivo es averiguar cu\u00e1ntas veces ha tenido cada empresa un incremento en su cotizaci\u00f3n, es decir, si una cotizaci\u00f3n es superior a su valor anterior, sumaremos uno, y si la cotizaci\u00f3n es inferior, no lo sumaremos. Es decir, el resultado ser\u00eda una lista de la siguiente forma:</p> <pre><code>SANTANDER 3888981\nTELEFONICA 3331923\n</code></pre> <p>Intentar este c\u00e1lculo leyendo el fichero de forma secuencial y teniendo un contador para cada empresa ser\u00eda un proceso que llevar\u00eda d\u00edas de procesamiento, as\u00ed que vamos a utilizar MapReduce para realizar este proceso.</p> <p>Como se describi\u00f3 anteriormente, el primer paso es crear la aplicaci\u00f3n, por ejemplo, utilizando lenguaje Java, y enviar el programa al cl\u00faster Hadoop utilizando el API de MapReduce para enviar trabajos.</p> <p>Una vez arrancada la aplicaci\u00f3n, en primer lugar decidir\u00e1 c\u00f3mo partir el fichero de entrada en fragmentos para que los datos puedan ser procesados en paralelo. El componente que realiza esta divisi\u00f3n de los ficheros de entrada se denomina InputFormat.</p> <p>Por cada fragmento del fichero de entrada, se crea una tarea map que ejecutar\u00e1 la funci\u00f3n map desarrollada en diferentes nodos y en paralelo, es decir, cada fragmento ser\u00e1 procesado en paralelo por diferentes nodos.</p> <p>La funci\u00f3n map toma cada l\u00ednea, que es separada por el InputFormat, la lee, y emite un resultado parcial, que ser\u00e1 <code>[Nombre de la empresa, 1]</code>, en los casos en los que vea que el valor actual es mayor que el valor anterior. Esta funci\u00f3n se ejecutar\u00e1 tantas veces como l\u00edneas tenga el fragmento de fichero asignado, y en tantos nodos como fragmentos se haya dividido el fichero.</p> <p>Es decir, para cada nodo, tendremos, por ejemplo, este resultado:</p> <pre><code>SANTANDER, 1\nWALLMART, 1\nTELEFONICA, 1\nAPPLE, 1\nALPHABET, 1\n...\n</code></pre> <p>Y tendremos tantos resultados de \u00e9stos como fragmentos del fichero haya, y en tantos nodos/servidores como se haya ejecutado la funci\u00f3n.</p> <p>A continuaci\u00f3n se ejecutan las fases de shuffle y sort de forma autom\u00e1tica y transparente para el desarrollador, donde se toman los resultados parciales, se ordenan por una clave, que en este caso ser\u00e1 el nombre de la empresa, se combinan y se ordenan, juntando todos los valores de cada empresa, es decir, teniendo la lista de valores con el siguiente formato:</p> <pre><code>SANTANDER, 1\nSANTANDER, 2\nSANTANDER, 1\n...\nTELEFONICA, 1\nTELEFONICA, 1\n...\nAPPLE, 1\n...\nALPHABET, 1\nALPHABET, 1\nALPHABET, 1\n...\n</code></pre> <p>Por \u00faltimo, se divide la lista ordenada en diferentes particiones, siendo cada partici\u00f3n un conjunto de datos con la misma clave, y se llaman a la funci\u00f3n reduce desarrollada por el usuario, que tomar\u00e1 los diferentes valores emitidos por la fase map, pero ya ordenados y unidos, e ir\u00e1 haciendo la suma de cada empresa, dando como resultado pares <code>[Nombre de la empresa, n\u00famero de veces que se ha encontrado una cotizaci\u00f3n incrementada]</code>.</p> <p>MapReduce, por \u00faltimo tomar\u00e1 todos los resultados de las funciones reduce y las unir\u00e1, formando el resultado final, que ser\u00e1 la lista total de empresas con el n\u00famero de veces en las que la cotizaci\u00f3n sube.</p> <p>El ejemplo puede parecer sencillo, pero permite entender c\u00f3mo funciona MapReduce para dividir un procesamiento en diferentes bloques de ejecuci\u00f3n que se ejecutan en paralelo.</p> <p>En la siguiente imagen puede verse de forma gr\u00e1fica el ejemplo anterior:</p> Figura4.8_MapReduce_Ejemplo2 Detallado. (Fuente: Ministerio de Educaci\u00f3n) <p>Sobre los ejemplos</p> <p>Quiz\u00e1s los ejemplos te parezcan muy sencillos y puedes pensar que MapReduce no resuelve problemas muy complejos. La realidad es que MapReduce permite resolver problemas de procesamiento de datos realmente complejos, pero requiere en primer lugar un estudio de problema y una divisi\u00f3n en problemas m\u00e1s sencillos que pueden resolverse en paralelo.</p> <p>Adem\u00e1s, cuando tienes que resolver un problema muy complejo, lo habitual es desarrollar un flujo de ejecuci\u00f3n en el que se concatenan varios procesos MapReduce, donde el primer proceso hace una parte del trabajo, su resultado lo coge otro proceso MapReduce que realiza otra parte, etc. hasta llegar a tener todo el problema resuelto.</p> <p>La esencia de MapReduce para resolver un problema es intentar descomponerlo en problemas m\u00e1s sencillos en los que cada problema se resuelve con una primera fase en la que se toman todos los datos de entrada uno a uno, se realiza alguna operaci\u00f3n con ellos, y los resultados son combinados y ejecutados por otra fase de ejecuci\u00f3n que realiza una operaci\u00f3n con la que se devuelve el resultado.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#5-ejercicios","title":"5. Ejercicios","text":"<p>Vamos a ver 2 ejercicios para poner en pr\u00e1ctica lo aprendido en MapReduce</p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#51-ejercicio-1","title":"5.1 Ejercicio 1","text":"<p>En este ejercicio, vamos a crear nuestro primer programa MapReduce. Asegurate de que tienes Apache Hadoop instalado y funcionando. Si no, accede al recurso correspondiente.</p> <ol> <li>Creamos un directorio en local donde vamos a desarrollar cada uno de los c\u00f3digos fuente en Java</li> </ol> <pre><code>mkdir -p $HOME/bda/MapReduce/ejercicios\ncd $HOME/bda/MapReduce/ejercicios\n</code></pre> <ol> <li>Descargamos el fichero fuente para el ejercicio en local. Contiene informaci\u00f3n relacionada con ventas, el nombre del producto, el precio, el modo de pago, la ciudad, el pa\u00eds del cliente, etc; alojado en este gist de github</li> </ol> <p><pre><code>wget https://gist.githubusercontent.com/jaimerabasco/cb528c32b4c4092e6a0763d8b6bc25c0/raw/54b30a89f3b608d0837bd1fc10bc31e64ba4c7c8/Ventas_Enero23.csv\n</code></pre> El objetivo ser\u00e1 descubrir el n\u00famero de productos vendidos en cada pa\u00eds.</p> <ol> <li>Generamos las clases necesarias para nuestro objetivo en MapReduce</li> </ol> VentasMapper.java<pre><code>package VentasPais;\n\nimport java.io.IOException;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.*;\n\npublic class VentasMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n    private final static IntWritable one = new IntWritable(1);\n\n    public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n        String valueString = value.toString();\n        String[] SingleData_porPais = valueString.split(\",\");\n        output.collect(new Text(SingleData_porPais[7]), one);\n    }\n}\n</code></pre> VentasPaisDriver.java<pre><code>package VentasPais;\n\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapred.*;\n\npublic class VentasPaisDriver {\n    public static void main(String[] args) {\n        JobClient my_client = new JobClient();\n        // Creamos el objeto configuraci\u00f3b para el trabajo\n        JobConf job_conf = new JobConf(VentasPaisDriver.class);\n\n        // Configuramos un nombre del trabajo\n        job_conf.setJobName(\"VentaPorPais\");\n\n        // Especificamos el tipo de dato de clave y valor de salida\n        job_conf.setOutputKeyClass(Text.class);\n        job_conf.setOutputValueClass(IntWritable.class);\n\n        // Especificamos nombres de la clase Mapper y Reducer\n        job_conf.setMapperClass(VentasPais.VentasMapper.class);\n        job_conf.setReducerClass(VentasPais.VentasPaisReducer.class);\n\n        // Especificamos formato del tipo de dato de entrada y salida\n        job_conf.setInputFormat(TextInputFormat.class);\n        job_conf.setOutputFormat(TextOutputFormat.class);\n\n        // Cambiamos la entrada y salida de directorios usando entrada como argumentos:\n        //arg[0] = nombre del directorio de entrada en HDFS,\n        //arg[1] = nombre del directorio de salida que se va a crear para almacenar el fichero de salida\n\n        FileInputFormat.setInputPaths(job_conf, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));\n\n        my_client.setConf(job_conf);\n        try {\n            // Ejecuta el job \n            JobClient.runJob(job_conf);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre> VentasPaisReducer.java<pre><code>package VentasPais;\n\nimport java.io.IOException;\nimport java.util.*;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.*;\n\npublic class VentasPaisReducer extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n    public void reduce(Text t_key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text,IntWritable&gt; output, Reporter reporter) throws IOException {\n        Text key = t_key;\n        int frequencyPorPais = 0;\n        while (values.hasNext()) {\n            // reemplazamos el tipo de valor con el tipo real de nuestro valor\n            IntWritable value = (IntWritable) values.next();\n            frequencyPorPais += value.get();\n\n        }\n        output.collect(key, new IntWritable(frequencyPorPais));\n    }\n}\n</code></pre> <ol> <li>Copia el fichero del \"Ventas_Enero23.csv\" a HDFS</li> </ol> <pre><code>hdfs dfs -mkdir -p /bda/mapreduce/ejercicios\nhdfs dfs -copyFromLocal Ventas_Enero23.csv /bda/mapreduce/ejercicios\n</code></pre> <ol> <li>Generamos los .class para crear el paquete jar</li> </ol> <pre><code>javac -cp `hadoop classpath` -d . VentasMapper.java VentasPaisReducer.java VentasPaisDriver.java\n</code></pre> <p>Se genera un directorio con los .class </p> <ol> <li>Creamos un archivo llamado <code>Manifest.txt</code> con el siguiente contenido, donde indicamos el nombre de la clase main</li> </ol> <p><pre><code>Main-Class: VentasPais.VentasPaisDriver\n</code></pre> Incluye tambi\u00e9n un salto de l\u00ednea al final</p> <ol> <li>Generamos el .jar</li> </ol> <pre><code>jar cfm VentasProductosPorPais.jar Manifest.txt VentasPais/*.class\n</code></pre> <ol> <li>Ejecutamos la aplicaci\u00f3n creada indicando el input de entrada(Ventas_Enero23.csv) y la salida</li> </ol> <p><pre><code>hadoop jar VentasProductosPorPais.jar /bda/mapreduce/ejercicios/Ventas_Enero23.csv /bda/mapreduce/ejercicios/salida_ventas\n</code></pre> Puedes usar tambi\u00e9n el comando <code>yarn jar</code></p> <pre><code>2023-11-27 18:47:24,828 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n2023-11-27 18:47:24,910 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n2023-11-27 18:47:24,911 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n2023-11-27 18:47:24,975 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n2023-11-27 18:47:25,160 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n2023-11-27 18:47:25,342 INFO mapred.FileInputFormat: Total input files to process : 1\n2023-11-27 18:47:25,484 INFO mapreduce.JobSubmitter: number of splits:1\n2023-11-27 18:47:25,639 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local514940019_0001\n2023-11-27 18:47:25,639 INFO mapreduce.JobSubmitter: Executing with tokens: []\n2023-11-27 18:47:25,910 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n2023-11-27 18:47:25,911 INFO mapreduce.Job: Running job: job_local514940019_0001\n2023-11-27 18:47:25,926 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n2023-11-27 18:47:25,927 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n2023-11-27 18:47:25,932 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2023-11-27 18:47:25,941 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2023-11-27 18:47:26,161 INFO mapred.LocalJobRunner: Waiting for map tasks\n2023-11-27 18:47:26,164 INFO mapred.LocalJobRunner: Starting task: attempt_local514940019_0001_m_000000_0\n2023-11-27 18:47:26,179 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2023-11-27 18:47:26,179 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2023-11-27 18:47:26,207 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2023-11-27 18:47:26,230 INFO mapred.MapTask: Processing split: hdfs://bda-iesgrancapitan:9000/bda/mapreduce/ejercicios/Ventas_Enero23.csv:0+123637\n2023-11-27 18:47:26,271 INFO mapred.MapTask: numReduceTasks: 1\n2023-11-27 18:47:26,349 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n2023-11-27 18:47:26,350 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n2023-11-27 18:47:26,350 INFO mapred.MapTask: soft limit at 83886080\n2023-11-27 18:47:26,350 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n2023-11-27 18:47:26,350 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n2023-11-27 18:47:26,352 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n2023-11-27 18:47:26,464 INFO mapred.LocalJobRunner: \n2023-11-27 18:47:26,471 INFO mapred.MapTask: Starting flush of map output\n2023-11-27 18:47:26,472 INFO mapred.MapTask: Spilling map output\n2023-11-27 18:47:26,472 INFO mapred.MapTask: bufstart = 0; bufend = 15743; bufvoid = 104857600\n2023-11-27 18:47:26,472 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210404(104841616); length = 3993/6553600\n2023-11-27 18:47:26,489 INFO mapred.MapTask: Finished spill 0\n2023-11-27 18:47:26,520 INFO mapred.Task: Task:attempt_local514940019_0001_m_000000_0 is done. And is in the process of committing\n2023-11-27 18:47:26,524 INFO mapred.LocalJobRunner: hdfs://bda-iesgrancapitan:9000/bda/mapreduce/ejercicios/Ventas_Enero23.csv:0+123637\n2023-11-27 18:47:26,524 INFO mapred.Task: Task 'attempt_local514940019_0001_m_000000_0' done.\n2023-11-27 18:47:26,530 INFO mapred.Task: Final Counters for attempt_local514940019_0001_m_000000_0: Counters: 23\n    File System Counters\n        FILE: Number of bytes read=3122\n        FILE: Number of bytes written=657555\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=123637\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=5\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=1\n        HDFS: Number of bytes read erasure-coded=0\n    Map-Reduce Framework\n        Map input records=999\n        Map output records=999\n        Map output bytes=15743\n        Map output materialized bytes=17747\n        Input split bytes=126\n        Combine input records=0\n        Spilled Records=999\n        Failed Shuffles=0\n        Merged Map outputs=0\n        GC time elapsed (ms)=22\n        Total committed heap usage (bytes)=167841792\n    File Input Format Counters \n        Bytes Read=123637\n2023-11-27 18:47:26,530 INFO mapred.LocalJobRunner: Finishing task: attempt_local514940019_0001_m_000000_0\n2023-11-27 18:47:26,530 INFO mapred.LocalJobRunner: map task executor complete.\n2023-11-27 18:47:26,553 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n2023-11-27 18:47:26,553 INFO mapred.LocalJobRunner: Starting task: attempt_local514940019_0001_r_000000_0\n2023-11-27 18:47:26,558 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n</code></pre> <ol> <li>Vemos el fichero resultante y comprobamos si hemos obtenido los datos que ten\u00edamos como objetivo</li> </ol> <p><pre><code>hdfs dfs -cat /bda/mapreduce/ejercicios/salida_ventas/part-00000\n</code></pre> <pre><code>Argentina   1\nAustralia   38\nAustria 7\nBahrain 1\nBelgium 8\nBermuda 1\nBrazil  5\nBulgaria    1\nCO  1\nCanada  76\nCayman Isls 1\nChina   1\nCosta Rica  1\nCountry 1\nCzech Republic  3\nDenmark 15\nDominican Republic  1\nFinland 2\nFrance  27\nGermany 25\nGreece  1\nGuatemala   1\nHong Kong   1\nHungary 3\nIceland 1\nIndia   2\nIreland 49\nIsrael  1\nItaly   15\nJapan   2\nJersey  1\nKuwait  1\nLatvia  1\nLuxembourg  1\nMalaysia    1\nMalta   2\nMauritius   1\nMoldova 1\nMonaco  2\nNetherlands 22\nNew Zealand 6\nNorway  16\nPhilippines 2\nPoland  2\nRomania 1\nRussia  1\nSouth Africa    5\nSouth Korea 1\nSpain   12\nSweden  13\nSwitzerland 36\nThailand    2\nThe Bahamas 2\nTurkey  6\nUkraine 1\nUnited Arab Emirates    6\nUnited Kingdom  100\nUnited States   462\n</code></pre></p>"},{"location":"UD4%20-%20Apache%20Hadoop/3_Hadoop_Map_Reduce.html#52-ejercicio-2","title":"5.2 Ejercicio 2","text":"<p>Vamos a realizar un ejercicio de ejemplo de uso de MapReduce. En este caso vamos a usar una de las aplicaciones que ya vienen dentro de MapReduce, que es el contador de palabras</p> <ol> <li>Primero descargamos un fichero en local de este gist alojado en github. En este caso vamos a contar las palabras de \"El quijote\".</li> </ol> <pre><code>wget https://gist.githubusercontent.com/jaimerabasco/cb528c32b4c4092e6a0763d8b6bc25c0/raw/54b30a89f3b608d0837bd1fc10bc31e64ba4c7c8/El_Quijote.txt\n</code></pre> <ol> <li>Crea en HDFS la carpeta donde vas a alojar el fichero</li> </ol> <pre><code>// No es necesario si ya los has creado en el ejercicio anterior \nhdfs dfs -mkdir /bda/mapreduce\nhdfs dfs -mkdir /bda/mapreduce/ejercicios\n</code></pre> <ol> <li>Copia el fichero del \"El quijote\" a HDFS</li> </ol> <pre><code>hdfs dfs -copyFromLocal El_Quijote.txt /bda/mapreduce/ejercicios\n</code></pre> <ol> <li>Listamos la lista de ejemplos que nos dispone MapReduce</li> </ol> <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar\n</code></pre> <ol> <li>Vemos muchos de ellos. Para este ejercicio nos interesa wordcount</li> </ol> <pre><code>An example program must be given as the first argument.\nValid program names are:\n  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n  dbcount: An example job that count the pageview counts from a database.\n  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n  grep: A map/reduce program that counts the matches of a regex in the input.\n  join: A job that effects a join over sorted, equally partitioned datasets\n  multifilewc: A job that counts words from several files.\n  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n  randomwriter: A map/reduce program that writes 10GB of random data per node.\n  secondarysort: An example defining a secondary sort to the reduce.\n  sort: A map/reduce program that sorts the data written by the random writer.\n  sudoku: A sudoku solver.\n  teragen: Generate data for the terasort\n  terasort: Run the terasort\n  teravalidate: Checking results of terasort\n  wordcount: A map/reduce program that counts the words in the input files.\n  wordmean: A map/reduce program that counts the average length of the words in the input files.\n  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n</code></pre> <ol> <li>Vemos los par\u00e1metros que necesitamos para wordcount</li> </ol> <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount\nUsage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;\n</code></pre> <ol> <li>Ejecuta el ejemplo de contar palabras sobre el \"El quijote\". Hay que indicar el fichero de entrada y de salida</li> </ol> <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /bda/mapreduce/ejercicios/El_Quijote.txt /bda/mapreduce/ejercicios/salida_quijote\n</code></pre> <pre><code>2023-11-27 18:59:30,696 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n2023-11-27 18:59:30,801 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n2023-11-27 18:59:30,801 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n2023-11-27 18:59:31,074 INFO input.FileInputFormat: Total input files to process : 1\n2023-11-27 18:59:31,149 INFO mapreduce.JobSubmitter: number of splits:1\n2023-11-27 18:59:31,314 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local498821156_0001\n2023-11-27 18:59:31,315 INFO mapreduce.JobSubmitter: Executing with tokens: []\n2023-11-27 18:59:31,470 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n2023-11-27 18:59:31,471 INFO mapreduce.Job: Running job: job_local498821156_0001\n2023-11-27 18:59:31,473 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n2023-11-27 18:59:31,477 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n2023-11-27 18:59:31,478 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2023-11-27 18:59:31,478 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2023-11-27 18:59:31,478 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n2023-11-27 18:59:31,601 INFO mapred.LocalJobRunner: Waiting for map tasks\n2023-11-27 18:59:31,602 INFO mapred.LocalJobRunner: Starting task: attempt_local498821156_0001_m_000000_0\n2023-11-27 18:59:31,615 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n2023-11-27 18:59:31,617 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2023-11-27 18:59:31,617 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2023-11-27 18:59:31,651 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2023-11-27 18:59:31,657 INFO mapred.MapTask: Processing split: hdfs://bda-iesgrancapitan:9000/bda/mapreduce/ejercicios/El_Quijote.txt:0+2161175\n2023-11-27 18:59:31,741 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n2023-11-27 18:59:31,741 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n2023-11-27 18:59:31,741 INFO mapred.MapTask: soft limit at 83886080\n2023-11-27 18:59:31,741 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n2023-11-27 18:59:31,741 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n2023-11-27 18:59:31,745 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n2023-11-27 18:59:32,166 INFO mapred.LocalJobRunner: \n2023-11-27 18:59:32,182 INFO mapred.MapTask: Starting flush of map output\n2023-11-27 18:59:32,182 INFO mapred.MapTask: Spilling map output\n2023-11-27 18:59:32,183 INFO mapred.MapTask: bufstart = 0; bufend = 3688770; bufvoid = 104857600\n2023-11-27 18:59:32,183 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24677300(98709200); length = 1537097/6553600\n2023-11-27 18:59:32,488 INFO mapreduce.Job: Job job_local498821156_0001 running in uber mode : false\n2023-11-27 18:59:32,488 INFO mapreduce.Job:  map 0% reduce 0%\n2023-11-27 18:59:32,803 INFO mapred.MapTask: Finished spill 0\n2023-11-27 18:59:32,823 INFO mapred.Task: Task:attempt_local498821156_0001_m_000000_0 is done. And is in the process of committing\n2023-11-27 18:59:32,825 INFO mapred.LocalJobRunner: map\n2023-11-27 18:59:32,826 INFO mapred.Task: Task 'attempt_local498821156_0001_m_000000_0' done.\n2023-11-27 18:59:32,830 INFO mapred.Task: Final Counters for attempt_local498821156_0001_m_000000_0: Counters: 24\n    File System Counters\n        FILE: Number of bytes read=281548\n        FILE: Number of bytes written=1526135\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=2161175\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=5\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=1\n        HDFS: Number of bytes read erasure-coded=0\n    Map-Reduce Framework\n        Map input records=37863\n        Map output records=384275\n        Map output bytes=3688770\n        Map output materialized bytes=605631\n        Input split bytes=135\n        Combine input records=384275\n        Combine output records=40067\n        Spilled Records=40067\n        Failed Shuffles=0\n        Merged Map outputs=0\n        GC time elapsed (ms)=31\n        Total committed heap usage (bytes)=167841792\n    File Input Format Counters \n        Bytes Read=2161175\n2023-11-27 18:59:32,831 INFO mapred.LocalJobRunner: Finishing task: attempt_local498821156_0001_m_000000_0\n2023-11-27 18:59:32,832 INFO mapred.LocalJobRunner: map task executor complete.\n2023-11-27 18:59:32,856 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n2023-11-27 18:59:32,857 INFO mapred.LocalJobRunner: Starting task: attempt_local498821156_0001_r_000000_0\n2023-11-27 18:59:32,861 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n2023-11-27 18:59:32,861 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n2023-11-27 18:59:32,861 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n2023-11-27 18:59:32,861 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2023-11-27 18:59:32,867 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4de8aeff\n2023-11-27 18:59:32,872 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n2023-11-27 18:59:32,917 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=693954112, maxSingleShuffleLimit=173488528, mergeThreshold=458009728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n2023-11-27 18:59:32,939 INFO reduce.EventFetcher: attempt_local498821156_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n2023-11-27 18:59:32,972 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local498821156_0001_m_000000_0 decomp: 605627 len: 605631 to MEMORY\n2023-11-27 18:59:32,974 INFO reduce.InMemoryMapOutput: Read 605627 bytes from map-output for attempt_local498821156_0001_m_000000_0\n2023-11-27 18:59:32,975 INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 605627, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;605627\n2023-11-27 18:59:32,976 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n2023-11-27 18:59:32,976 INFO mapred.LocalJobRunner: 1 / 1 copied.\n2023-11-27 18:59:32,976 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n2023-11-27 18:59:32,982 INFO mapred.Merger: Merging 1 sorted segments\n2023-11-27 18:59:32,982 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 605620 bytes\n2023-11-27 18:59:33,049 INFO reduce.MergeManagerImpl: Merged 1 segments, 605627 bytes to disk to satisfy reduce memory limit\n2023-11-27 18:59:33,049 INFO reduce.MergeManagerImpl: Merging 1 files, 605631 bytes from disk\n2023-11-27 18:59:33,050 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n2023-11-27 18:59:33,050 INFO mapred.Merger: Merging 1 sorted segments\n2023-11-27 18:59:33,050 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 605620 bytes\n2023-11-27 18:59:33,051 INFO mapred.LocalJobRunner: 1 / 1 copied.\n2023-11-27 18:59:33,164 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n2023-11-27 18:59:33,463 INFO mapred.Task: Task:attempt_local498821156_0001_r_000000_0 is done. And is in the process of committing\n2023-11-27 18:59:33,465 INFO mapred.LocalJobRunner: 1 / 1 copied.\n2023-11-27 18:59:33,465 INFO mapred.Task: Task attempt_local498821156_0001_r_000000_0 is allowed to commit now\n2023-11-27 18:59:33,490 INFO mapreduce.Job:  map 100% reduce 0%\n2023-11-27 18:59:33,526 INFO output.FileOutputCommitter: Saved output of task 'attempt_local498821156_0001_r_000000_0' to hdfs://bda-iesgrancapitan:9000/bda/mapreduce/ejercicios/salida_quijote\n2023-11-27 18:59:33,526 INFO mapred.LocalJobRunner: reduce &gt; reduce\n2023-11-27 18:59:33,526 INFO mapred.Task: Task 'attempt_local498821156_0001_r_000000_0' done.\n2023-11-27 18:59:33,527 INFO mapred.Task: Final Counters for attempt_local498821156_0001_r_000000_0: Counters: 30\n    File System Counters\n        FILE: Number of bytes read=1492842\n        FILE: Number of bytes written=2131766\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=2161175\n        HDFS: Number of bytes written=448985\n        HDFS: Number of read operations=10\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=3\n        HDFS: Number of bytes read erasure-coded=0\n    Map-Reduce Framework\n        Combine input records=0\n        Combine output records=0\n        Reduce input groups=40067\n        Reduce shuffle bytes=605631\n        Reduce input records=40067\n        Reduce output records=40067\n        Spilled Records=40067\n        Shuffled Maps =1\n        Failed Shuffles=0\n        Merged Map outputs=1\n        GC time elapsed (ms)=5\n        Total committed heap usage (bytes)=167841792\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Output Format Counters \n        Bytes Written=448985\n2023-11-27 18:59:33,528 INFO mapred.LocalJobRunner: Finishing task: attempt_local498821156_0001_r_000000_0\n2023-11-27 18:59:33,528 INFO mapred.LocalJobRunner: reduce task executor complete.\n2023-11-27 18:59:34,491 INFO mapreduce.Job:  map 100% reduce 100%\n2023-11-27 18:59:34,493 INFO mapreduce.Job: Job job_local498821156_0001 completed successfully\n2023-11-27 18:59:34,528 INFO mapreduce.Job: Counters: 36\n    File System Counters\n        FILE: Number of bytes read=1774390\n        FILE: Number of bytes written=3657901\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=4322350\n        HDFS: Number of bytes written=448985\n        HDFS: Number of read operations=15\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=4\n        HDFS: Number of bytes read erasure-coded=0\n    Map-Reduce Framework\n        Map input records=37863\n        Map output records=384275\n        Map output bytes=3688770\n        Map output materialized bytes=605631\n        Input split bytes=135\n        Combine input records=384275\n        Combine output records=40067\n        Reduce input groups=40067\n        Reduce shuffle bytes=605631\n        Reduce input records=40067\n        Reduce output records=40067\n        Spilled Records=80134\n        Shuffled Maps =1\n        Failed Shuffles=0\n        Merged Map outputs=1\n        GC time elapsed (ms)=36\n        Total committed heap usage (bytes)=335683584\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Input Format Counters \n        Bytes Read=2161175\n    File Output Format Counters \n        Bytes Written=448985\n</code></pre> <ol> <li>Leemos el fichero de salida. Aqu\u00ed est\u00e1n listados todas las palabras de El Quijote y cuantas veces aparece cada palabra</li> </ol> <pre><code>hdfs dfs -cat /bda/mapreduce/ejercicios/salida_quijote/part-r-00000\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html","title":"UD 4 - Apache Hadoop - Yarn","text":"<p>Introducci\u00f3n</p> <p>Seguimos adentr\u00e1ndonos en lo que es Apache Hadoop. Ya hemos entendido que Apache Hadoop ofrece una capa de almacenamiento, que es HDFS, con la que podr\u00e1 almacenar todos los datos. Hemos entendido c\u00f3mo funciona HDFS y qu\u00e9 aspectos debe tener en cuenta para dimensionar correctamente su plataforma. El siguiente paso en su camino por entender Hadoop es conocer la capacidad que ofrece Hadoop para procesar todos los datos almacenados en HDFS. Es el turno de conocer YARN. YARN ser\u00e1 la base sobre la que se ejecutar\u00e1n todas las aplicaciones de procesamiento o an\u00e1lisis de datos, as\u00ed que es un punto importante a conocer.</p> <p>YARN es el acr\u00f3nimo de Yet Another Resource Negotiator, es decir, seg\u00fan su acr\u00f3nimo es un gestor de recursos.</p> <p>En las primeras versiones de Hadoop, todo el procesamiento se realizaba con MapReduce, y que, pese a que su funcionamiento era correcto, ya que era capaz de ofrecer la capacidad de desarrollar aplicaciones complejas que procesaran un gran volumen de datos, ten\u00eda varios problemas:</p> <ul> <li>Restring\u00eda mucho el tipo de aplicaciones que los desarrolladores pod\u00edan realizar, ya que hab\u00eda que ce\u00f1irse a las operaciones y forma de ejecuci\u00f3n que MapReduce ofrec\u00eda, por lo que era dif\u00edcil utilizar los datos de HDFS para otro tipo de usos como el procesamiento en tiempo real.</li> <li>MapReduce es un modelo de programaci\u00f3n muy poco eficiente, lo que hace que los casos de uso que requieren respuestas r\u00e1pidas no sean viables.</li> <li>La concurrencia en la ejecuci\u00f3n de aplicaciones no estaba bien resuelta, por lo que cuando un usuario o aplicaci\u00f3n lanzaba un trabajo MapReduce, se podr\u00eda decir que el resto ten\u00eda que esperar a que terminara la tarea para poder lanzar nuevos trabajos.</li> </ul> Figura 4.1_Yarn: Arquitectura YARN Hadoop. (Fuente: Ministerio de Educaci\u00f3n)  <p>Por este motivo, en la versi\u00f3n 2 de Hadoop se introdujo YARN. El objetivo de YARN era poder independizar el almacenamiento del procesamiento, abrir Hadoop a cualquier tipode aplicaci\u00f3n que quiera trabajar con los datos de HDFS, y dar la posibilidad de quem\u00faltiples usuarios puedan trabajar con la plataforma.</p> Figura 4.2_Yarn: Arquitectura YARN Hadoop. (Fuente: Ministerio de Educaci\u00f3n)"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#1-contenedores","title":"1. Contenedores","text":"<p>En YARN es importante conocer el concepto de contenedor, que es la unidad m\u00ednima de recursos de ejecuci\u00f3n para las aplicaciones, y que representa una cantidad espec\u00edfica de memoria, n\u00facleos de procesamiento (cores) y otros recursos (disco, red), para procesar sus aplicaciones.</p> <p>Todas las tareas de las aplicaciones YARN se ejecutan en contenedores. Cada trabajo puede contener m\u00faltiples tareas y cada una de las tareas se ejecuta en su propio contenedor. Por otro lado, al iniciar un trabajo, YARN puede asignar a cada tarea un conjunto de contenedores dependiendo de la demanda de la aplicaci\u00f3n (al lanzar la tarea se le puede indicar el n\u00famero de contenedores que necesita) y a la disponibilidad de los contenedores que hay en el cl\u00faster en ese momento (si hay menos contenedores disponibles de los solicitados, YARN se encargar\u00e1 de aplicar las reglas de prioridad para asignar contenedores que a lo mejor est\u00e1n siendo usados por otras aplicaciones).</p> <p>Los contenedores se pueden configurar en cuanto al tama\u00f1o de memoria y la cantidad de elementos de procesamiento. La cantidad de tareas y, por lo tanto, la cantidad de aplicaciones de YARN que puede ejecutar en cualquier momento, est\u00e1 limitada por la cantidad de contenedores que tiene un cl\u00faster.</p> Figura 4.3_Yarn: Nodos y Servicios YARN. (Fuente: Ministerio de Educaci\u00f3n)  <p>Existe un nodo maestro, el ResourceManager, que coordina, asigna y controla la ejecuci\u00f3n de todas las tareas, y nodos worker que disponen de un servicio NodeManager, que monitoriza el estado de ejecuci\u00f3n de las tareas en el worker, as\u00ed como el estado de los recursos/contenedores en dicho nodo.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#11-resourcemanager","title":"1.1 ResourceManager","text":"<p>Este servicio ser\u00eda el equivalente al Namenode en HDFS, ya que es el maestro que controla la ejecuci\u00f3n de todas las tareas que est\u00e1n en ejecuci\u00f3n, o las solicitudes de ejecuci\u00f3n existentes.</p> <p>Cuando un cliente quiere ejecutar una aplicaci\u00f3n en YARN, se comunica con el ResourceManager, que ser\u00e1 el encargado de asignarle los recursos en base a las pol\u00edticas de prioridad asignadas y los recursos disponibles, distribuir la aplicaci\u00f3n (el ejecutable) por los diferentes nodos worker que realizar\u00e1n la ejecuci\u00f3n, controlar la ejecuci\u00f3n para detectar si ha habido una ca\u00edda de una de las tareas, para relanzarla en otro nodo, y liberar los recursos una vez la ejecuci\u00f3n haya finalizado.</p> <p>El ResourceManager tiene dos componentes principales::</p> <ul> <li> <p>El ApplicationMaster, que es el servicio que recibe las peticiones de ejecuci\u00f3n por parte de los clientes, distribuye las aplicaciones por los nodos worker, asigna los recursos, coordina la ejecuci\u00f3n de las tareas, monitoriza la ejecuci\u00f3n, solventa los fallos en las ejecuciones, y libera los recursos una vez las tareas han finalizado.</p> </li> <li> <p>El Scheduler, que es el servicio que asigna prioridades y establece los recursos/containers que disfrutar\u00e1 cada aplicaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red).</p> </li> </ul> Figura 4.4_Yarn: Arquitectura YARN. (Fuente: Apache Hadoop)"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#12-node-manager","title":"1.2 Node Manager","text":"<p>El servicio NodeManager se ejecuta en cada nodo worker y proporciona los recursos computacionales necesarios para las aplicaciones en forma de contenedores. Implementa Heartbeats para mantener informado del estado al Resource Manager. Realiza las siguientes funciones:</p> <ul> <li>Monitoriza y proporciona informaci\u00f3n sobre el consumo de recursos (CPU/memoria) por parte de los contenedores al ResourceManager.</li> <li>Env\u00eda mensajes para notificar al ResourceManager su actividad (no est\u00e1 ca\u00eddo) as\u00ed como la informaci\u00f3n sobre su estado a nivel de recursos.</li> <li>Supervisa el ciclo de vida de los contenedores de aplicaciones.</li> <li>Supervisa la ejecuci\u00f3n de las distintas tareas en contenedores y termina aquellas tareas que se han quedado bloqueadas.</li> <li>Almacena un log (fichero en HDFS) con todas las operaciones que se realizan en el nodo.</li> <li>Lanza procesos ApplicationMaster, que coordinan los trabajos para cada aplicaci\u00f3n.</li> </ul> <p>Info</p> <p>Los NodeManager, al igual que los Datanodes en HDFS, son tolerantes a fallos, por lo que en caso de ca\u00edda de alguno de ellos, el ResourceManager detectar\u00e1 que no funciona y redirigir\u00e1 la ejecuci\u00f3n de las aplicaciones al resto de nodos activos.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#13-aplicationmaster","title":"1.3 AplicationMaster","text":"<p>Existe un proceso ApplicationMaster por aplicaci\u00f3n. Este proceso se encarga de negociar con el ResourceManager los recursos necesarios para la ejecuci\u00f3n de las tareas de su aplicaci\u00f3n.</p> <p>El ApplicationMaster se ejecuta en uno de los nodos worker, para garantizar la escalabilidad de YARN, ya que si se ejecutaran todos los ApplicationMaster en el nodo maestro, junto con el ResourceManager, \u00e9ste ser\u00eda un cuello de botella para poder escalar o poder lanzar un gran n\u00famero de aplicaciones sobre el cl\u00faster.</p> <p>Asimismo, a diferencia del ResourceManager y los NodeManager, el ApplicationMaster es espec\u00edfico para una aplicaci\u00f3n por lo que, cuando la aplicaci\u00f3n finaliza, el proceso ApplicationMaster termina. En el caso de los servicios ResourceManager y NodeManager, siempre se est\u00e1n ejecutando aunque no haya aplicaciones activas en el cl\u00faster. Cada vez que se inicia una nueva aplicaci\u00f3n, ResourceManager asigna un contenedor que ejecuta ApplicationMaster en uno de los nodos del cl\u00faster.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#2-funcionamiento","title":"2. Funcionamiento","text":"<p>YARN, en concreto, el ResourceManager, es invocado por los clientes cuando quieren lanzar una aplicaci\u00f3n en el cl\u00faster para su ejecuci\u00f3n.</p> Figura 4.5_Yarn: Funcionamiento YARN. (Fuente: Ministerio de Educaci\u00f3n)  <p>La secuencia de ejecuci\u00f3n de una aplicaci\u00f3n es la siguiente:</p> <p>Note</p> <p>Como has visto tanto en YARN como en HDFS, en Hadoop se intenta que los nodos maestros hagan el menor n\u00famero de operaciones posibles para cada tarea, con el objetivo de poder escalar. Si por cada tarea a ejecutar necesitaran realizar un gran n\u00famero de tareas, la capacidad de escalar se ver\u00eda muy reducida, ya que los nodos master son los \u00fanicos que pueden realizar este tipo de tareas, siendo el l\u00edmite del sistema Hadoop la capacidad m\u00e1xima que puede tener un nodo maestro.</p> <ol> <li>El cliente se comunica con el ResourceManager para solicitarle la ejecuci\u00f3n de una aplicaci\u00f3n. En la llamada, le env\u00eda el c\u00f3digo/ejecutable de la aplicaci\u00f3n, as\u00ed como unos par\u00e1metros sobre los recursos necesarios para dicha ejecuci\u00f3n.</li> <li>El ApplicationMaster, tras chequear con el Scheduler la disponibilidad de recursos y su prioridad, pide al NodeManager de un nodo la creaci\u00f3n de un container que ejecutar\u00e1 el ApplicationMaster de la aplicaci\u00f3n.</li> <li>El NodeManager crea el contenedor y arranca su ejecuci\u00f3n.</li> <li>El ResourceManager se comunica con el ApplicationMaster para solicitarle los contenedores necesarios para la ejecuci\u00f3n de la aplicaci\u00f3n en caso necesario.</li> <li>El ApplicationMaster se comunica con los contenedores donde se est\u00e1 ejecutando distintas tareas para controlar su ejecuci\u00f3n, y va notificando el status de la ejecuci\u00f3n al ResourceManager.</li> <li>El NodeManager, asimismo, env\u00eda informaci\u00f3n al ResourceManager sobre el consumo de recursos y notificando que el nodo est\u00e1 activo.</li> </ol> <p>YARN soporta la reserva de recursos mediante el Reservation System, un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales (deadlines) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva.</p> <p>Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de YARN Federation. Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#3-configuracion","title":"3. Configuraci\u00f3n","text":"<p>Para configurar YARN, primero editaremos el archivo <code>yarn-site.xml</code> para indicar quien va a ser el nodo maestro, as\u00ed como el manager y la gesti\u00f3n para hacer el MapReduce:</p> <p>Tip</p> <p>Recuerda que los archivos de configuraci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> yarn-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.webapp.ui2.enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;  \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;bda-iesgrancapitan&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;   \n&lt;/configuration&gt;\n</code></pre> <p>Menci\u00f3n especial tiene la configuraci\u00f3n del par\u00e1metro <code>yarn.application.classpath</code>. Si no tenemos bien configurado este par\u00e1metro, no funcionar\u00e1 MapReducev2 ejecutado sobre Yarn, ya que no encontrar\u00e1 las librer\u00edas necesar\u00edas para su correcta ejecuci\u00f3n.</p> <p>Para obtener la ruta correcta del classpath de Hadoop ejecutamos la siguiente instrucci\u00f3n</p> <pre><code>echo `hadoop classpath`\n</code></pre> <p>Y es esta salida la que tenemos que poner como valor de la propiedad. En mi caso:</p> <p><pre><code>/opt/hadoop-3.3.6/etc/hadoop:/opt/hadoop-3.3.6/share/hadoop/common/lib/*:/opt/hadoop-3.3.6/share/hadoop/common/*:/opt/hadoop-3.3.6/share/hadoop/hdfs:/opt/hadoop-3.3.6/share/hadoop/hdfs/lib/*:/opt/hadoop-3.3.6/share/hadoop/hdfs/*:/opt/hadoop-3.3.6/share/hadoop/mapreduce/*:/opt/hadoop-3.3.6/share/hadoop/yarn:/opt/hadoop-3.3.6/share/hadoop/yarn/lib/*:/opt/hadoop-3.3.6/share/hadoop/yarn/*\n</code></pre> Tambi\u00e9n hay que tener en cuenta, que si te conectas desde otra m\u00e1quina, hay que dar permiso de acceso desde fuera de la m\u00e1quina con la siguiente propiedad:</p> yarn-site.xml<pre><code>    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n        &lt;value&gt;0.0.0.0:8088&lt;/value&gt;\n    &lt;/property&gt; \n</code></pre> <p>Por tanto, la configuraci\u00f3n final del archivo de configuraci\u00f3n <code>yarn-site.xml</code> ser\u00eda: </p> yarn-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.webapp.ui2.enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;  \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;bda-iesgrancapitan&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop-3.3.6/etc/hadoop:/opt/hadoop-3.3.6/share/hadoop/common/lib/*:/opt/hadoop-3.3.6/share/hadoop/common/*:/opt/hadoop-3.3.6/share/hadoop/hdfs:/opt/hadoop-3.3.6/share/hadoop/hdfs/lib/*:/opt/hadoop-3.3.6/share/hadoop/hdfs/*:/opt/hadoop-3.3.6/share/hadoop/mapreduce/*:/opt/hadoop-3.3.6/share/hadoop/yarn:/opt/hadoop-3.3.6/share/hadoop/yarn/lib/*:/opt/hadoop-3.3.6/share/hadoop/yarn/*&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n        &lt;value&gt;0.0.0.0:8088&lt;/value&gt;\n    &lt;/property&gt;      \n&lt;/configuration&gt;\n</code></pre> <p>Y finalmente el archivo <code>mapred-site.xml</code> para indicar que utilice YARN como framework MapReduce:</p> <p>mapred-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> Si prefieres usar la versi\u00f3n anterior y ejecutar MapReduce sobre HDFS, elimina esta propiedad</p> <p>Levantamos YARN</p> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop-3.3.4/sbin$ start-yarn.sh \nStarting resourcemanager\nStarting nodemanagers\n</code></pre> <p>Al habilitar en la configuraci\u00f3n la WebUI de Yarn, podemos acceder a su API Web en el puerto <code>8088</code>. Podemos acceder a 2 versiones de WebUI.</p> <ol> <li>Versi\u00f3n antigua de Hadoop Yarn. En mi caso</li> </ol> <pre><code>http://bda-iesgrancapitan:8088/cluster\n</code></pre> Figura 4.6_Yarn: WebUI YARN Antigua. (Fuente: Propia) <ol> <li>Versi\u00f3n actual de Hadoop Yarn. En mi caso:</li> </ol> <pre><code>http://bda-iesgrancapitan:8088/ui2 \n</code></pre> Figura 4.7_Yarn: WebUI YARN Nueva. (Fuente: Propia) <p>Ya podemos acceder a la interfaz de YARN. Adem\u00e1s de en el log, podemos observar aqu\u00ed todos los trabajos que se van realizando. Lo comprobaremos cuando lancemos alguna aplicaci\u00f3n MapReduce en la siguiente parte del tema</p>"},{"location":"UD4%20-%20Apache%20Hadoop/4_Hadoop_Yarn.html#4-ejercicio","title":"4. Ejercicio","text":"<p>Para comprobar la ejecuci\u00f3n de Yarn sobre MapReduce, vamos a realizar el mismo ejercicio de contar las palabras de \"El Quijote\" que hicimos en el apartado anterior de MapReduce y observemos los cambios.</p> <ol> <li>Ejecutamos de nuevo el programa <code>wordcount</code></li> <li>En esta ocasi\u00f3n, debe ejecutarse sobre Yarn</li> </ol> <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /bda/mapreduce/ejercicios/El_Quijote.txt /bda/mapreduce/ejercicios/salida_quijote_yarn\n</code></pre> <ol> <li>Vemos como a generado un job nuevo</li> </ol> <pre><code>2023-11-27 19:23:23,184 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at bda-iesgrancapitan/127.0.0.1:8032\n2023-11-27 19:23:24,179 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701109095543_0001\n2023-11-27 19:23:25,308 INFO input.FileInputFormat: Total input files to process : 1\n2023-11-27 19:23:25,860 INFO mapreduce.JobSubmitter: number of splits:1\n2023-11-27 19:23:26,161 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701109095543_0001\n2023-11-27 19:23:26,161 INFO mapreduce.JobSubmitter: Executing with tokens: []\n2023-11-27 19:23:26,352 INFO conf.Configuration: resource-types.xml not found\n2023-11-27 19:23:26,352 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n2023-11-27 19:23:26,859 INFO impl.YarnClientImpl: Submitted application application_1701109095543_0001\n2023-11-27 19:23:26,908 INFO mapreduce.Job: The url to track the job: http://hadoop-VirtualBox:8088/proxy/application_1701109095543_0001/\n2023-11-27 19:23:26,912 INFO mapreduce.Job: Running job: job_1701109095543_0001\n2023-11-27 19:23:39,204 INFO mapreduce.Job: Job job_1701109095543_0001 running in uber mode : false\n2023-11-27 19:23:39,211 INFO mapreduce.Job:  map 0% reduce 0%\n2023-11-27 19:23:48,367 INFO mapreduce.Job:  map 100% reduce 0%\n2023-11-27 19:23:55,455 INFO mapreduce.Job:  map 100% reduce 100%\n2023-11-27 19:23:56,495 INFO mapreduce.Job: Job job_1701109095543_0001 completed successfully\n2023-11-27 19:23:56,589 INFO mapreduce.Job: Counters: 54\n    File System Counters\n        FILE: Number of bytes read=605631\n        FILE: Number of bytes written=1763985\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=2161310\n        HDFS: Number of bytes written=448985\n        HDFS: Number of read operations=8\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=2\n        HDFS: Number of bytes read erasure-coded=0\n    Job Counters \n        Launched map tasks=1\n        Launched reduce tasks=1\n        Data-local map tasks=1\n        Total time spent by all maps in occupied slots (ms)=6062\n        Total time spent by all reduces in occupied slots (ms)=4465\n        Total time spent by all map tasks (ms)=6062\n        Total time spent by all reduce tasks (ms)=4465\n        Total vcore-milliseconds taken by all map tasks=6062\n        Total vcore-milliseconds taken by all reduce tasks=4465\n        Total megabyte-milliseconds taken by all map tasks=6207488\n        Total megabyte-milliseconds taken by all reduce tasks=4572160\n    Map-Reduce Framework\n        Map input records=37863\n        Map output records=384275\n        Map output bytes=3688770\n        Map output materialized bytes=605631\n        Input split bytes=135\n        Combine input records=384275\n        Combine output records=40067\n        Reduce input groups=40067\n        Reduce shuffle bytes=605631\n        Reduce input records=40067\n        Reduce output records=40067\n        Spilled Records=80134\n        Shuffled Maps =1\n        Failed Shuffles=0\n        Merged Map outputs=1\n        GC time elapsed (ms)=113\n        CPU time spent (ms)=1740\n        Physical memory (bytes) snapshot=368336896\n        Virtual memory (bytes) snapshot=5014446080\n        Total committed heap usage (bytes)=230821888\n        Peak Map Physical memory (bytes)=237412352\n        Peak Map Virtual memory (bytes)=2503553024\n        Peak Reduce Physical memory (bytes)=130924544\n        Peak Reduce Virtual memory (bytes)=2510893056\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Input Format Counters \n        Bytes Read=2161175\n    File Output Format Counters \n        Bytes Written=448985\n</code></pre> <ol> <li>Observamos como la WebUI Yarn muestra la ejecuci\u00f3n del job</li> </ol> Figura 4.8_Yarn: WebUI YARN Wordcount. (Fuente: Propia) <ol> <li>Detalle de la ejecuci\u00f3n del job wordcount</li> </ol> Figura 4.8_Yarn: WebUI YARN detalle Wordcount job. (Fuente: Propia) <ol> <li>Leemos el fichero de salida. Aqu\u00ed est\u00e1n listados todas las palabras de El Quijote y cuantas veces aparece cada palabra</li> </ol> <pre><code>hdfs dfs -cat /bda/mapreduce/ejercicios/salida_quijote_yarn/part-r-00000\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html","title":"UD 4 - Apache Hadoop - Cluster","text":"<p>En este recurso vamos a explicar como se instala y configura un cluster con Apache Hadoop</p>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#1-pre-requisitos","title":"1. Pre-requisitos","text":"<p>Debemos tener instalado VirtualBox. </p>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#11-configurar-red-nat","title":"1.1 Configurar Red NAT","text":"<p>Para crear nuestro cluster, vamos a configurar un red NAT para que los nodos tenga conexi\u00f3n entre ellos y salida a Internet a trav\u00e9s del Host</p> <p>Success</p> <p>Hemos elegido esta configuraci\u00f3n de red por varios motivos.</p> <ol> <li> <p>Puedas usar tu cluster en tu port\u00e1til independientemente de la red o lugar en la que est\u00e9s conectado</p> </li> <li> <p>Ser\u00eda m\u00e1s sencillo hacerla en modo bridge, pero no tenemos IPs suficientes debido al alto n\u00famero de alumnado, y habr\u00eda que configurar las IPs cada vez que te conectes a una red distinta a la de clase.</p> </li> <li> <p>En todo caso, si decides optar por esta opci\u00f3n, s\u00f3lo tienes que adaptar los pasos a tu configuraci\u00f3n de red. Ser\u00eda algo m\u00e1s sencilla.</p> </li> </ol> <p>Para ello, explicamos con una imagen como funciona VirtualBox en este tipo de configuraci\u00f3n de red</p> Figura 1 Cluster Hadoop: Red NAT Virtualbox. (Fuente: medium.com/@sidlors) <p>Puedes observar que podemos configurar nuestra propia subred, dentro de las cuales, hay 2 ips que VirtualBox asigna est\u00e1ticas dentro de la red: la puerta de enlace(primera de la red) y el DHCP (tercera de la red). Para m\u00e1s informaci\u00f3n, consulta la documentaci\u00f3n oficial de VirtualBox</p> <p>Teniendo en cuenta esto, vamos a configurar nuestra propia subred, que ser\u00e1 la <code>192.168.11.0/24</code></p> <ol> <li> <p>Abrimos la configuraci\u00f3n de VirtualBox para crear una nueva red NAT en <code>Preferencias -&gt; Redes -&gt; Redes NAT -&gt; Agregar nueva red NAT</code></p> </li> <li> <p>Creamos una nueva Red NAT llamada <code>BDA</code>. Usaremos la red <code>192.168.11.0/24</code> con DHCP Deshabilitado. Puedes elegir cualquier otra si quieres.</p> </li> </ol> Figura 2 Cluster Hadoop: Red NAT. (Fuente: Propia) <ol> <li>Una vez configurada la Red NAT, podemos empezar a instalar y configurar el cluster.</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#12-configuracion-de-las-maquinas","title":"1.2 Configuraci\u00f3n de las m\u00e1quinas","text":"<p>Vamos a crear una primera m\u00e1quina que despu\u00e9s clonaremos y cambiaremos las configuraciones necesarias. Las m\u00e1quinas tendr\u00e1n la siguiente configuraci\u00f3n (siempre que sea posible):</p> <ul> <li>Nombre: master (las otras 3 m\u00e1quinas se llamar\u00e1n nodo1, nodo2 y nodo3)</li> <li>RAM: 4GB (yo por ejemplo, no puedo darle m\u00e1s de 3GB)</li> <li>N\u00facleos: 2</li> <li>Disco duro: 50GB</li> <li>Interfaz de red: Red NAT (\"BDA\"): 192.0.11.10 (las s de las otras 3 m\u00e1quinas ser\u00e1n 192.0.11.11, 192.0.11.12 y 192.0.11.13)</li> <li>Sistema operativo: Ubuntu server 22.04</li> <li>Usuario: hadoop</li> </ul>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#13-configuracion-de-red","title":"1.3 Configuraci\u00f3n de red","text":"<p>Habiendo entendido correctamente lo explicado en los puntos anteriores,podemos configurar la interfaz de red de forma manual, con la siguiente configuraci\u00f3n:</p> <ul> <li>Subred: <code>192.168.11.0/24</code></li> <li>Direci\u00f3n Ip: <code>192.168.11.10</code></li> <li>Puerta de enlace: <code>192.168.11.1</code> </li> <li>DNS: <code>8.8.8.8,1.1.1.1</code></li> </ul> Figura 3 Cluster Hadoop: Interfaz de Red Nodo Master. (Fuente: Propia)"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#14-acceso-a-las-maquinas","title":"1.4 Acceso a las m\u00e1quinas","text":"<p>Para un manejo m\u00e1s c\u00f3modo e intuitivo de las m\u00e1quinas, podemos preparar cada nodo para que sea accesible desde nuestro anfitri\u00f3n y conectarnos mediante ssh a cada una de las m\u00e1quinas.</p> <p>Para ello debemos a\u00f1adir otro interfaz de red en modo \"Adaptador s\u00f3lo anfitri\u00f3n\"  </p> <p>Warning</p> <p>Si tienes como anfitri\u00f3n un equipo linux y no puedes configurar un \"adaptador s\u00f3lo anfitri\u00f3n\" debes crearlo y levantarlo manualmente</p> <pre><code>sudo vboxmanage hostonlyif create\nsudo ifconfig vboxnet0 up\n</code></pre> <p>Se creara una interfaz de red 192.168.56.0/24. Puedes cambiarla y configurarla tambi\u00e9n manualmente</p> <pre><code>sudo vboxmanage hostonlyif ipconfig vboxnet0 --ip 10.0.2.18\n</code></pre> <p>Puedes crear todas las que quieras por este proceso. Si se actualiza el kernel de Linux o la versi\u00f3n de Virtualbox se debe repetir este proceso. Para mas informaci\u00f3n accede a la documentaci\u00f3n oficial de VirtualBox</p> <p>Ahora, una vez dentro de la m\u00e1quina virtual, no olvides configurar la ip de esta subred.</p> <ol> <li>Abrimos el archivo <code>/etc/netplan/00-instaler-config.yaml</code> y a\u00f1adir la configuraci\u00f3n. Por ejemplo, terminado en la misma ip que la Red NAT</li> <li>Esta ser\u00eda la configuraci\u00f3n completa</li> </ol> <p>00-instaler-config.yaml<pre><code>network:\nethernets:\n    enp3s0:\n        addresses:\n        - 192.168.11.10/24\n        nameservers:\n            addresses: [8.8.8.8, 1.1.1.1]\n        routes:\n        - to: default\n          via: 192.168.11.1\n    enp8s0:\n        addresses:\n        - 192.168.56.10/24\nversion: 2\n</code></pre> 3. Aplicamos la nueva configuraci\u00f3n</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#2-nodo-master","title":"2. Nodo Master","text":"<p>Creamos en VirtualBox el nodo <code>master</code> configurando el Interfaz de red como Red NAT (opcionalmente otra interfaz red s\u00f3lo anfitri\u00f3n) y elegimos la que acabamos de crear <code>BDA</code></p>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#21-instalacion","title":"2.1 Instalaci\u00f3n","text":"<ol> <li>Java\u2122 debe ser instalado. Las versiones de Java recomendadas se encuentran descritas en HadoopJavaVersions.</li> </ol> <pre><code>sudo apt-get install openjdk-8-jdk\n/usr/bin/java -version\n</code></pre> <ol> <li>ssh debe estar instalado y sshd debe estar ejecut\u00e1ndose para usar las secuencias de comandos de Hadoop que administran los demonios ssh remotos de Hadoop, ya que vamos a usar las secuencias de comandos de inicio y detecci\u00f3n opcionales. Si no lo has hecho durante la instalaci\u00f3n, hazlo ahora.</li> </ol> <pre><code>sudo apt-get install ssh\n</code></pre> <ol> <li>Para obtener la distribuci\u00f3n de Apache Hadoop, descarga la versi\u00f3n estable m\u00e1s reciente desde Apache Download Mirrors</li> </ol> <pre><code>wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n</code></pre> <ol> <li>Una vez descargado, desempaquetamos el archivo descargado con el comando tar y entra dentro de la carpeta:</li> </ol> <pre><code>sudo tar -xzf hadoop-3.3.6.tar.gz -C /opt\ncd /opt/hadoop-3.3.6\n</code></pre> <ol> <li>Edita el siguiente archivo <code>etc/hadoop/hadoop-env.sh</code> para definir la variable de entorno de Java y a\u00f1\u00e1dela.</li> </ol> <pre><code># Technically, the only required environment variable is JAVA_HOME.\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n</code></pre> <ol> <li>Para poder usar los comandos de HDFS en cualquier lugar del sistema, sin tener que hacerlo desde el directorio de Hadoop (por ejemplo <code>/opt/hadoop-3.3.6/bin</code>), creamos las variables de entorno y a\u00f1adimos al PATH. Para ello abrimos el archivo <code>~/.bashrc</code> y a\u00f1adimos al final el siguiente c\u00f3digo y ejecuta el comando <code>source ~/.bashrc</code></li> </ol> ~/.bashrc<pre><code>export HADOOP_HOME=/opt/hadoop-3.3.6\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li>Ejecuta el siguiente comando. Si no da error, podemos continuar</li> </ol> <pre><code>hadoop version\n</code></pre> <p>Nos debe salir la versi\u00f3n de hadoop</p> <pre><code>Hadoop 3.3.6\nSource code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\nCompiled by ubuntu on 2023-06-18T08:22Z\nCompiled on platform linux-x86_64\nCompiled with protoc 3.7.1\nFrom source with checksum 5652179ad55f76cb287d9c633bb53bbd\nThis command was run using /opt/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#22-configuracion","title":"2.2 Configuraci\u00f3n","text":"<p>Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> <ol> <li>El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo <code>core-site.xml</code>. En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos (namenode) de la arquitectura. Pod\u00e9is sustituir el nombre del dominio <code>bda-iesgrancapitan</code> por el que quer\u00e1is. En mi caso sera <code>cluster-bda</code></li> </ol> core-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://cluster-bda:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>El siguiente paso es configurar el archivo <code>hdfs-site.xml</code> donde se indica tanto el factor de replica como la ruta donde se almacenan tanto los metadatos (namenode) como los datos en s\u00ed (datanode). Aqu\u00ed puedes consultar todos los par\u00e1metros por defecto susceptibles de cambio se encuentran en este recurso</li> </ol> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Crea los directorios de <code>hadoop-data</code> configurados anteriormente en <code>hdfs-site.xml</code> para cuando ejecutemos hadoop y configura los permisos oportunos.</li> </ol> <pre><code>sudo mkdir -p /opt/hadoop\nsudo chown -R hadoop:hadoop /opt/hadoop\n</code></pre> <ol> <li>Configuramos MapReduce</li> </ol> mapred-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Configuramos Yarn</li> </ol> <p>Primero obtenemos la ruta correcta del classpath de Hadoop ejecutamos la siguiente instrucci\u00f3n</p> <pre><code>echo `hadoop classpath`\n</code></pre> <p>Y es esta salida la que tenemos que poner como valor de la propiedad. En mi caso:</p> <pre><code>/opt/hadoop-3.3.6/etc/hadoop:/opt/hadoop-3.3.6/share/hadoop/common/lib/*:/opt/hadoop-3.3.6/share/hadoop/common/*:/opt/hadoop-3.3.6/share/hadoop/hdfs:/opt/hadoop-3.3.6/share/hadoop/hdfs/lib/*:/opt/hadoop-3.3.6/share/hadoop/hdfs/*:/opt/hadoop-3.3.6/share/hadoop/mapreduce/*:/opt/hadoop-3.3.6/share/hadoop/yarn:/opt/hadoop-3.3.6/share/hadoop/yarn/lib/*:/opt/hadoop-3.3.6/share/hadoop/yarn/*\n</code></pre> <p>Por tanto la configuraci\u00f3n final ser\u00e1</p> yarn-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.webapp.ui2.enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;  \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;cluster-bda&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n        &lt;value&gt;/opt/hadoop-3.3.6/etc/hadoop:/opt/hadoop-3.3.6/share/hadoop/common/lib/*:/opt/hadoop-3.3.6/share/hadoop/common/*:/opt/hadoop-3.3.6/share/hadoop/hdfs:/opt/hadoop-3.3.6/share/hadoop/hdfs/lib/*:/opt/hadoop-3.3.6/share/hadoop/hdfs/*:/opt/hadoop-3.3.6/share/hadoop/mapreduce/*:/opt/hadoop-3.3.6/share/hadoop/yarn:/opt/hadoop-3.3.6/share/hadoop/yarn/lib/*:/opt/hadoop-3.3.6/share/hadoop/yarn/*&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n        &lt;value&gt;0.0.0.0:8088&lt;/value&gt;\n    &lt;/property&gt;      \n&lt;/configuration&gt;\n</code></pre> <ol> <li>A\u00f1ade a <code>/etc/hosts</code>el nombre de tu dominio indicado en <code>core-site.xml</code> para que no te de error de resoluci\u00f3n de nombres. En mi caso a\u00f1ado la siguiente linea:</li> </ol> <pre><code>192.168.11.10   cluster-bda\n192.168.11.10   master\n192.168.11.11   nodo1\n192.168.11.12   nodo2\n192.168.11.13   nodo3\n</code></pre> <ol> <li>Reiniciamos el servicio</li> </ol> <pre><code>sudo systemctl restart systemd-resolved.service\n</code></pre> <ol> <li>Generamos las claves ssh:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Comprobamos que podemos acceder por ssh</li> </ol> <pre><code>ssh cluter-bda\n#exit\nssh master\n#exit\n</code></pre> <ol> <li>Configura el archivo <code>$HADOOP_HOME/etc/hadoop/workers</code> que le indica a hadoop los nodos que van a actuar como workers.</li> </ol> workers<pre><code>nodo1\nnodo2\nnodo3\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#23-ejecucion","title":"2.3 Ejecuci\u00f3n","text":"<ol> <li>Ejecuta el siguiente comando</li> </ol> <pre><code>hdfs namenode -format\n</code></pre> <ol> <li> <p>Comprobamos la correcta configuraci\u00f3n</p> </li> <li> <p>Iniciando el demonio Namenode y Datanode</p> </li> </ol> <pre><code>start-dfs.sh\n</code></pre> <ol> <li>Iniciando Yarn</li> </ol> <pre><code>start-yarn.sh\n</code></pre> <ol> <li>Nos debe levantar el servicio correctamente, indicando que no puede conectarse a los nodos 1, 2 y 3, ya que todav\u00eda no los hemos creado.</li> </ol> <pre><code>nodo3: ssh: Could not resolve hostname nodo3: Temporary failure in name resolution\nnodo2: ssh: Could not resolve hostname nodo2: Temporary failure in name resolution\nnodo1: ssh: Could not resolve hostname nodo1: Temporary failure in name resolution\n</code></pre> <ol> <li> <p>Accede desde el navegador del anfitri\u00f3n con la ip configurada en la Interfaz de red solo anfitri\u00f3n. En mi caso a <code>http://192.168.56.11:9870/</code> para acceder al interfaz web de HDFS</p> </li> <li> <p>Accede tambi\u00e9n a la WebUI de Yarn <code>http://192.168.56.11:8088/ui2</code></p> </li> <li> <p>Paramos hadoop</p> </li> </ol> <pre><code>stop-all.sh\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#3-nodos","title":"3. Nodos","text":""},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#31-clonacion-de-la-maquina-master","title":"3.1 Clonaci\u00f3n de la m\u00e1quina master","text":"<ol> <li>Paramos la m\u00e1quina <code>master</code></li> <li>Clonamos la m\u00e1quina 3 veces para crear los 3 nodos del cluster.</li> <li>A la hora de clonar, genera nuevas direcciones MAC para los interfaces de red</li> <li>Clonaci\u00f3n completa</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#32-configuracion-nodos","title":"3.2 Configuraci\u00f3n nodos","text":"<p>Tenemos que cambiar algunas configuraciones en nuestros nodos: - El nombre del hostname - Configuraci\u00f3n de red</p> <ol> <li>Cambiamos el nombre del host:</li> </ol> <pre><code>sudo hostnamectl set-hostname nodo1\n</code></pre> <ol> <li> <p>Actualizamos <code>/etc/hosts</code> y sustituimos <code>master</code> por <code>nodo1</code></p> </li> <li> <p>Accedemos a la configuraci\u00f3n de red a trav\u00e9s de netplan en el fichero <code>/etc/netplan/00-instaler-config.yaml</code> y cambiamos la configuraci\u00f3n de las 2 interfaces de red con las ips correspondientes</p> </li> <li> <p>Esta ser\u00eda la configuraci\u00f3n completa</p> </li> </ol> 00-instaler-config.yaml<pre><code>network:\nethernets:\n    enp3s0:\n        addresses:\n        - 192.168.11.11/24\n        nameservers:\n            addresses: [8.8.8.8, 1.1.1.1]\n        routes:\n        - to: default\n            via: 192.168.11.1\n    enp8s0:\n        addresses:\n        - 192.168.56.11/24\nversion: 2\n</code></pre> <ol> <li>Aplicamos la nueva configuraci\u00f3n</li> </ol> <pre><code>sudo netplan apply\n</code></pre> <ol> <li>Reiniciamos</li> </ol> <pre><code>reboot\n</code></pre> <ol> <li>Realizamos las mismas operaciones en el <code>nodo2</code> y <code>nodo3</code> con sus correspondientes IPs</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#4-cluster","title":"4. Cluster","text":""},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#41-configuracion-ssh","title":"4.1 Configuraci\u00f3n ssh","text":"<p>Por \u00faltimo, nos queda crear las correspondientes claves ssh de los nodos para que exista una correcta comunicaci\u00f3n entre todos los nodos del cluster.</p> <ol> <li> <p>Todos los nodos del cluster deben tener todas las claves p\u00fablicas del resto de nodos, para su correcto funcionamiento. Por tanto, vamos a ir generando una por una en cada nodo del cluster y las vamos a\u00f1adiendo a medida que recorremos el cluster. Finalmente, el fichero resultante en el \u00faltimo nodo del cluster contendr\u00e1 todas las claves. Este deber\u00e1 ser copiado en todos los nodos los cluster. Vamos a ver el paso a paso para entenderlo mejor.</p> </li> <li> <p>Entramos en el <code>master</code> y generamos las claves ssh:</p> </li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <ol> <li>La copiamos en el siguiente nodo. En nuestro caso <code>nodo1</code></li> </ol> <pre><code>scp ~/.ssh/authorized_keys hadoop@nodo1:~/.ssh/authorized_keys\n</code></pre> <ol> <li>Accedemos al nodo <code>nodo1</code> por ssh. Como <code>nodo1</code> ya conoce la clave de master, no nos pide contrase\u00f1a. As\u00ed tambi\u00e9n vamos comprobando que la configuraci\u00f3n es correcta en cada nodo.</li> </ol> <pre><code>ssh nodo1\n</code></pre> <ol> <li>Generamos las claves ssh del <code>nodo1</code>:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Comprobamos que tenemos todas las claves generadas hasta ahora</li> </ol> <pre><code>cat ~/.ssh/authorized_keys\n</code></pre> <pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCTyaPCHbtoUIIu/L+9ykq65descImdc1eXv7WUkDL803hDsedzBiMYW4UrNjr2ZhJAPjiUtnqM3xnlYdgJ2T5sWdSO45Dh3IIRufSCOJNvWG0PMM2klAokhqoW2Vhbh50z/2AXdRU00cMd+uE1ETLBg8kvTc+RDc94ctzdlG9hQpe6psRJ2xoG1kJ8DpvXExrynTgTPbIlGiU8K3Z3+fh5WHOwJOjXu1zNMMTNsKRBf39zW0gkyZOcRpjgg4VO2T2esbXffihRLWgCnYjQf683ctoS8nZEigoQkMV8EgRtcvCnBYYmPy5VL155DqN3c1luR0rOOLxB3WSfAm+rod/jd1SLU1nQzhRryFfWu5YEns+hSO7gZG4RzNtfKVVO78xubiprab2gg2ySIvQH4qE8ytqRq44zn7P/KvkenXRk/PRZaBsSXTejLxORTyo0iR4ssWsoMJDPqmGDQ9xPVobROG0ZDiQ2TWAE/B1ww463a2HHXh58zCcrAc+MW5hJNv0= hadoop@master\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC8Y4ksoigj130KJcNRoVZTJ9ehdWkXyb9VMlTi6YZ9WHdvDafJtW2oILCm9PTvOlAjaCJ7D8na5b/CC7BYk2lNAKgw+wQBG9TekCV+HZHcFrXuw6UkdyqQ29rujgyYBqg7dZHtJk8lMBJXY7N/fkSk10uSTNQ3AlqBa6vR23CWVMFhHrkM314/GFcP2aSLoFnIVEm4eyWSPL+5dFchM74EuxJpgP5NqfLrR/nTLQpXn8FJDjj5cxlCGSA/HV82SxJk0TAw4tPH3+q4zuLTzTMkUk8flUPWcon626vt+5wS0cdHo4A0UPsYs10MQ0YiEp00FUFWoQeTVGKvDkRwuMXHjkM+axIiqY9CPIEKT+5O8H/pIJyH0onGOpz4sUhiyj/UzQz4B5J/ky/CAA+TTy2ZWjSX9HN9Sr/0cyVlrjNcIZSpck+XqielhfuRjudEzt9emFCPI1ylRhrE4xt5XfCUSTPp8TnwSrzxGJuIHNbFTaVQ/g2CpkDoBe3xaJ/UaHU= hadoop@nodo1\n</code></pre> <ol> <li>La copiamos en el siguiente nodo. En nuestro caso <code>nodo2</code></li> </ol> <pre><code>scp ~/.ssh/authorized_keys hadoop@nodo2:~/.ssh/authorized_keys\n</code></pre> <ol> <li>Accedemos al nodo <code>node2</code> por ssh.</li> </ol> <pre><code>ssh nodo2\n</code></pre> <ol> <li>Generamos las claves ssh del <code>nodo2</code>:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Comprobamos que tenemos todas las claves generadas hasta ahora</li> </ol> <pre><code>cat ~/.ssh/authorized_keys\n</code></pre> <ol> <li>La copiamos en el siguiente nodo. En nuestro caso <code>nodo3</code></li> </ol> <pre><code>scp ~/.ssh/authorized_keys hadoop@nodo3:~/.ssh/authorized_keys\n</code></pre> <ol> <li>Accedemos al nodo <code>nodo3</code> por ssh.</li> </ol> <pre><code>ssh nodo3\n</code></pre> <ol> <li>Generamos las claves ssh del <code>nodo2</code>:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Comprobamos que tenemos todas las claves generadas hasta ahora</li> </ol> <pre><code>cat ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Una vez terminado todo nuestro cluster, la copiamos en todos los nodos.</li> </ol> <pre><code>scp ~/.ssh/authorized_keys hadoop@nodo2:~/.ssh/authorized_keys\nscp ~/.ssh/authorized_keys hadoop@nodo1:~/.ssh/authorized_keys\nscp ~/.ssh/authorized_keys hadoop@master:~/.ssh/authorized_keys\n</code></pre> <ol> <li>Por \u00faltimo, entramos a cada uno de los nodos y cambiamos los permisos de <code>~/.ssh/authorized_keys</code>.</li> </ol> <pre><code>chmod 0600 ~/.ssh/authorized_keys\n</code></pre> <ol> <li>Cerramos todas las conexiones ssh</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#42-preparando-los-nodos-del-cluster","title":"4.2 Preparando los nodos del cluster","text":"<ol> <li> <p>Ahora nos queda hacer una correcta gesti\u00f3n de las carpetas de namenode y datanode. Recuerda que los archivos de datanode deben estar en los workers</p> </li> <li> <p>Por tanto, en cada uno de los nodos workers(<code>nodo1</code>,<code>nodo2</code>,<code>nodo3</code>), borramos la carpeta <code>namenode</code> y el interior de la carpeta <code>datanode</code>, donde tenemos el directorio <code>current</code></p> </li> </ol> <pre><code>sudo rm -rf /opt/hadoop/hadoop_data/hdfs/namenode\nsudo rm -rf /opt/hadoop/hadoop_data/hdfs/datanode/current\n</code></pre> <ol> <li>Siguiendo la l\u00f3gica, en el nodo <code>master</code> eliminamos el directorio <code>datanode</code></li> </ol> <pre><code>sudo rm -rf /opt/hadoop/hadoop_data/hdfs/datanode\n</code></pre> <ol> <li>Desde el nodo <code>master</code>, damos de nuevo formato a HDFS</li> </ol> <pre><code>hdfs namenode -format\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/5_Cluster_Hadoop.html#43-levantamos-el-cluster","title":"4.3 Levantamos el cluster","text":"<ol> <li>Iniciando el demonio Namenode y Datanode</li> </ol> <pre><code>start-dfs.sh\n</code></pre> <ol> <li>Vemos como arranca <code>namenode</code> y <code>secondarynamenode</code> en el nodo master y el datanode</li> </ol> <pre><code>Starting namenodes on [cluster-bda]\nStarting datanodes\nStarting secondary namenodes [master]\n</code></pre> <ol> <li>Iniciamos Yarn</li> </ol> <pre><code>start-yarn.sh\n</code></pre> <ol> <li>Se inician el <code>resourcemanager</code> y el <code>nodemanagers</code></li> </ol> <pre><code>Starting resourcemanager\nStarting nodemanagers\n</code></pre> <ol> <li>Arrancamos tambi\u00e9n el servidor de historial de trabajos (JobHistory Server) en un cl\u00faster de Hadoop</li> </ol> <pre><code>mapred --daemon start historyserver\n</code></pre> <ol> <li>Comprobamos con <code>jps</code>. Vemos que se ejecutan los servicios que se tienen que ejecutar en el nodo <code>master</code></li> </ol> <pre><code>2946 ResourceManager\n2741 SecondaryNameNode\n2553 NameNode\n3324 Jps\n3263 JobHistoryServer\n</code></pre> <ol> <li>Comprobamos con <code>jps</code> en cualquier nodo worker(<code>nodo1</code>,<code>nodo2</code>,<code>nodo3</code>). Tambi\u00e9n vemos que se ejecutan los servicios que se tienen que ejecutar en los nodos worker.</li> </ol> <pre><code>2790 Jps\n2554 DataNode\n2702 NodeManager\n</code></pre> <ol> <li>Comprobamos con nuestra interfaz web de HDFS</li> </ol> Figura 4 Cluster Hadoop: Interfaz Web Datanodes Cluster. (Fuente: Propia) <ol> <li>Ya podemos utilizar el cluster. Compru\u00e9balo realizando alguno de los ejemplos desarrollados en los puntos anteriores</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html","title":"Big Data Aplicado","text":""},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#ud-6-apache-hadoop","title":"UD 6 - Apache Hadoop","text":""},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#instalacion-apache-hadoop-ubuntu-2204-hue","title":"Instalaci\u00f3n Apache Hadoop Ubuntu 22.04 - HUE","text":"<p>HUE (Hadoop User Experience) es una interfaz de usuario web para la gesti\u00f3n de Hadoop, adem\u00e1s de una plataforma para construir aplicaciones a medida sobre esta librer\u00eda UI.</p> <p>Para simplificar el proceso de creaci\u00f3n, mantenimiento y ejecuci\u00f3n de muchos tipos de trabajos de Hadoop, Hue (Hadoop User Experience) ofrece una GUI web para los usuarios de Hadoop. B\u00e1sicamente, se compone de varias aplicaciones que interact\u00faan con los componentes de Hadoop, y tambi\u00e9n tiene un SDK abierto para permitir la creaci\u00f3n de nuevas aplicaciones.</p> <p>Es un proyecto de Cloudera que se abri\u00f3 como proyecto opensource. Fue desarrollado ya que muchas de las operaciones en el ecosistema de Hadoop se operan a trav\u00e9s de una interfaz de l\u00ednea de comandos solamente.</p> <p>Instalaci\u00f3n https://docs.gethue.com/administrator/installation/</p> <p>https://docs.gethue.com/administrator/installation/dependencies/ https://docs.gethue.com/administrator/installation/install/ https://docs.gethue.com/administrator/installation/cloud/</p> <p>Elige el release. En nuestro caso la [4.10.0] (https://docs.gethue.com/releases/release-notes-4.10.0/)</p> <p>Descarga HUE tallbar</p> <pre><code>wget https://cdn.gethue.com/downloads/hue-4.11.0.tgz\ntar -xzf hue-4.11.0.tgz\n</code></pre> <p>https://docs.gethue.com/releases/</p> <p>Configuraci\u00f3n</p> <p>Ay\u00fadate tambi\u00e9n de la informaci\u00f3n de la p\u00e1gina oficial</p> <p>Note</p> <p>Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida, porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#instalacion","title":"Instalaci\u00f3n","text":"<ol> <li>Java\u2122 debe ser instalado. Las versiones de Java recomendadas se encuentran descritas en HadoopJavaVersions.</li> </ol> <pre><code>sudo apt-get install openjdk-11-jdk\n/usr/bin/java\n</code></pre> <ol> <li>ssh debe estar instalado y sshd debe estar ejecut\u00e1ndose para usar las secuencias de comandos de Hadoop que administran los demonios ssh remotos de Hadoop, ya que vamos a usar las secuencias de comandos de inicio y detecci\u00f3n opcionales.</li> </ol> <pre><code>sudo apt-get install ssh\n</code></pre> <ol> <li> <p>Abre la terminal en el directorio <code>$HOME</code></p> </li> <li> <p>Para obtener la distribuci\u00f3n de Apache Haddop, descarga la versi\u00f3n estable m\u00e1s reciente desde Apache Download Mirrors</p> </li> </ol> <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n</code></pre> <ol> <li>Una vez descargado, desempaquetamos el archivo descargado con el comando tar y entra dentro de la carpeta:</li> </ol> <pre><code>tar -xzf hadoop-3.3.4.tar.gz\ncd hadoop-3.3.4\n</code></pre> <ol> <li>Edita el siguiente archivo <code>etc/hadoop/hadoop-env.sh</code> para definir la variable de entorno de Java y a\u00f1adela.</li> </ol> <pre><code># Technically, the only required environment variable is JAVA_HOME.\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n</code></pre> <ol> <li>Para poder usar los comandos de HDFS en cualquier lugar del sistema, sin tener que hacerlo desde el directorio de Hadoop (por ejemplo /home/hadoop/hadoop-3.3.4/bin), creamos las variables de entorno y a\u00f1adimos al PATH. Para ello abrimos el archivo <code>~/.bashrc</code> y a\u00f1adimos al final el siguiente c\u00f3digo y ejecuta el comando <code>source ~/.bashrc</code></li> </ol> <pre><code>export HADOOP_HOME=$HOME/hadoop-3.3.4\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li>Ejecuta el siguiente comando. Si no da error, podemos continuar</li> </ol> <pre><code>bin/hadoop\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#configuracion-pseudo-distributed-operation","title":"Configuraci\u00f3n (Pseudo-Distributed Operation)","text":"<p>Hadoop se puede ejecutar en un solo nodo en un modo pseudo-distributed donde cada demonio de Hadoop se ejecuta en un proceso Java separado.</p> <p>Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> <ol> <li>El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo <code>core-site.xml</code>. En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos (namenode) de la arquitectura.</li> </ol> core-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://bda-iesgrancapitan:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>El siguiente paso es configurar el archivo <code>hdfs-site.xml</code> donde se indica tanto el factor de replica como la ruta donde se almacenan tanto los metadatos (namenode) como los datos en s\u00ed (datanode):</li> </ol> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Opcional: Si quieres especificar la ruta donde se almacenan los metadatos(namenode) y los datos(datanode) donde el propio hadoop los configura por defecto puedes hacerlo cambiando dichos par\u00e1metroscorrespondintes. Todos lo par\u00e1metros por defecto subceptibles de cambio se encuentran en este recurso</li> </ol> <p>Note</p> <p>Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode.</p> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/home/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;/home/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Comprobamos que podemos entrar por ssh al localhost sin un passphrase:</li> </ol> <pre><code>ssh localhost\nexit //Si hemos podido acceder\n</code></pre> <ol> <li>Si no puedes, ejecuta los siguientes comandos:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n</code></pre> <ol> <li>A\u00f1ade a <code>/etc/hosts</code>el nombre de tu dominio indicado en <code>core-site.xml</code> para que no te de error de resoluci\u00f3n de nombres. En mi caso a\u00f1ado la siguiente linea y reinicia el servicio:</li> </ol> <pre><code>127.0.0.1   bda-iesgrancapitan\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#ejecucion","title":"Ejecuci\u00f3n","text":"<ol> <li>Ejecuta el siguiente comando</li> </ol> <pre><code>bin/hdfs namenode -format\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>WARNING: /home/hadoop/hadoop-3.3.4/logs does not exist. Creating.\n2023-01-12 13:44:51,867 INFO namenode.NameNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-VirtualBox/127.0.1.1\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 3.3.4\n......\n.....\n2023-01-12 13:44:54,306 INFO namenode.FSImage: Allocated new BlockPoolId: BP-721308298-127.0.1.1-1673527494283\n2023-01-12 13:44:54,613 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.\n2023-01-12 13:44:54,707 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n2023-01-12 13:44:54,867 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n2023-01-12 13:44:55,026 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0\n2023-01-12 13:44:55,066 INFO namenode.FSNamesystem: Stopping services started for active state\n2023-01-12 13:44:55,066 INFO namenode.FSNamesystem: Stopping services started for standby state\n2023-01-12 13:44:55,071 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n2023-01-12 13:44:55,072 INFO namenode.NameNode: SHUTDOWN_MSG: \n</code></pre> <ol> <li>Iniciando el demonio Namenode y Datanode</li> </ol> <pre><code>sbin/start-dfs.sh\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop-3.3.4$ sbin/start-dfs.sh\nStarting namenodes on [bda-iesgrancapitan]\nStarting datanodes\nStarting secondary namenodes [hadoop-VirtualBox]\nhadoop@hadoop-VirtualBox:~/hadoop-3.3.4$ jps\n11760 NameNode\n12087 SecondaryNameNode\n12128 DataNode\n12205 Jps\n</code></pre> <ol> <li>Accede desde el navegador a <code>http://bda-iesgrancapitan:9870/</code> para acceder al interfaz web de HDFS</li> </ol> Figura1_Instalando Hadoop HDFS Interfaz Web"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#usando-hdfs","title":"Usando HDFS","text":"<p>Vamos a investigar cu\u00e1l es el funcionamiento interno de HDFS estudiado en la teor\u00eda.</p> <p>Siguiendo la configuraci\u00f3n de la m\u00e1quina, hemos configurado que la carpeta de almacenamiento de datos en nuestro HDFS es <code>/home/hadoop/hadoop_data/hdfs/datanode</code></p> <p>Para ello vamos a a\u00f1adir a HDFS un fichero de gran volumen. Accede al enlace y descarga el archivo genome_2021.zip</p> <ol> <li>Descargamos el archivo en el sistema de archivos local</li> </ol> <p><pre><code>wget https://files.grouplens.org/datasets/tag-genome-2021/genome_2021.zip\n</code></pre> 2. Lo a\u00f1adimos a HDFS</p> <pre><code>hdfs dfs -copyFromLocal genome_2021.zip /\n</code></pre> <p>La salida del log nos indica la divisi\u00f3n en bloques y la adici\u00f3n de la transacci\u00f3n en el EditLog ()</p> <pre><code>2023-01-13 13:40:56,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:40:58,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:00,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:03,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:05,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:09,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:12,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:14,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:19,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:22,834 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:24,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:28,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:31,031 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:34,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:40,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:40,688 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 46 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 21 Number of syncs: 24 SyncTimes(ms): 18313 \n2023-01-13 13:41:41,902 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /genome_2021.zip._COPYING_ is closed by DFSClient_NONMAPREDUCE_-534012830_1\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 49\n2023-01-13 13:41:43,323 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 50 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 22 Number of syncs: 29 SyncTimes(ms): 19270 \n2023-01-13 13:41:43,327 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000001 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000001-0000000000000000050\n2023-01-13 13:41:43,464 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 51\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 51, 51\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 200 \n2023-01-13 13:42:43,986 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 245 \n2023-01-13 13:42:43,989 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000051 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000051-0000000000000000052\n2023-01-13 13:42:43,989 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 53\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 53, 53\n2023-01-13 13:43:44,279 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 169 \n2023-01-13 13:43:44,409 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 298 \n2023-01-13 13:43:44,414 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000053 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000053-0000000000000000054\n2023-01-13 13:43:44,415 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 55\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 55, 55\n2023-01-13 13:44:44,727 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 179 \n2023-01-13 13:44:44,897 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 349 \n2023-01-13 13:44:44,899 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000055 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000055-0000000000000000056\n2023-01-13 13:44:44,899 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 57\n</code></pre> Figura2_SecondaryNamenode y Namenode <ol> <li> <p>Como puedes observar en el log, se generan un conjunto de ficheros en la carpeta <code>current</code>, que continen un conjunto de ficheros cuyos prefijos son:     - edits_000NNN: hist\u00f3rico de cambios que se van produciendo.    - edits_inprogress_NNN: cambios actuales en memoria que no se han persistido.    - fsimagen_000NNN: snapshot en el tiempo del sistema de ficheros.</p> </li> <li> <p>Si accedes a la carpeta HDFS <code>/home/hadoop/hadoop_data/hdfs/namenode/current</code> desde nuestro sistema de archivos, puedes observarlos tambi\u00e9n</p> </li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop_data/hdfs/namenode/current$ ls\nedits_0000000000000000001-0000000000000000050\nedits_0000000000000000051-0000000000000000052\nedits_0000000000000000053-0000000000000000054\nedits_0000000000000000055-0000000000000000056\nedits_0000000000000000057-0000000000000000058\nedits_0000000000000000059-0000000000000000060\nedits_0000000000000000061-0000000000000000062\nedits_0000000000000000063-0000000000000000064\nedits_0000000000000000065-0000000000000000066\nedits_0000000000000000067-0000000000000000068\nedits_0000000000000000069-0000000000000000070\nedits_0000000000000000071-0000000000000000072\nedits_0000000000000000073-0000000000000000074\nedits_0000000000000000075-0000000000000000076\nedits_inprogress_0000000000000000077\nfsimage_0000000000000000000\nfsimage_0000000000000000000.md5\nseen_txid\nVERSION\n</code></pre> <ol> <li>Por otro lado, si accedemos a la carpeta HDFS <code>/home/hadoop/hadoop_data/hdfs/datanode</code> desde nuestro sistema de archivos, y entramos dentro de su subdirectorio creado despu\u00e9s de la transacci\u00f3n, tambi\u00e9n podemos observar la generaci\u00f3n de los diferentes bloques</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop_data/hdfs/datanode/current/BP-844695326-127.0.1.1-1673613552206/current/finalized/subdir0/subdir0$ ls -lh\ntotal 1,9G\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:40 blk_1073741825\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:40 blk_1073741825_1001.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741826\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741826_1002.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741827\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741827_1003.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741828\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741828_1004.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741829\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741829_1005.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741830\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741830_1006.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741831\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741831_1007.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741832\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741832_1008.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741833\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741833_1009.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741834\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741834_1010.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741835\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741835_1011.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741836\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741836_1012.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741837\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741837_1013.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741838\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741838_1014.meta\n-rw-rw-r-- 1 hadoop hadoop  47M ene 13 13:41 blk_1073741839\n</code></pre> <ol> <li>Comprobamos toda esta informaci\u00f3n y mucha m\u00e1s adicional a trav\u00e9s de la interfaz web de HDFS <code>http://bda-iesgrancapitan:9870/</code> (que es mayor que la que vimos con Cloudera, cuya versi\u00f3n de Hadoop es inferior)</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#administracion","title":"Administraci\u00f3n","text":"<p>HDFS tambi\u00e9n permite administraci\u00f3n desde linea de comandos. El m\u00e1s usado es la opci\u00f3n \u00b4hdfs dfsadmin`</p> <p>Puedes ver todas las opciones en la documentaci\u00f3n oficial.</p> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs dfsadmin\nUsage: hdfs dfsadmin\nNote: Administrative commands can only be run as the HDFS superuser.\n    [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]\n    [-safemode &lt;enter | leave | get | wait | forceExit&gt;]\n    [-saveNamespace [-beforeShutdown]]\n    [-rollEdits]\n    [-restoreFailedStorage true|false|check]\n    [-refreshNodes]\n    [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]\n    [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-finalizeUpgrade]\n    [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]\n    [-upgrade &lt;query | finalize&gt;]\n    [-refreshServiceAcl]\n    [-refreshUserToGroupsMappings]\n    [-refreshSuperUserGroupsConfiguration]\n    [-refreshCallQueue]\n    [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]\n    [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;]\n    [-printTopology]\n    [-refreshNamenodes datanode_host:ipc_port]\n    [-getVolumeReport datanode_host:ipc_port]\n    [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]\n    [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]\n    [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]\n    [-fetchImage &lt;local directory&gt;]\n    [-allowSnapshot &lt;snapshotDir&gt;]\n    [-disallowSnapshot &lt;snapshotDir&gt;]\n    [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]\n    [-evictWriters &lt;datanode_host:ipc_port&gt;]\n    [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]\n    [-metasave filename]\n    [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt; [-namenode &lt;namenode_host:ipc_port&gt;]]\n    [-listOpenFiles [-blockingDecommission] [-path &lt;path&gt;]]\n    [-help [cmd]]\n\nGeneric options supported are:\n-conf &lt;configuration file&gt;        specify an application configuration file\n-D &lt;property=value&gt;               define a value for a given property\n-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager\n-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster\n-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath\n-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines\n\nThe general command line syntax is:\ncommand [genericOptions] [commandOptions]\n</code></pre> <p>Vamos a probra algunas de ellas:</p> <ul> <li><code>hdfs dfsadmin -report</code>: Realiza un resumen del sistema HDFS, donde podemos comprobar el estado de los diferentes nodos. Es similar al que aparece en el interfaz web,</li> <li><code>hdfs dfsadmin -listOpenFiles</code>: Comprueba si hay alg\u00fan fichero abierto.</li> <li><code>hdfs dfsadmin -printTopology</code>: Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo.</li> <li><code>hdfs dfsadmin -safemode enter</code>: Pone el sistema en modo seguro, el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.</li> <li><code>hdfs dfsadmin -safemode leave</code>: Sale del modo seguro.</li> </ul> <p>Otro ejemplo:</p> <ul> <li><code>hdfs fsck</code>: Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: <code>hdfs fsck /</code></li> </ul> <p>\u00b4\u00b4\u00b4 hadoop@hadoop-VirtualBox:~$ hdfs fsck / Connecting to namenode via http://bda-iesgrancapitan:9870/fsck?ugi=hadoop&amp;path=%2F FSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path / at Mon Jan 16 13:01:09 CET 2023</p> <p>Status: HEALTHY  Number of data-nodes:  1  Number of racks:       1  Total dirs:            1  Total symlinks:        0</p> <p>Replicated Blocks:  Total size:    1928028583 B  Total files:   1  Total blocks (validated):  15 (avg. block size 128535238 B)  Minimally replicated blocks:   15 (100.0 %)  Over-replicated blocks:    0 (0.0 %)  Under-replicated blocks:   0 (0.0 %)  Mis-replicated blocks:     0 (0.0 %)  Default replication factor:    1  Average block replication: 1.0  Missing blocks:        0  Corrupt blocks:        0  Missing replicas:      0 (0.0 %)  Blocks queued for replication: 0</p> <p>Erasure Coded Block Groups:  Total size:    0 B  Total files:   0  Total block groups (validated):    0  Minimally erasure-coded block groups:  0  Over-erasure-coded block groups:   0  Under-erasure-coded block groups:  0  Unsatisfactory placement block groups: 0  Average block group size:  0.0  Missing block groups:      0  Corrupt block groups:      0  Missing internal blocks:   0  Blocks queued for replication: 0 FSCK ended at Mon Jan 16 13:01:09 CET 2023 in 36 milliseconds</p> <p>The filesystem under path '/' is HEALTHY \u00b4\u00b4\u00b4</p> <p>Tambi\u00e9n existen otros comandos interesantes como: <code>balancer</code>, <code>cacheadmin</code>, <code>datanode</code>, <code>namenode</code>,... </p> <p>Puedes consultar la lista completa en la documentaci\u00f3n oficial</p>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#snapshots","title":"Snapshots","text":"<p>Mediante Snapshots podemos guardar la instant\u00e1nea de como se encuentra todo nuestros datos dentro del sismeta de ficheros, que puede servir como copia de seguridad, para un futuro backup.</p> <p>Vamos a realizar un ejemplo. Creamos un directorio dentro de nuestro HDFS y copiamos nuestro fichero de genoma 2021 dentro de \u00e9l:</p> <p><pre><code>hdfs dfs -mkdir /bda\nhdfs dfs -cp /genome_2021.zip /bda\nhdfs dfs -ls /bda\n</code></pre> Activamos el uso de snapshot en el directorio que queramos obtener una instant\u00e1nea:</p> <pre><code>hdfs dfsadmin -allowSnapshot /bda\n</code></pre> <p>Procedemos a crear una instant\u00e1nea indicando la carpeta y el nombre que va a tener</p> <pre><code>hdfs dfs -createSnapshot /bda snapshot_bda_1\n</code></pre> <p>Se crea una carpeta oculta dentro de la carpeta que contendr\u00e1 la informaci\u00f3n <code>/bda/.snapshot/snapshot_bda_1</code></p> <p>Puedes verlo tambi\u00e9n desde la interfaz web de HDFS en su apartado de Snapshot</p> Figura3_Snapshot <p>Vamos a borrar el archivo que hemos copiado y comprobamos</p> <p><pre><code>hdfs dfs -rm /bda/genome_2021.zip\n//Deleted /bda/genome_2021.zip\nhdfs dfs -ls /bda/\n</code></pre> Para recuperar el fichero usamos el snapshot creado anteriormente</p> <pre><code>hdfs dfs -cp /bda/.snapshot/snapshot_bda_1/genome_2021.zip /bda/genome_2021.zip\n</code></pre> <p>Para comprobar los directorios que actualmente soportan snapshot hacemos un ls de los mismos con su comando correspondiente:</p> <pre><code>hdfs lsSnapshottableDir\n</code></pre> <p>Por \u00faltimo, para borrar un snapshot:</p> <pre><code>hdfs dfs -deleteSnapshot /bda snapshot_bda_1\n</code></pre> <p>Y si queremos desabilitarlo los snapshot:</p> <pre><code>hdfs dfsadmin -disallowSnapshot /bda\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#permisos-hdfs-ui","title":"Permisos HDFS UI","text":"<p>Desde el apartado <code>Browser Directory</code> del Web IU <code>http://bda-iesgrancapitan:9870/explorer.html</code> Podemos acceder al sistema de ficheros y su contenido de HDFS.</p> <p>Pero si intentamos borrar alg\u00fan contenido nos salta un error de permisos: <code>Permission denied: user=dr.who, access=WRITE, inode=\"/bda\":hadoop:supergroup:drwxr-xr-x</code>. Esto es debido a que, por defecto, los recursos v\u00eda web se realizan desde el usuario <code>dr.who</code></p> Figura3_Snapshot <p>Para poder tener permisos para ello podemos modificar los permisos:</p> <pre><code>hdfs dfs -mkdir /bda/prueba_permisos\nhdfs dfs -chmod /bda/prueba_permisos \n</code></pre> <p>Otra posibilidad es modificar el archivo de configuraci\u00f3nb <code>core-site.xml</code> y a\u00f1adir la propiedad para modificar el usuario est\u00e1tico, en mi caso, el usuario <code>hadoop</code></p> core-site.xml<pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Hue.html#acceso-a-hdfs-a-traves-de-python","title":"Acceso a HDFS a trav\u00e9s de Python","text":"<p>Para ello, usaremos la libreria (HdfsCLI)[https://pypi.org/project/hdfs/]. La instalamos mediante <code>pip</code></p> <pre><code>pip install hdfs\n</code></pre> <p>Para nuestro ejemplo, vamos a descargar un ejemplo con formato csv y a\u00f1adirlo a nuestro HDFS</p> <pre><code>wget https://www.ine.es/jaxi/files/tpx/csv_bdsc/53938.csv\nhdfs dfs -mkdir /bda/python\nhdfs dfs -copyFromLocal 53938.csv  /bda/python/\nhdfs dfs -ls /bda/python\n</code></pre> <p>Teniendo como referencia la documentaci\u00f3n, vamos a conectarnos a HDFS y copiar un archivo</p> <pre><code>from hdfs import InsecureClient\n\n# Datos de conexi\u00f3n\nHDFS_HOSTNAME = 'bda-iesgrancapitan'\nHDFS_PORT = 9870\nHDFS_CONNECTION = f'http://{HDFS_HOSTNAME}:{HDFS_PORT}'\n\n# En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura\nhdfs_client = InsecureClient(HDFS_CONNECTION)\n\n#Lectura\n# Leemos el fichero de 'genome2021.zip' que tenemos en HDFS\nfichero = '/bda/python/53938.csv'\nwith hdfs_client.read(fichero) as reader:\n    texto = reader.read()\n\nprint(texto)\n\n#Escritura\n# Escribimos los elementos de la lista en formato csv\ndatos=\"dni,nombre,apellidos,direccion,cp\\n\"\nlista = [['123', 'Nombre1', 'Apellidos1', 'Mikasa1', '14000'],\n         ['456', 'Nombre4', 'Apellidos4', 'Mikasa4', '41000'],\n         ['789', 'Nombre7', 'Apellidos7', 'Mikasa7', '19000']]\nfor i in range(0, len(lista), 1):\n    for j in range(0, len(lista[i]), 1):\n        if(j&lt;len(lista[i])-1):\n            datos+=f'{lista[i][j]},'\n        else:\n            datos+=f'{lista[i][j]}\\n'\nhdfs_client.write(\"/bda/python/datos.csv\", datos)\n</code></pre> <p>Adicionalmente, esta librer\u00eda te da funcionalidad opcional para <code>avro</code>, <code>dataframe</code> (con Pandas) y <code>Kerberos</code>. Estos casos son los m\u00e1s habituales en el mundo real.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html","title":"Big Data Aplicado","text":""},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#ud-6-apache-hadoop","title":"UD 6 - Apache Hadoop","text":""},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#instalacion-apache-hadoop-ubuntu-2204","title":"Instalaci\u00f3n Apache Hadoop Ubuntu 22.04","text":"<p>Para trabajar en esta y las siguientes sesiones, vamos a utilizar una m\u00e1quina virtual. A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario hadoop y la contrase\u00f1a hadoop.</p> <p>Tambi\u00e9n puedes instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Ubuntu Mate 22.04 LTS y la versi\u00f3n 3.3.4 de Hadoop. Puedes seguir las instrucciones de esta secci\u00f3n.</p> <p>Ay\u00fadate tambi\u00e9n de la informaci\u00f3n de la p\u00e1gina oficial</p> <p>Note</p> <p>Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida, porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.</p>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#instalacion","title":"Instalaci\u00f3n","text":"<ol> <li>Java\u2122 debe ser instalado. Las versiones de Java recomendadas se encuentran descritas en HadoopJavaVersions.</li> </ol> <pre><code>sudo apt-get install openjdk-11-jdk\n/usr/bin/java\n</code></pre> <ol> <li>ssh debe estar instalado y sshd debe estar ejecut\u00e1ndose para usar las secuencias de comandos de Hadoop que administran los demonios ssh remotos de Hadoop, ya que vamos a usar las secuencias de comandos de inicio y detecci\u00f3n opcionales.</li> </ol> <pre><code>sudo apt-get install ssh\n</code></pre> <ol> <li> <p>Abre la terminal en el directorio <code>$HOME</code></p> </li> <li> <p>Para obtener la distribuci\u00f3n de Apache Haddop, descarga la versi\u00f3n estable m\u00e1s reciente desde Apache Download Mirrors</p> </li> </ol> <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n</code></pre> <ol> <li>Una vez descargado, desempaquetamos el archivo descargado con el comando tar y entra dentro de la carpeta:</li> </ol> <pre><code>tar -xzf hadoop-3.3.4.tar.gz\ncd hadoop-3.3.4\n</code></pre> <ol> <li>Edita el siguiente archivo <code>etc/hadoop/hadoop-env.sh</code> para definir la variable de entorno de Java y a\u00f1adela.</li> </ol> <pre><code># Technically, the only required environment variable is JAVA_HOME.\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n</code></pre> <ol> <li>Para poder usar los comandos de HDFS en cualquier lugar del sistema, sin tener que hacerlo desde el directorio de Hadoop (por ejemplo /home/hadoop/hadoop-3.3.4/bin), creamos las variables de entorno y a\u00f1adimos al PATH. Para ello abrimos el archivo <code>~/.bashrc</code> y a\u00f1adimos al final el siguiente c\u00f3digo y ejecuta el comando <code>source ~/.bashrc</code></li> </ol> ~/.bashrc<pre><code>export HADOOP_HOME=$HOME/hadoop-3.3.4\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li>Ejecuta el siguiente comando. Si no da error, podemos continuar</li> </ol> <pre><code>bin/hadoop\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#configuracion-pseudo-distributed-operation","title":"Configuraci\u00f3n (Pseudo-Distributed Operation)","text":"<p>Hadoop se puede ejecutar en un solo nodo en un modo pseudo-distributed donde cada demonio de Hadoop se ejecuta en un proceso Java separado.</p> <p>Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta <code>$HADOOP_HOME/etc/hadoop</code>.</p> <ol> <li>El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo <code>core-site.xml</code>. En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos (namenode) de la arquitectura.</li> </ol> core-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://bda-iesgrancapitan:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>El siguiente paso es configurar el archivo <code>hdfs-site.xml</code> donde se indica tanto el factor de replica como la ruta donde se almacenan tanto los metadatos (namenode) como los datos en s\u00ed (datanode):</li> </ol> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Opcional: Si quieres especificar la ruta donde se almacenan los metadatos(namenode) y los datos(datanode) donde el propio hadoop los configura por defecto puedes hacerlo cambiando dichos par\u00e1metroscorrespondintes. Todos lo par\u00e1metros por defecto subceptibles de cambio se encuentran en este recurso</li> </ol> <p>Note</p> <p>Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode.</p> hdfs-site.xml<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/home/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;/home/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Comprobamos que podemos entrar por ssh al localhost sin un passphrase:</li> </ol> <pre><code>ssh localhost\nexit //Si hemos podido acceder\n</code></pre> <ol> <li>Si no puedes, ejecuta los siguientes comandos:</li> </ol> <pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n</code></pre> <ol> <li>A\u00f1ade a <code>/etc/hosts</code>el nombre de tu dominio indicado en <code>core-site.xml</code> para que no te de error de resoluci\u00f3n de nombres. En mi caso a\u00f1ado la siguiente linea y reinicia el servicio:</li> </ol> <pre><code>127.0.0.1   bda-iesgrancapitan\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#ejecucion","title":"Ejecuci\u00f3n","text":"<ol> <li>Ejecuta el siguiente comando</li> </ol> <pre><code>bin/hdfs namenode -format\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>WARNING: /home/hadoop/hadoop-3.3.4/logs does not exist. Creating.\n2023-01-12 13:44:51,867 INFO namenode.NameNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-VirtualBox/127.0.1.1\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 3.3.4\n......\n.....\n2023-01-12 13:44:54,306 INFO namenode.FSImage: Allocated new BlockPoolId: BP-721308298-127.0.1.1-1673527494283\n2023-01-12 13:44:54,613 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.\n2023-01-12 13:44:54,707 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n2023-01-12 13:44:54,867 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n2023-01-12 13:44:55,026 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0\n2023-01-12 13:44:55,066 INFO namenode.FSNamesystem: Stopping services started for active state\n2023-01-12 13:44:55,066 INFO namenode.FSNamesystem: Stopping services started for standby state\n2023-01-12 13:44:55,071 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n2023-01-12 13:44:55,072 INFO namenode.NameNode: SHUTDOWN_MSG: \n</code></pre> <ol> <li>Iniciando el demonio Namenode y Datanode</li> </ol> <pre><code>sbin/start-dfs.sh\n</code></pre> <ol> <li>Deber\u00eda darte una salida como la siguiente</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop-3.3.4$ sbin/start-dfs.sh\nStarting namenodes on [bda-iesgrancapitan]\nStarting datanodes\nStarting secondary namenodes [hadoop-VirtualBox]\nhadoop@hadoop-VirtualBox:~/hadoop-3.3.4$ jps\n11760 NameNode\n12087 SecondaryNameNode\n12128 DataNode\n12205 Jps\n</code></pre> <ol> <li>Accede desde el navegador a <code>http://bda-iesgrancapitan:9870/</code> para acceder al interfaz web de HDFS</li> </ol> Figura1_Instalando Hadoop HDFS Interfaz Web"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#usando-hdfs","title":"Usando HDFS","text":"<p>Vamos a investigar cu\u00e1l es el funcionamiento interno de HDFS estudiado en la teor\u00eda.</p> <p>Siguiendo la configuraci\u00f3n de la m\u00e1quina, hemos configurado que la carpeta de almacenamiento de datos en nuestro HDFS es <code>/home/hadoop/hadoop_data/hdfs/datanode</code></p> <p>Para ello vamos a a\u00f1adir a HDFS un fichero de gran volumen. Accede al enlace y descarga el archivo genome_2021.zip</p> <ol> <li>Descargamos el archivo en el sistema de archivos local</li> </ol> <p><pre><code>wget https://files.grouplens.org/datasets/tag-genome-2021/genome_2021.zip\n</code></pre> 2. Vamos a observar la salida de logs en cada uno de los siguientes pasos, que nos va a servir para afianzar como como funciona HDFS. Observamos el log del namenode. En mi caso:</p> <pre><code>tail -f HADOOP_HOME/logs/hadoop-hadoop-namenode-hadoop-VirtualBox.log\n</code></pre> <ol> <li>Lo a\u00f1adimos a HDFS</li> </ol> <pre><code>hdfs dfs -copyFromLocal genome_2021.zip /\n</code></pre> <p>La salida del log nos indica la divisi\u00f3n en bloques y la adici\u00f3n de la transacci\u00f3n en el EditLog ()</p> <pre><code>2023-01-13 13:40:56,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:40:58,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:00,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:03,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:05,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:09,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:12,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:14,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:19,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:22,834 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:24,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:28,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:31,031 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:34,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:40,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:9866 for /genome_2021.zip._COPYING_\n2023-01-13 13:41:40,688 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 46 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 21 Number of syncs: 24 SyncTimes(ms): 18313 \n2023-01-13 13:41:41,902 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /genome_2021.zip._COPYING_ is closed by DFSClient_NONMAPREDUCE_-534012830_1\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:41:43,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 49\n2023-01-13 13:41:43,323 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 50 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 22 Number of syncs: 29 SyncTimes(ms): 19270 \n2023-01-13 13:41:43,327 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000001 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000001-0000000000000000050\n2023-01-13 13:41:43,464 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 51\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 51, 51\n2023-01-13 13:42:43,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 200 \n2023-01-13 13:42:43,986 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 245 \n2023-01-13 13:42:43,989 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000051 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000051-0000000000000000052\n2023-01-13 13:42:43,989 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 53\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:43:44,278 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 53, 53\n2023-01-13 13:43:44,279 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 169 \n2023-01-13 13:43:44,409 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 298 \n2023-01-13 13:43:44,414 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000053 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000053-0000000000000000054\n2023-01-13 13:43:44,415 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 55\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs\n2023-01-13 13:44:44,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 55, 55\n2023-01-13 13:44:44,727 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 179 \n2023-01-13 13:44:44,897 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 349 \n2023-01-13 13:44:44,899 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop_data/hdfs/namenode/current/edits_inprogress_0000000000000000055 -&gt; /home/hadoop/hadoop_data/hdfs/namenode/current/edits_0000000000000000055-0000000000000000056\n2023-01-13 13:44:44,899 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 57\n</code></pre> Figura2_SecondaryNamenode y Namenode <ol> <li>Como puedes observar en el log, se generan un conjunto de ficheros en la carpeta <code>current</code>, que continen un conjunto de ficheros cuyos prefijos son:</li> </ol> <ul> <li>edits_000NNN: hist\u00f3rico de cambios que se van produciendo.</li> <li>edits_inprogress_NNN: cambios actuales en memoria que no se han persistido.</li> <li>fsimagen_000NNN: snapshot en el tiempo del sistema de ficheros.</li> </ul> <ol> <li>Si accedes a la carpeta HDFS <code>/home/hadoop/hadoop_data/hdfs/namenode/current</code> desde nuestro sistema de archivos, puedes observarlos tambi\u00e9n</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop_data/hdfs/namenode/current$ ls\nedits_0000000000000000001-0000000000000000050\nedits_0000000000000000051-0000000000000000052\nedits_0000000000000000053-0000000000000000054\nedits_0000000000000000055-0000000000000000056\nedits_0000000000000000057-0000000000000000058\nedits_0000000000000000059-0000000000000000060\nedits_0000000000000000061-0000000000000000062\nedits_0000000000000000063-0000000000000000064\nedits_0000000000000000065-0000000000000000066\nedits_0000000000000000067-0000000000000000068\nedits_0000000000000000069-0000000000000000070\nedits_0000000000000000071-0000000000000000072\nedits_0000000000000000073-0000000000000000074\nedits_0000000000000000075-0000000000000000076\nedits_inprogress_0000000000000000077\nfsimage_0000000000000000000\nfsimage_0000000000000000000.md5\nseen_txid\nVERSION\n</code></pre> <ol> <li>Por otro lado, si accedemos a la carpeta HDFS <code>/home/hadoop/hadoop_data/hdfs/datanode</code> desde nuestro sistema de archivos, y entramos dentro de su subdirectorio creado despu\u00e9s de la transacci\u00f3n, tambi\u00e9n podemos observar la generaci\u00f3n de los diferentes bloques</li> </ol> <pre><code>hadoop@hadoop-VirtualBox:~/hadoop_data/hdfs/datanode/current/BP-844695326-127.0.1.1-1673613552206/current/finalized/subdir0/subdir0$ ls -lh\ntotal 1,9G\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:40 blk_1073741825\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:40 blk_1073741825_1001.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741826\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741826_1002.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741827\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741827_1003.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741828\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741828_1004.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741829\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741829_1005.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741830\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741830_1006.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741831\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741831_1007.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741832\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741832_1008.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741833\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741833_1009.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741834\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741834_1010.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741835\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741835_1011.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741836\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741836_1012.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741837\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741837_1013.meta\n-rw-rw-r-- 1 hadoop hadoop 128M ene 13 13:41 blk_1073741838\n-rw-rw-r-- 1 hadoop hadoop 1,1M ene 13 13:41 blk_1073741838_1014.meta\n-rw-rw-r-- 1 hadoop hadoop  47M ene 13 13:41 blk_1073741839\n</code></pre> <ol> <li>Comprobamos toda esta informaci\u00f3n y mucha m\u00e1s adicional a trav\u00e9s de la interfaz web de HDFS <code>http://bda-iesgrancapitan:9870/</code> (que es mayor que la que vimos con Cloudera, cuya versi\u00f3n de Hadoop es inferior)</li> </ol>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#administracion","title":"Administraci\u00f3n","text":"<p>HDFS tambi\u00e9n permite administraci\u00f3n desde linea de comandos. El m\u00e1s usado es la opci\u00f3n <code>hdfs dfsadmin</code></p> <p>Puedes ver todas las opciones en la documentaci\u00f3n oficial.</p> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs dfsadmin\nUsage: hdfs dfsadmin\nNote: Administrative commands can only be run as the HDFS superuser.\n    [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]\n    [-safemode &lt;enter | leave | get | wait | forceExit&gt;]\n    [-saveNamespace [-beforeShutdown]]\n    [-rollEdits]\n    [-restoreFailedStorage true|false|check]\n    [-refreshNodes]\n    [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]\n    [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]\n    [-finalizeUpgrade]\n    [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]\n    [-upgrade &lt;query | finalize&gt;]\n    [-refreshServiceAcl]\n    [-refreshUserToGroupsMappings]\n    [-refreshSuperUserGroupsConfiguration]\n    [-refreshCallQueue]\n    [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]\n    [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;]\n    [-printTopology]\n    [-refreshNamenodes datanode_host:ipc_port]\n    [-getVolumeReport datanode_host:ipc_port]\n    [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]\n    [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]\n    [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]\n    [-fetchImage &lt;local directory&gt;]\n    [-allowSnapshot &lt;snapshotDir&gt;]\n    [-disallowSnapshot &lt;snapshotDir&gt;]\n    [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]\n    [-evictWriters &lt;datanode_host:ipc_port&gt;]\n    [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]\n    [-metasave filename]\n    [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt; [-namenode &lt;namenode_host:ipc_port&gt;]]\n    [-listOpenFiles [-blockingDecommission] [-path &lt;path&gt;]]\n    [-help [cmd]]\n\nGeneric options supported are:\n-conf &lt;configuration file&gt;        specify an application configuration file\n-D &lt;property=value&gt;               define a value for a given property\n-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager\n-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster\n-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath\n-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines\n\nThe general command line syntax is:\ncommand [genericOptions] [commandOptions]\n</code></pre> <p>Vamos a probra algunas de ellas:</p> <ul> <li><code>hdfs dfsadmin -report</code>: Realiza un resumen del sistema HDFS, donde podemos comprobar el estado de los diferentes nodos. Es similar al que aparece en el interfaz web.</li> <li><code>hdfs dfsadmin -listOpenFiles</code>: Comprueba si hay alg\u00fan fichero abierto.</li> <li><code>hdfs dfsadmin -printTopology</code>: Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo.</li> <li><code>hdfs dfsadmin -safemode enter</code>: Pone el sistema en modo seguro, el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.</li> <li><code>hdfs dfsadmin -safemode leave</code>: Sale del modo seguro.</li> </ul> <p>Otro ejemplo:</p> <ul> <li><code>hdfs fsck</code>: Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: <code>hdfs fsck /</code></li> </ul> <pre><code>hadoop@hadoop-VirtualBox:~$ hdfs fsck /\nConnecting to namenode via http://bda-iesgrancapitan:9870/fsck?ugi=hadoop&amp;path=%2F\nFSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path / at Mon Jan 16 13:01:09 CET 2023\n\nStatus: HEALTHY\n Number of data-nodes:  1\n Number of racks:       1\n Total dirs:            1\n Total symlinks:        0\n\nReplicated Blocks:\n Total size:    1928028583 B\n Total files:   1\n Total blocks (validated):  15 (avg. block size 128535238 B)\n Minimally replicated blocks:   15 (100.0 %)\n Over-replicated blocks:    0 (0.0 %)\n Under-replicated blocks:   0 (0.0 %)\n Mis-replicated blocks:     0 (0.0 %)\n Default replication factor:    1\n Average block replication: 1.0\n Missing blocks:        0\n Corrupt blocks:        0\n Missing replicas:      0 (0.0 %)\n Blocks queued for replication: 0\n\nErasure Coded Block Groups:\n Total size:    0 B\n Total files:   0\n Total block groups (validated):    0\n Minimally erasure-coded block groups:  0\n Over-erasure-coded block groups:   0\n Under-erasure-coded block groups:  0\n Unsatisfactory placement block groups: 0\n Average block group size:  0.0\n Missing block groups:      0\n Corrupt block groups:      0\n Missing internal blocks:   0\n Blocks queued for replication: 0\nFSCK ended at Mon Jan 16 13:01:09 CET 2023 in 36 milliseconds\n\n\nThe filesystem under path '/' is HEALTHY\n</code></pre> <p>Tambi\u00e9n existen otros comandos interesantes como: <code>balancer</code>, <code>cacheadmin</code>, <code>datanode</code>, <code>namenode</code>,... </p> <p>Puedes consultar la lista completa en la documentaci\u00f3n oficial</p>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#snapshots","title":"Snapshots","text":"<p>Mediante Snapshots podemos guardar la instant\u00e1nea de como se encuentra todo nuestros datos dentro del sismeta de ficheros, que puede servir como copia de seguridad, para un futuro backup.</p> <p>Vamos a realizar un ejemplo. Creamos un directorio dentro de nuestro HDFS y copiamos nuestro fichero de genoma 2021 dentro de \u00e9l:</p> <p><pre><code>hdfs dfs -mkdir /bda\nhdfs dfs -cp /genome_2021.zip /bda\nhdfs dfs -ls /bda\n</code></pre> Activamos el uso de snapshot en el directorio que queramos obtener una instant\u00e1nea:</p> <pre><code>hdfs dfsadmin -allowSnapshot /bda\n</code></pre> <p>Procedemos a crear una instant\u00e1nea indicando la carpeta y el nombre que va a tener</p> <pre><code>hdfs dfs -createSnapshot /bda snapshot_bda_1\n</code></pre> <p>Se crea una carpeta oculta dentro de la carpeta que contendr\u00e1 la informaci\u00f3n <code>/bda/.snapshot/snapshot_bda_1</code></p> <p>Puedes verlo tambi\u00e9n desde la interfaz web de HDFS en su apartado de Snapshot</p> Figura3_Snapshot <p>Vamos a borrar el archivo que hemos copiado y comprobamos</p> <p><pre><code>hdfs dfs -rm /bda/genome_2021.zip\n//Deleted /bda/genome_2021.zip\nhdfs dfs -ls /bda/\n</code></pre> Para recuperar el fichero usamos el snapshot creado anteriormente</p> <pre><code>hdfs dfs -cp /bda/.snapshot/snapshot_bda_1/genome_2021.zip /bda/genome_2021.zip\n</code></pre> <p>Para comprobar los directorios que actualmente soportan snapshot hacemos un ls de los mismos con su comando correspondiente:</p> <pre><code>hdfs lsSnapshottableDir\n</code></pre> <p>Por \u00faltimo, para borrar un snapshot:</p> <pre><code>hdfs dfs -deleteSnapshot /bda snapshot_bda_1\n</code></pre> <p>Y si queremos desabilitarlo los snapshot:</p> <pre><code>hdfs dfsadmin -disallowSnapshot /bda\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#permisos-hdfs-ui","title":"Permisos HDFS UI","text":"<p>Desde el apartado <code>Browser Directory</code> del Web IU <code>http://bda-iesgrancapitan:9870/explorer.html</code> Podemos acceder al sistema de ficheros y su contenido de HDFS.</p> <p>Pero si intentamos borrar alg\u00fan contenido nos salta un error de permisos: <code>Permission denied: user=dr.who, access=WRITE, inode=\"/bda\":hadoop:supergroup:drwxr-xr-x</code>. Esto es debido a que, por defecto, los recursos v\u00eda web se realizan desde el usuario <code>dr.who</code></p> Figura3_Snapshot <p>Para poder tener permisos para ello podemos modificar los permisos:</p> <pre><code>hdfs dfs -mkdir /bda/prueba_permisos\nhdfs dfs -chmod 777 /bda/prueba_permisos \n</code></pre> <p>Otra posibilidad es modificar el archivo de configuraci\u00f3nb <code>core-site.xml</code> y a\u00f1adir la propiedad para modificar el usuario est\u00e1tico, en mi caso, el usuario <code>hadoop</code></p> core-site.xml<pre><code>&lt;property&gt;\n    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n    &lt;value&gt;hadoop&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"UD4%20-%20Apache%20Hadoop/Ejercicios/Instalando_Hadoop.html#acceso-a-hdfs-a-traves-de-python","title":"Acceso a HDFS a trav\u00e9s de Python","text":"<p>Para ello, usaremos la libreria (HdfsCLI)[https://pypi.org/project/hdfs/]. La instalamos mediante <code>pip</code></p> <pre><code>pip install hdfs\n</code></pre> <p>Para nuestro ejemplo, vamos a descargar un ejemplo con formato csv y a\u00f1adirlo a nuestro HDFS</p> <pre><code>wget https://www.ine.es/jaxi/files/tpx/csv_bdsc/53938.csv\nhdfs dfs -mkdir /bda/python\nhdfs dfs -copyFromLocal 53938.csv  /bda/python/\nhdfs dfs -ls /bda/python\n</code></pre> <p>Teniendo como referencia la documentaci\u00f3n, vamos a conectarnos a HDFS y copiar un archivo</p> <pre><code>from hdfs import InsecureClient\n\n# Datos de conexi\u00f3n\nHDFS_HOSTNAME = 'bda-iesgrancapitan'\nHDFS_PORT = 9870\nHDFS_CONNECTION = f'http://{HDFS_HOSTNAME}:{HDFS_PORT}'\n\n# En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura\nhdfs_client = InsecureClient(HDFS_CONNECTION)\n\n#Lectura\n# Leemos el fichero de 'genome2021.zip' que tenemos en HDFS\nfichero = '/bda/python/53938.csv'\nwith hdfs_client.read(fichero) as reader:\n    texto = reader.read()\n\nprint(texto)\n\n#Escritura\n# Escribimos los elementos de la lista en formato csv\ndatos=\"dni,nombre,apellidos,direccion,cp\\n\"\nlista = [['123', 'Nombre1', 'Apellidos1', 'Mikasa1', '14000'],\n         ['456', 'Nombre4', 'Apellidos4', 'Mikasa4', '41000'],\n         ['789', 'Nombre7', 'Apellidos7', 'Mikasa7', '19000']]\nfor i in range(0, len(lista), 1):\n    for j in range(0, len(lista[i]), 1):\n        if(j&lt;len(lista[i])-1):\n            datos+=f'{lista[i][j]},'\n        else:\n            datos+=f'{lista[i][j]}\\n'\nhdfs_client.write(\"/bda/python/datos.csv\", datos)\n</code></pre> <p>Adicionalmente, esta librer\u00eda te da funcionalidad opcional para <code>avro</code>, <code>dataframe</code> (con Pandas) y <code>Kerberos</code>. Estos casos son los m\u00e1s habituales en el mundo real.</p>"}]}
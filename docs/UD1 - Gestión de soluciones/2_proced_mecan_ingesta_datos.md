# UD 1 Gesti√≥n de Soluciones - Procedimientos y Mecanismos de Ingesta de Datos

En esta secci√≥n se profundizar√° en cada una de las **etapas necesarias para dise√±ar e implementar un almac√©n de datos**. Para ello, en primer lugar se describir√° el proceso de **Extracci√≥n, Transformaci√≥n y Carga (ETL)**, fundamental para construir y alimentar un almac√©n de datos. Despu√©s, se desarrollar√°n dos de los dise√±os m√°s extendidos de almac√©n de datos: el **dise√±o en estrella** y el **dise√±o en copo de nieve**.


## 1. El proceso de Extracci√≥n, Transformaci√≥n y Carga (ETL)

El proceso ETL es el encargado de extraer, limpiar e integrar los datos provenientes de las fuentes de datos para alimentar el almac√©n de datos. Este proceso tambi√©n es el encargado de alimentar la capa de datos reconciliados en la arquitectura de tres capas. El proceso ETL tiene lugar cuando se puebla el almac√©n de datos y se lleva a cabo cada vez que el almac√©n de datos se actualiza. A continuaci√≥n, se describen detalladamente cada una de las fases de las que consta este proceso


### 1.1 ‚¨ÜÔ∏è Extracci√≥n

Etapa que consiste en la **lectura de los datos de las distintas fuentes de las que provienen**. Cuando un almac√©n de datos se rellena por primera vez, se suele utilizar la t√©cnica de **extracci√≥n est√°tica**, la cual consiste en extraer una instant√°nea de los datos operacionales. A partir de entonces, se utiliza la **extracci√≥n incremental** para actualizar peri√≥dicamente los datos del almac√©n de datos, recogiendo los cambios aplicados desde la √∫ltima extracci√≥n. Para ello, se utiliza el registro mantenido por el SGBD que, por ejemplo, asocia una **marca de tiempo (timestamp)** a los datos operacionales para registrar cuando fueron modificados y agilizar el proceso de extracci√≥n.

En la actualidad, existe una gran cantidad de conjuntos de datos o _data sets_ p√∫blicos, conocidos bajo el nombre de Open Data, que abarcan una gran cantidad de dominios y con los que es posible trabajar para construir soluciones big data.  

!!! note "Open Data"

    üìã **Open Data**: Se trata de datos que han sido generados por una fuente en particular, que abarcan un dominio tem√°tico o disciplinar y tienen atributos, dentro de los cuales est√° la frecuencia de actualizaci√≥n. Adem√°s, cuentan con una licencia espec√≠fica que indica las condiciones de reutilizaci√≥n de los mismos.

**La fuente de los datos es en muchos de los casos el estado** nacional, provincial, municipal u organizaciones comerciales. En otras ocasiones, **la fuente de los datos es fruto del estudio o medici√≥n por parte de particulares**. Los **atributos** de los conjuntos de datos deben especificar c√≥mo fueron obtenidos, incluyendo fechas de obtenci√≥n, actualizaci√≥n y validez, as√≠ como el p√∫blico involucrado, la metodolog√≠a de recogida o muestreo, etc.

Algunas de las fuentes m√°s utilizadas en la actualidad para la obtenci√≥n de datos abiertos provienen de **los centros nacionales e internacionales de estad√≠sticas**, como son el Instituto Nacional de Estad√≠stica de Espa√±a (INE) [^1] , eurostat [^2] , la oficina europea de estad√≠sticas, la Organizaci√≥n Mundial de la Salud (OMS) [^3] ... Por otra parte, existen **repositorios p√∫blicos de datos abiertos y a disposici√≥n de los usuarios como UCI4 [^4] o Kaggle [^5]** , que es una comunidad de cient√≠ficos de datos donde empresas y organizaciones suben sus datos y plantean problemas que son resueltos por los miembros de la comunidad. La figura 1.8 muestra las fuentes de datos que se acaban de mencionar.

<figure style="align: center;">
    <img src="images/Figura1.11_Fuentes_para_la_obtenci√≥n_de_datos_abiertos.jpg">
    <figcaption>Figura 1.11: Fuentes para la obtenci√≥n de datos abiertos. (Fuente: Propia)</figcaption>
</figure>


[^1]: [https://www.ine.es](https://www.ine.es)
[^2]: [https://ec.europa.eu/eurostat](https://ec.europa.eu/eurostat)
[^3]: [https://www.who.int/es/data/gho/publications/world-health-statistics](https://www.who.int/es/data/gho/publications/world-health-statistics)
[^4]: [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)
[^5]: [https://www.kaggle.com](https://www.kaggle.com)


### 1.2 üîÑ Transformaci√≥n

La etapa de transformaci√≥n es la fase clave para **transformar los datos operativos en datos con un formato espec√≠fico para alimentar un almac√©n de datos**. En esta etapa, **los datos se limpian y se transforman, a√±adi√©ndoles contexto y significado**. En caso de implementar un almac√©n de datos siguiendo una arquitectura de tres capas, **el proceso de transformaci√≥n es el encargado de obtener la capa de datos reconciliados**. Si bien es cierto que algunos autores separan la limpieza y la transformaci√≥n de los datos en dos etapas distintas, en este cap√≠tulo se considerar√°n ambas dentro de la fase de transformaci√≥n.

!!! info "Transformaci√≥n"

    üìÑ La etapa de **transformaci√≥n** engloba todos los procesos de limpieza y manipulaci√≥n de los datos, con el objetivo de transformar los datos operativos propios de sistemas relacionales (OLTP) en datos preparados para ser incluidos dentro del almac√©n de datos (OLAP).

La **limpieza de los datos** o _data cleaning_ engloba todos aquellos procedimientos necesarios para detectar y resolver situaciones problem√°ticas con los datos de partida que pudieran suponer problemas potenciales a la hora de analizarlos. As√≠ pues, los datos de partida pueden ser **incompletos**, es decir, pueden contener atributos sin valor o valores agregados, **incorrectos** que incluyan errores o valores sin ning√∫n significado, lo cual es com√∫n cuando los datos se introducen manualmente en el sistema, o inconsistentes cuando los cambios no son propagados a todos los m√≥dulos del sistema, los rangos de un determinado atributo son cambiantes, existen datos duplicados...


Otra tarea muy importante que se debe abordar en la fase de limpieza es la gesti√≥n de los **datos perdidos**. Se trata de **datos que no est√°n disponibles para algunas de las variables en alguna instancia**. Esto puede ser debido, por ejemplo, a que no se registraran las modificaciones sufridas por los datos, se produjera un mal funcionamiento del equipo, los datos nunca fueron rellenados o no exist√≠an en el momento en que fueron completados, se eliminaron por ser inconsistentes con la informaci√≥n registrada...

En general, se distinguen **dos tipos de situaciones cuando existen valores perdidos: datos perdidos completamente aleatorios y datos perdidos de forma no completamente aleatoria**. En el segundo caso, puede ser interesante intentar analizar la raz√≥n de la p√©rdida de los datos, la cual puede ser indicativa. En muchas ocasiones, los valores perdidos tienen relaci√≥n con un subconjunto de variables predictoras y no se encuentran aleatoriamente distribuidos por todas ellas. Por todo ello, las aproximaciones m√°s comunes a la hora de gestionar datos perdidos son:

- **Eliminaci√≥n de instancias que contengan valores perdidos**: consiste en establecer un porcentaje umbral de tal forma que, si una instancia contiene un n√∫mero de valores perdidos que supera este porcentaje, la instancia se descarta. Se trata de una t√©cnica √∫til cuando el conjunto de datos es grande y el n√∫mero de instancias con valores perdidos no es muy elevado. No obstante, esta estrategia se debe utilizar con precauci√≥n, ya que puede suponer una excesiva p√©rdida de informaci√≥n si el conjunto de datos no es muy grande, el n√∫mero de instancias con valores perdidos es alto o el porcentaje umbral es muy peque√±o.

- **Asignaci√≥n de valores fijos**: consiste en asignar un valor fijo a todos los valores perdidos de todas las variables. Este valor puede ser el n√∫mero 0 o incluso un valor desconocido _Unknown_ o no num√©rico (NaN, Not a Number) en funci√≥n del lenguaje de programaci√≥n utilizado.

- **Asignaci√≥n de valores de referencia**: asigna un valor de referencia a los valores perdidos para cada variable. Estos valores de referencia suelen ser medidas de centralizaci√≥n como la media o la mediana de los valores de cada variable.

- **Imputaci√≥n de valores perdidos**: consiste en la aplicaci√≥n de t√©cnicas m√°s sofisticadas, como pueden ser t√©cnicas estad√≠sticas o de aprendizaje autom√°tico para predecir o averiguar los valores que se han perdido.

Finalmente, la etapa de limpieza de datos tambi√©n se encarga de la **detecci√≥n de valores an√≥malos o outliers**. Se trata de valores que se han introducido de forma err√≥nea o bien a una deformaci√≥n en la distribuci√≥n de valores. El proceso de detecci√≥n de anomal√≠as consiste, fundamentalmente, en dos etapas: en primer lugar, **asumir que existe un modelo generador de datos**, como podr√≠a ser una distribuci√≥n de probabilidad. En segundo lugar, considerar que **las anomal√≠as representan un modelo generador distinto**, que no coincide con el original. Existen multitud de t√©cnicas para detectar y descartar o imputar valores an√≥malos, como lo son t√©cnicas estad√≠sticas basadas en la desviaci√≥n y el rango intercuart√≠lico o t√©cnicas de aprendizaje autom√°tico.


Despu√©s de la etapa de limpieza, comienza la etapa de **transformaci√≥n/manipulaci√≥n**. En ella, los datos se preparan para su carga en el almac√©n de datos. Para ello, se transformar√°n los datos provenientes de sistemas transaccionales OLTP en datos preparados para su an√°lisis OLAP. A continuaci√≥n, se muestran algunos de los procesos de transformaci√≥n m√°s comunes:

- **Estandarizaci√≥n de c√≥digos y formatos de representaci√≥n**: incluye tareas como transformar la informaci√≥n EBCDIC a formato ASCII o Unicode, convertir n√∫meros cardinales en ordinales o viceversa, homogeneizaci√≥n y tratamiento de fechas, expandir codificaciones a√±adiendo textos descriptivos, unificaci√≥n de c√≥digos (sexo, estado civil, etc) y est√°ndares (medidas, monedas, etc). 

- **Conversiones y combinaciones de campos**: realizaci√≥n de agregaciones y c√°lculos simples (importe neto, beneficio, realizaci√≥n de cuentas y promedios), c√°lculos derivados con valores num√©ricos y textuales... 

- **Correcciones**: en caso de que no se pudiera hacer en el origen o en la etapa de limpieza, se deben corregir errores tipogr√°ficos, datos sin sentido ni significado, resolver conflictos de dominio de variables, aclaraci√≥n de datos ambiguos y asignaci√≥n de valores a datos nulos.

- **Integraci√≥n de varias fuentes**: implica la resoluci√≥n de claves y utilizaci√≥n de diccionarios de datos y repositorios de metadatos para el dise√±o del almac√©n de datos.

- **Eliminaci√≥n de datos y/o registros duplicados**: frecuente cuando se trabaja con distintas fuentes de datos provenientes de diferentes dimensiones o departamentos de la organizaci√≥n. Para ello, es muy √∫til el uso de funciones de b√∫squeda y agrupamiento aproximado.

- **Escalado y centrado**: para reducir los efectos adversos de contar con datos dispersos, es muy √∫til utilizar t√©cnicas para escalar y centrar los datos. Una posible transformaci√≥n consiste en restar a cada valor de cada instancia el valor medio de la variable correspondiente y despu√©s dividir los valores por la desviaci√≥n est√°ndar. De esta forma, los valores se escalan y se centran entorno al cero. Esta transformaci√≥n tambi√©n puede hacerse entorno al valor de la media o la mediana. Tambi√©n es posible escalar los valores en el intervalo [0‚àí1] o aplicar el logaritmo, para reducir la dispersi√≥n de los datos y mejorar la visualizaci√≥n.

- **Discretizaci√≥n**: la aplicaci√≥n de muchas t√©cnicas de aprendizaje autom√°tico requiere que los valores de las variables sean discreto, en lugar de continuos. La discretizaci√≥n es el proceso por el cual se divide un intervalo de valores continuos en tantos fragmentos como etiquetas o valores discretos se quieran asignar, sustituyendo los valores originales por la etiqueta o valor discreto correspondiente.

- **Selecci√≥n de caracter√≠sticas**: permite reducir el coste computacional de trabajar con todas las variables del conjunto de datos cuando, en muchas ocasiones, existen variables que no aportan informaci√≥n para el aprendizaje. Adem√°s de reducir el coste computacional, se reduce el n√∫mero de dimensiones del conjunto de datos, mejorando la visualizaci√≥n y mejorando el rendimiento de los modelos no solo en t√©rminos de tiempo, sino tambi√©n de rendimiento, puesto que existen m√©todos de aprendizaje cuyo rendimiento es peor cuando se ejecutan utilizando variables no significativas.


### 1.3 üîÄ Carga

Se trata de la √∫ltima fase de cara a incluir datos en el almac√©n de datos. **La carga inicial de los datos puede requerir bastante tiempo al cargar de forma progresiva todos los datos hist√≥ricos**, por lo que es normal realizarla en horas de baja carga de los sistemas. Una vez que el almac√©n de datos ha sido inicialmente cargado, las sucesivas cargas de datos se pueden realizar de dos formas:

- **Refresco**: El almac√©n de datos se reescribe completamente, de forma que se reemplazan los datos antiguos. El refresco **es utilizado habitualmente junto con la extracci√≥n est√°tica** y suele ser una estrategia muy utilizada para **la carga inicial** del almac√©n de datos, aunque puede tambi√©n realizarse a posteriori.

- **Actualizaci√≥n**: **Se a√±aden al almac√©n de datos solamente aquellos datos nuevos que se pretenden incluir, sin eliminar ni modificar los datos ya existentes**. Esta t√©cnica es **utilizada frecuentemente junto con la extracci√≥n incremental** para actualizar regularmente el almac√©n de datos. Se trata de una estrategia de carga **compleja de dise√±ar**, ya que requiere que sea eficiente computacionalmente. Para ello, se requieren mecanismos de detecci√≥n del cambio como aquellos basados en la marca de tiempo (timestamp) o la utilizaci√≥n de tablas de log, entre otros.

 
### 1.4 ‚¨ÜÔ∏èüîÑüîÄ Frameworks ETL

Recientemente, han aparecido m√∫ltiples _frameworks_ que ofrecen herramientas tecnol√≥gicas para dar un soporte integrado al proceso ETL. A continuaci√≥n, se describen algunos de los m√°s utilizados:

- **[Bubbles](http://bubbles.databrewery.org)**: Se trata de un framework ETL escrito en Python, aunque es posible utilizarlo desde otros lenguajes. Ofrece un marco de trabajo sencillo, donde el proceso ETL se describe como una secuencia de nodos conectados que representan los datos y las distintas operaciones que se pueden realizar con ellos. De esta forma, es posible construir un grafo que implemente el proceso ETL completo para un conjunto de datos.

- **[Apache Camel](https://camel.apache.org)**: Este framework escrito en lenguaje Java y de acceso abierto se enfoca en facilitar la integraci√≥n entre distintas fuentes de datos, haciendo el proceso m√°s accesible a los desarrolladores, ofreciendo distintas herramientas para dar soporte al proceso ETL.

- **[Keetle](https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/Pentaho_Data_Integration)**: Es el framework de Pentaho para dar soporte al proceso ETL. Pentaho es una suite o conjunto de programas para construir soluciones de inteligencia de negocio y Keetle es su entorno de trabajo ETL. Similar a Bubbles, ofrece un entorno de trabajo sencillo para implementar un proceso ETL a trav√©s de nodos y conexiones entre ellos. Adem√°s, Keetle es de acceso abierto.

## 2. Dise√±o en Estrella

A la hora de dise√±ar un almac√©n de datos, existen dos alternativas ampliamente utilizadas: el **dise√±o en estrella**, que promueve el dise√±o directo de estructuras l√≥gicas sobre el modelo relacional, y el **dise√±o en copo de nieve**, como variante del dise√±o en estrella.

El proceso de desarrollo de un almac√©n de datos siguiendo el dise√±o en estrella puede estructurarse seg√∫n las siguientes fases:

1. **Elegir un proceso de negocio a modelar**: cualquier proceso de negocio puede ser modelado como un almac√©n de datos. No obstante, ser√°n de especial inter√©s aquellos procesos **necesarios para la toma de decisiones y que involucran a diferentes unidades de la organizaci√≥n**.

2. **Escoger la granularidad del proceso de negocio**: Los datos almacenados en el almac√©n de datos **deben expresarse siempre al mismo nivel de detalle**. Por este motivo es necesario escoger un nivel de granularidad al comienzo del dise√±o del almac√©n de datos.

3. **Selecci√≥n de medidas/hechos**: indican **qu√© se necesita medir o evaluar enel proceso de negocio** con el fin de dar respuesta a las necesidades de informaci√≥n y toma de decisiones por las cuales se pretende dise√±ar el almac√©nde datos. Por ejemplo, en el √°mbito log√≠stico las medidas podr√≠an ser las unidades aceptadas odevueltas, mientras que en el √°mbito del control de calidad podr√≠an ser lasunidades producidas, defectuosas, costo de la producci√≥n.

4. **Elegir las dimensiones que se aplicar√°n a cada medida o hecho**: las dimensiones especifican **las distintas propiedades de las medidas o hechos que se pretenden almacenar e integrar dentro del almac√©n de datos**. Por ejemplo, si las medidas seleccionadas est√°n relacionadas con las ventas de una determinada empresa, las dimensiones podr√≠an ser: fecha de la venta, vendedor ,cliente, producto...

A la hora de dise√±ar el almac√©n de datos siguiendo el dise√±o en estrella, se crea una **tabla central, tambi√©n llamada tabla de hechos, tabla factual o fact table**. Esta tabla es la que posee los datos (medidas o hechos) sobre las diferentes combinaciones de las dimensiones. En algunas ocasiones, un dise√±o en estrella presenta m√°s de una tabla de hechos. Los hechos introducidos pueden ser de distintos tipos: **completamente aditivos** (total de ventas de un departamento), **no aditivos** (m√°rgen de beneficios expresado como porcentaje) o **semiaditivos** (como datos intermedios que pueden agregarse con datos de otras dimensiones).

Rodeando a la tabla de hechos, se encuentra u**na tabla por cada una de las dimensiones definidas**. La clave primaria de la tabla de hechos se crea combinando las claves primarias de sus dimensiones relacionadas, de forma que est√° compuesta por las claves ajenas de las tablas de dimensiones que relaciona. De esta forma, la tabla de hechos presenta una relaci√≥n del tipo ‚Äúmuchos a muchos‚Äù entre todas las tablas de dimensiones que relaciona, a la vez que una relaci√≥n ‚Äúmuchos a uno‚Äù con cada tabla de dimensi√≥n por separado. Este esquema con forma de estrella da nombre a este tipo de dise√±o.

**Ejemplo**

Sea un almac√©n de datos en el que se pretende almacenar **las ventas llevadas a cabo en una organizaci√≥n**. La tabla central de hechos representa los datos de cada venta, mientras que las dimensiones que rodean a la tabla central recogen datos sobre los productos vendidos, la fecha de la venta, el canal de distribuci√≥n y el lugar de entrega. La [figura1.12][figura1.12] muestra un ejemplo de este esquema de almac√©n de datos.

<figure style="align: center;">
    <img src="images/Figura1.12_Ejemplo_de_almac√©n_de_datos_dise√±ado_en_estrella.jpg">
    <figcaption>Figura 1.12: Ejemplo de almac√©n de datos dise√±ado en estrella. (Fuente: UCLM)</figcaption>
</figure>

**Ventajas e incovenientes**

Entre las **ventajas del dise√±o en estrella**, destaca ser un dise√±o **f√°cil de modificar que proporciona tiempos r√°pidos de respuesta**, simplificando la navegaci√≥n de los medatadatos. Adem√°s, este dise√±o **permite simular vistas de los datos que obtendr√°n los usuarios finales y facilita la interacci√≥n con otras herramientas**.

Sin embargo, el dise√±o en estrella presenta algunos **inconvenientes** que surgen, por ejemplo, cuando **se combinan dimensiones con distintos niveles de detalle**. Adem√°s, cuando se pretende limitar los niveles de una dimensi√≥n se debe a√±adir un campo de nivel o utilizar un modelo de tipo constelaci√≥n, donde se incluyen diferentes estrellas que almacenan tablas de hechos agregadas, lo cual **a√±ade complejidad al esquema**. 

**Resumen**

üìÑ El **dise√±o en estrella** permite crear un almac√©n de datos con una estructura centralizada. El dise√±o se compone de una tabla de hechos central, que recoge los valores de las medidas del proceso de negocio que se pretende modelar. Rodeando a la tabla de hechos, se incluyen tantas tablas como dimensiones se hayan especificado en el modelo, las cuales se relacionan entre s√≠ a trav√©s de la tabla de hechos.


## 3. Dise√±o en Copo de Nieve

Otro de los dise√±os m√°s extendidos a la hora de implementar un almac√©n de datos es el **dise√±o en copo de nieve**. De hecho, el dise√±o de copo de nieve es considerado una variante o derivado del dise√±o en estrella y, por tanto, puede construirse a partir de este siguiendo las mismas etapas que se describieron en la secci√≥n anterior. 

Este dise√±o es muy utilizado cuando se dispone de **tablas de dimensi√≥n con una gran cantidad de atributos**. En esta circunstancia, el dise√±o de copo de nieve permite **normalizar las tablas de dimensi√≥n**, ofreciendo un mejor rendimiento cuando las consultas realizadas sobre el almac√©n de datos requieren del uso de operadores de agregaci√≥n. De esta forma, la tabla de hechos deja de ser la √∫nica tabla que presenta relaciones con las dem√°s, apareciendo nuevas relaciones que se dan entre las tablas normalizadas que componen las tablas de dimensiones. Este esquema, que incluye ramificaciones en cada dimensi√≥n que se corresponden con las tablas necesarias para su normalizaci√≥n, guarda un gran parecido con la estructura de un copo de nieve, lo cual da nombre a este dise√±o.

**La diferencia entre los dise√±os en estrella y copo de nieve radica fundamentalmente en la modelizaci√≥n de las tablas de dimensiones**. Para obtener un dise√±o en copo de nieve, basta con partir de un dise√±o en estrella en el que, tras un proceso de normalizaci√≥n, se obtienen varias tablas de dimensi√≥n a partir de una tabla desnormalizada. Dentro del dise√±o de copo de nieve, es posible distinguir entre **copo de nieve parcial**, en el que solo algunas de las dimensiones son normalizadas y **copo de nieve completo**, en el que todas las dimensiones del esquema son normalizadas.

Siguiendo con el ejemplo anterior, en el que se mostraba un almac√©n de datos para representar las ventas realizadas por una organizaci√≥n, la [figura1.13][figura1.13] muestra un dise√±o para un almac√©n de datos siguiendo un esquema de copo de nieve parcial, en el que las dimensiones geogr√°fica y producto han sido normalizadas. 

<figure style="align: center;">
    <img src="images/Figura1.13_Ejemplo_de_almac√©n_de_datos_dise√±ado_en_copo_de_nieve.jpg">
    <figcaption>Figura 1.13: Ejemplo de almac√©n de datos dise√±ado en copo de nieve. (Fuente: UCLM)</figcaption>
</figure>


**Ventajas e Inconvenientes**

La normalizaci√≥n de las tablas de dimensiones proporciona al dise√±o de copo de nieve la **mejora de la eficiencia en la realizaci√≥n de consultas complejas que requieren el uso de operadores de agregaci√≥n**. Sin embargo, este dise√±o es conceptualmente m√°s dif√≠cil de implementar, ya que **incluye un mayor n√∫mero de tablas** y requiere de realizar muchos joins entre ellas, lo que **incrementa los tiempos de consulta**.


En ocasiones, se requiere dise√±ar un almac√©n de datos que contenga m√°s de una tabla de hechos. Para ello, el **esquema o dise√±o en constelaci√≥n** permite crear un almac√©n de datos de forma similar al dise√±o en estrella, con la diferencia de que este modelo incluye m√°s de una tabla de hechos. Adem√°s, cada tabla de hechos puede vincularse con todas o solo algunas tablas de dimensiones. Este esquema contribuye a la reutilizaci√≥n de las tablas de dimensiones, las cuales se pueden relacionar con m√°s de una tabla de hechos.

**Resumen**

üìÑ El **dise√±o en copo de nieve** permite crear un almac√©n de datos a partir de un dise√±o en estrella. Para ello, las tablas de dimensiones se normalizan, generando m√°s de una tabla para cada una de las dimensiones, mejorando la eficiencia de consultas complejas que requieren el uso de operadores avanzados.



[figura1.11]: images/Figura1.11_Fuentes_para_la_obtenci√≥n_de_datos_abiertos.jpg "Figura1.11_Fuentes para la obtenci√≥n de datos abiertos.jpg"
[figura1.12]: images/Figura1.12_Ejemplo_de_almac√©n_de_datos_dise√±ado_en_estrella.jpg "Figura1.12_Ejemplo de almac√©n de datos dise√±ado en estrella.jpg"
[figura1.13]: images/Figura1.13_Ejemplo_de_almac√©n_de_datos_dise√±ado_en_copo_de_nieve.jpg "Figura1.13_Ejemplo de almac√©n de datos dise√±ado en copo de nieve.jpg"